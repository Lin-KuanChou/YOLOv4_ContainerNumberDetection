{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from tensorflow.keras import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# !! 注意 !! 訓練前需先執行過voc_annotation.py 檔 !!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 參數設置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_path = 'model_data/cls_classes.txt'\n",
    "anchors_path = 'model_data/yolo_anchors.txt'\n",
    "anchors_mask = [[6, 7, 8], [3, 4, 5], [0, 1, 2]]\n",
    "model_path      = 'model_data/yolo4_weight.h5'\n",
    "train_annotation_path = 'Data/train/list/yolo_train.txt'\n",
    "val_annotation_path = 'Data/train/list/yolo_val.txt'\n",
    "\n",
    "class_names = ['ContainerNum']\n",
    "num_classes = 1\n",
    "input_shape = [416, 416]\n",
    "Max_epoch = 10000\n",
    "lr = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_anchors(anchors_path):\n",
    "    '''loads the anchors from a file'''\n",
    "    with open(anchors_path, encoding='utf-8') as f:\n",
    "        anchors = f.readline()\n",
    "    anchors = [float(x) for x in anchors.split(',')]\n",
    "    anchors = np.array(anchors).reshape(-1, 2)\n",
    "    return anchors, len(anchors)\n",
    "\n",
    "anchors, num_anchors = get_anchors(anchors_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init_lr_fit 0.0003\n",
      "Min_lr_fit 2.9999999999999997e-06\n"
     ]
    }
   ],
   "source": [
    "Min_lr = lr * 0.01\n",
    "Init_lr_fit = min(max(4 / 64 * lr, 3e-4), 1e-3)\n",
    "Min_lr_fit = min(max(4 / 64 * Min_lr, 3e-4 * 1e-2), 1e-3 * 1e-2)\n",
    "print('Init_lr_fit', Init_lr_fit)\n",
    "print('Min_lr_fit', Min_lr_fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 載入資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.dataloader import YoloDatasets\n",
    "\n",
    "with open(train_annotation_path, encoding='utf-8') as f:\n",
    "    train_lines = f.readlines()\n",
    "with open(val_annotation_path, encoding='utf-8') as f:\n",
    "    val_lines   = f.readlines()\n",
    "\n",
    "train_dataloader = YoloDatasets(train_lines, input_shape, anchors, batch_size=4, num_classes=1, anchors_mask=anchors_mask, epoch_now=0, epoch_length=Max_epoch, mosaic=True, train=True)\n",
    "val_dataloader = YoloDatasets(val_lines, input_shape, anchors, batch_size=4, num_classes=1, anchors_mask=anchors_mask, epoch_now=0, epoch_length=Max_epoch, mosaic=False, train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 建立Yolo模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 416, 416, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 416, 416, 32) 864         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 416, 416, 32) 128         conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "mish (Mish)                     (None, 416, 416, 32) 0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d (ZeroPadding2D)  (None, 417, 417, 32) 0           mish[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 208, 208, 64) 18432       zero_padding2d[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 208, 208, 64) 256         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "mish_1 (Mish)                   (None, 208, 208, 64) 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 208, 208, 64) 4096        mish_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 208, 208, 64) 256         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "mish_3 (Mish)                   (None, 208, 208, 64) 0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 208, 208, 32) 2048        mish_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 208, 208, 32) 128         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "mish_4 (Mish)                   (None, 208, 208, 32) 0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 208, 208, 64) 18432       mish_4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 208, 208, 64) 256         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "mish_5 (Mish)                   (None, 208, 208, 64) 0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 208, 208, 64) 0           mish_3[0][0]                     \n",
      "                                                                 mish_5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 208, 208, 64) 4096        add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 208, 208, 64) 4096        mish_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 208, 208, 64) 256         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 208, 208, 64) 256         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "mish_6 (Mish)                   (None, 208, 208, 64) 0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "mish_2 (Mish)                   (None, 208, 208, 64) 0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 208, 208, 128 0           mish_6[0][0]                     \n",
      "                                                                 mish_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 208, 208, 64) 8192        concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 208, 208, 64) 256         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "mish_7 (Mish)                   (None, 208, 208, 64) 0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_1 (ZeroPadding2D (None, 209, 209, 64) 0           mish_7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 104, 104, 128 73728       zero_padding2d_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 104, 104, 128 512         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "mish_8 (Mish)                   (None, 104, 104, 128 0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 104, 104, 64) 8192        mish_8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 104, 104, 64) 256         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_10 (Mish)                  (None, 104, 104, 64) 0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 104, 104, 64) 4096        mish_10[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 104, 104, 64) 256         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_11 (Mish)                  (None, 104, 104, 64) 0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 104, 104, 64) 36864       mish_11[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 104, 104, 64) 256         conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_12 (Mish)                  (None, 104, 104, 64) 0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 104, 104, 64) 0           mish_10[0][0]                    \n",
      "                                                                 mish_12[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 104, 104, 64) 4096        add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 104, 104, 64) 256         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_13 (Mish)                  (None, 104, 104, 64) 0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 104, 104, 64) 36864       mish_13[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 104, 104, 64) 256         conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_14 (Mish)                  (None, 104, 104, 64) 0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 104, 104, 64) 0           add_1[0][0]                      \n",
      "                                                                 mish_14[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 104, 104, 64) 4096        add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 104, 104, 64) 8192        mish_8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 104, 104, 64) 256         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 104, 104, 64) 256         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "mish_15 (Mish)                  (None, 104, 104, 64) 0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mish_9 (Mish)                   (None, 104, 104, 64) 0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 104, 104, 128 0           mish_15[0][0]                    \n",
      "                                                                 mish_9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 104, 104, 128 16384       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 104, 104, 128 512         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_16 (Mish)                  (None, 104, 104, 128 0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_2 (ZeroPadding2D (None, 105, 105, 128 0           mish_16[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 52, 52, 256)  294912      zero_padding2d_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 52, 52, 256)  1024        conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_17 (Mish)                  (None, 52, 52, 256)  0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 52, 52, 128)  32768       mish_17[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 52, 52, 128)  512         conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_19 (Mish)                  (None, 52, 52, 128)  0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 52, 52, 128)  16384       mish_19[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 52, 52, 128)  512         conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_20 (Mish)                  (None, 52, 52, 128)  0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 52, 52, 128)  147456      mish_20[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 52, 52, 128)  512         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_21 (Mish)                  (None, 52, 52, 128)  0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 52, 52, 128)  0           mish_19[0][0]                    \n",
      "                                                                 mish_21[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 52, 52, 128)  16384       add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 52, 52, 128)  512         conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_22 (Mish)                  (None, 52, 52, 128)  0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 52, 52, 128)  147456      mish_22[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 52, 52, 128)  512         conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_23 (Mish)                  (None, 52, 52, 128)  0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 52, 52, 128)  0           add_3[0][0]                      \n",
      "                                                                 mish_23[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 52, 52, 128)  16384       add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 52, 52, 128)  512         conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_24 (Mish)                  (None, 52, 52, 128)  0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 52, 52, 128)  147456      mish_24[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 52, 52, 128)  512         conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_25 (Mish)                  (None, 52, 52, 128)  0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 52, 52, 128)  0           add_4[0][0]                      \n",
      "                                                                 mish_25[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 52, 52, 128)  16384       add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 52, 52, 128)  512         conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_26 (Mish)                  (None, 52, 52, 128)  0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 52, 52, 128)  147456      mish_26[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 52, 52, 128)  512         conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_27 (Mish)                  (None, 52, 52, 128)  0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 52, 52, 128)  0           add_5[0][0]                      \n",
      "                                                                 mish_27[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 52, 52, 128)  16384       add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 52, 52, 128)  512         conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_28 (Mish)                  (None, 52, 52, 128)  0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 52, 52, 128)  147456      mish_28[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 52, 52, 128)  512         conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_29 (Mish)                  (None, 52, 52, 128)  0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 52, 52, 128)  0           add_6[0][0]                      \n",
      "                                                                 mish_29[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 52, 52, 128)  16384       add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 52, 52, 128)  512         conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_30 (Mish)                  (None, 52, 52, 128)  0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 52, 52, 128)  147456      mish_30[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 52, 52, 128)  512         conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_31 (Mish)                  (None, 52, 52, 128)  0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 52, 52, 128)  0           add_7[0][0]                      \n",
      "                                                                 mish_31[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 52, 52, 128)  16384       add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 52, 52, 128)  512         conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_32 (Mish)                  (None, 52, 52, 128)  0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 52, 52, 128)  147456      mish_32[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 52, 52, 128)  512         conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_33 (Mish)                  (None, 52, 52, 128)  0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 52, 52, 128)  0           add_8[0][0]                      \n",
      "                                                                 mish_33[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 52, 52, 128)  16384       add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 52, 52, 128)  512         conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_34 (Mish)                  (None, 52, 52, 128)  0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 52, 52, 128)  147456      mish_34[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 52, 52, 128)  512         conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_35 (Mish)                  (None, 52, 52, 128)  0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 52, 52, 128)  0           add_9[0][0]                      \n",
      "                                                                 mish_35[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 52, 52, 128)  16384       add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 52, 52, 128)  32768       mish_17[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 52, 52, 128)  512         conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 52, 52, 128)  512         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_36 (Mish)                  (None, 52, 52, 128)  0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mish_18 (Mish)                  (None, 52, 52, 128)  0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 52, 52, 256)  0           mish_36[0][0]                    \n",
      "                                                                 mish_18[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 52, 52, 256)  65536       concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 52, 52, 256)  1024        conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_37 (Mish)                  (None, 52, 52, 256)  0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_3 (ZeroPadding2D (None, 53, 53, 256)  0           mish_37[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 26, 26, 512)  1179648     zero_padding2d_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 26, 26, 512)  2048        conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_38 (Mish)                  (None, 26, 26, 512)  0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, 26, 26, 256)  131072      mish_38[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 26, 26, 256)  1024        conv2d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_40 (Mish)                  (None, 26, 26, 256)  0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, 26, 26, 256)  65536       mish_40[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 26, 26, 256)  1024        conv2d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_41 (Mish)                  (None, 26, 26, 256)  0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)              (None, 26, 26, 256)  589824      mish_41[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 26, 26, 256)  1024        conv2d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_42 (Mish)                  (None, 26, 26, 256)  0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 26, 26, 256)  0           mish_40[0][0]                    \n",
      "                                                                 mish_42[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_43 (Conv2D)              (None, 26, 26, 256)  65536       add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_43 (BatchNo (None, 26, 26, 256)  1024        conv2d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_43 (Mish)                  (None, 26, 26, 256)  0           batch_normalization_43[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)              (None, 26, 26, 256)  589824      mish_43[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_44 (BatchNo (None, 26, 26, 256)  1024        conv2d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_44 (Mish)                  (None, 26, 26, 256)  0           batch_normalization_44[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 26, 26, 256)  0           add_11[0][0]                     \n",
      "                                                                 mish_44[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_45 (Conv2D)              (None, 26, 26, 256)  65536       add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_45 (BatchNo (None, 26, 26, 256)  1024        conv2d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_45 (Mish)                  (None, 26, 26, 256)  0           batch_normalization_45[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_46 (Conv2D)              (None, 26, 26, 256)  589824      mish_45[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_46 (BatchNo (None, 26, 26, 256)  1024        conv2d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_46 (Mish)                  (None, 26, 26, 256)  0           batch_normalization_46[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, 26, 26, 256)  0           add_12[0][0]                     \n",
      "                                                                 mish_46[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_47 (Conv2D)              (None, 26, 26, 256)  65536       add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_47 (BatchNo (None, 26, 26, 256)  1024        conv2d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_47 (Mish)                  (None, 26, 26, 256)  0           batch_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_48 (Conv2D)              (None, 26, 26, 256)  589824      mish_47[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_48 (BatchNo (None, 26, 26, 256)  1024        conv2d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_48 (Mish)                  (None, 26, 26, 256)  0           batch_normalization_48[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_14 (Add)                    (None, 26, 26, 256)  0           add_13[0][0]                     \n",
      "                                                                 mish_48[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_49 (Conv2D)              (None, 26, 26, 256)  65536       add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_49 (BatchNo (None, 26, 26, 256)  1024        conv2d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_49 (Mish)                  (None, 26, 26, 256)  0           batch_normalization_49[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_50 (Conv2D)              (None, 26, 26, 256)  589824      mish_49[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_50 (BatchNo (None, 26, 26, 256)  1024        conv2d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_50 (Mish)                  (None, 26, 26, 256)  0           batch_normalization_50[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_15 (Add)                    (None, 26, 26, 256)  0           add_14[0][0]                     \n",
      "                                                                 mish_50[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_51 (Conv2D)              (None, 26, 26, 256)  65536       add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_51 (BatchNo (None, 26, 26, 256)  1024        conv2d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_51 (Mish)                  (None, 26, 26, 256)  0           batch_normalization_51[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_52 (Conv2D)              (None, 26, 26, 256)  589824      mish_51[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_52 (BatchNo (None, 26, 26, 256)  1024        conv2d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_52 (Mish)                  (None, 26, 26, 256)  0           batch_normalization_52[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_16 (Add)                    (None, 26, 26, 256)  0           add_15[0][0]                     \n",
      "                                                                 mish_52[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_53 (Conv2D)              (None, 26, 26, 256)  65536       add_16[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_53 (BatchNo (None, 26, 26, 256)  1024        conv2d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_53 (Mish)                  (None, 26, 26, 256)  0           batch_normalization_53[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_54 (Conv2D)              (None, 26, 26, 256)  589824      mish_53[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_54 (BatchNo (None, 26, 26, 256)  1024        conv2d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_54 (Mish)                  (None, 26, 26, 256)  0           batch_normalization_54[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_17 (Add)                    (None, 26, 26, 256)  0           add_16[0][0]                     \n",
      "                                                                 mish_54[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_55 (Conv2D)              (None, 26, 26, 256)  65536       add_17[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_55 (BatchNo (None, 26, 26, 256)  1024        conv2d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_55 (Mish)                  (None, 26, 26, 256)  0           batch_normalization_55[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_56 (Conv2D)              (None, 26, 26, 256)  589824      mish_55[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_56 (BatchNo (None, 26, 26, 256)  1024        conv2d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_56 (Mish)                  (None, 26, 26, 256)  0           batch_normalization_56[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_18 (Add)                    (None, 26, 26, 256)  0           add_17[0][0]                     \n",
      "                                                                 mish_56[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_57 (Conv2D)              (None, 26, 26, 256)  65536       add_18[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 26, 26, 256)  131072      mish_38[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_57 (BatchNo (None, 26, 26, 256)  1024        conv2d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 26, 26, 256)  1024        conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_57 (Mish)                  (None, 26, 26, 256)  0           batch_normalization_57[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mish_39 (Mish)                  (None, 26, 26, 256)  0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 26, 26, 512)  0           mish_57[0][0]                    \n",
      "                                                                 mish_39[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_58 (Conv2D)              (None, 26, 26, 512)  262144      concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_58 (BatchNo (None, 26, 26, 512)  2048        conv2d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_58 (Mish)                  (None, 26, 26, 512)  0           batch_normalization_58[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_4 (ZeroPadding2D (None, 27, 27, 512)  0           mish_58[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_59 (Conv2D)              (None, 13, 13, 1024) 4718592     zero_padding2d_4[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_59 (BatchNo (None, 13, 13, 1024) 4096        conv2d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_59 (Mish)                  (None, 13, 13, 1024) 0           batch_normalization_59[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_61 (Conv2D)              (None, 13, 13, 512)  524288      mish_59[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_61 (BatchNo (None, 13, 13, 512)  2048        conv2d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_61 (Mish)                  (None, 13, 13, 512)  0           batch_normalization_61[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_62 (Conv2D)              (None, 13, 13, 512)  262144      mish_61[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_62 (BatchNo (None, 13, 13, 512)  2048        conv2d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_62 (Mish)                  (None, 13, 13, 512)  0           batch_normalization_62[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_63 (Conv2D)              (None, 13, 13, 512)  2359296     mish_62[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_63 (BatchNo (None, 13, 13, 512)  2048        conv2d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_63 (Mish)                  (None, 13, 13, 512)  0           batch_normalization_63[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_19 (Add)                    (None, 13, 13, 512)  0           mish_61[0][0]                    \n",
      "                                                                 mish_63[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_64 (Conv2D)              (None, 13, 13, 512)  262144      add_19[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_64 (BatchNo (None, 13, 13, 512)  2048        conv2d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_64 (Mish)                  (None, 13, 13, 512)  0           batch_normalization_64[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_65 (Conv2D)              (None, 13, 13, 512)  2359296     mish_64[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_65 (BatchNo (None, 13, 13, 512)  2048        conv2d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_65 (Mish)                  (None, 13, 13, 512)  0           batch_normalization_65[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_20 (Add)                    (None, 13, 13, 512)  0           add_19[0][0]                     \n",
      "                                                                 mish_65[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_66 (Conv2D)              (None, 13, 13, 512)  262144      add_20[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_66 (BatchNo (None, 13, 13, 512)  2048        conv2d_66[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_66 (Mish)                  (None, 13, 13, 512)  0           batch_normalization_66[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_67 (Conv2D)              (None, 13, 13, 512)  2359296     mish_66[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_67 (BatchNo (None, 13, 13, 512)  2048        conv2d_67[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_67 (Mish)                  (None, 13, 13, 512)  0           batch_normalization_67[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_21 (Add)                    (None, 13, 13, 512)  0           add_20[0][0]                     \n",
      "                                                                 mish_67[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_68 (Conv2D)              (None, 13, 13, 512)  262144      add_21[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_68 (BatchNo (None, 13, 13, 512)  2048        conv2d_68[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_68 (Mish)                  (None, 13, 13, 512)  0           batch_normalization_68[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_69 (Conv2D)              (None, 13, 13, 512)  2359296     mish_68[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_69 (BatchNo (None, 13, 13, 512)  2048        conv2d_69[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_69 (Mish)                  (None, 13, 13, 512)  0           batch_normalization_69[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_22 (Add)                    (None, 13, 13, 512)  0           add_21[0][0]                     \n",
      "                                                                 mish_69[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_70 (Conv2D)              (None, 13, 13, 512)  262144      add_22[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_60 (Conv2D)              (None, 13, 13, 512)  524288      mish_59[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_70 (BatchNo (None, 13, 13, 512)  2048        conv2d_70[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_60 (BatchNo (None, 13, 13, 512)  2048        conv2d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_70 (Mish)                  (None, 13, 13, 512)  0           batch_normalization_70[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mish_60 (Mish)                  (None, 13, 13, 512)  0           batch_normalization_60[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 13, 13, 1024) 0           mish_70[0][0]                    \n",
      "                                                                 mish_60[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_71 (Conv2D)              (None, 13, 13, 1024) 1048576     concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_71 (BatchNo (None, 13, 13, 1024) 4096        conv2d_71[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_71 (Mish)                  (None, 13, 13, 1024) 0           batch_normalization_71[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_72 (Conv2D)              (None, 13, 13, 512)  524288      mish_71[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_72 (BatchNo (None, 13, 13, 512)  2048        conv2d_72[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)         (None, 13, 13, 512)  0           batch_normalization_72[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_73 (Conv2D)              (None, 13, 13, 1024) 4718592     leaky_re_lu[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_73 (BatchNo (None, 13, 13, 1024) 4096        conv2d_73[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, 13, 13, 1024) 0           batch_normalization_73[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_74 (Conv2D)              (None, 13, 13, 512)  524288      leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_74 (BatchNo (None, 13, 13, 512)  2048        conv2d_74[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, 13, 13, 512)  0           batch_normalization_74[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 13, 13, 512)  0           leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 13, 13, 512)  0           leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 13, 13, 512)  0           leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 13, 13, 2048) 0           max_pooling2d[0][0]              \n",
      "                                                                 max_pooling2d_1[0][0]            \n",
      "                                                                 max_pooling2d_2[0][0]            \n",
      "                                                                 leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_75 (Conv2D)              (None, 13, 13, 512)  1048576     concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_75 (BatchNo (None, 13, 13, 512)  2048        conv2d_75[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)       (None, 13, 13, 512)  0           batch_normalization_75[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_76 (Conv2D)              (None, 13, 13, 1024) 4718592     leaky_re_lu_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_76 (BatchNo (None, 13, 13, 1024) 4096        conv2d_76[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)       (None, 13, 13, 1024) 0           batch_normalization_76[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_77 (Conv2D)              (None, 13, 13, 512)  524288      leaky_re_lu_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_77 (BatchNo (None, 13, 13, 512)  2048        conv2d_77[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)       (None, 13, 13, 512)  0           batch_normalization_77[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_78 (Conv2D)              (None, 13, 13, 256)  131072      leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_79 (Conv2D)              (None, 26, 26, 256)  131072      mish_58[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_78 (BatchNo (None, 13, 13, 256)  1024        conv2d_78[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_79 (BatchNo (None, 26, 26, 256)  1024        conv2d_79[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)       (None, 13, 13, 256)  0           batch_normalization_78[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)       (None, 26, 26, 256)  0           batch_normalization_79[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d (UpSampling2D)    (None, 26, 26, 256)  0           leaky_re_lu_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 26, 26, 512)  0           leaky_re_lu_7[0][0]              \n",
      "                                                                 up_sampling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_80 (Conv2D)              (None, 26, 26, 256)  131072      concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_80 (BatchNo (None, 26, 26, 256)  1024        conv2d_80[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)       (None, 26, 26, 256)  0           batch_normalization_80[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_81 (Conv2D)              (None, 26, 26, 512)  1179648     leaky_re_lu_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_81 (BatchNo (None, 26, 26, 512)  2048        conv2d_81[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)       (None, 26, 26, 512)  0           batch_normalization_81[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_82 (Conv2D)              (None, 26, 26, 256)  131072      leaky_re_lu_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_82 (BatchNo (None, 26, 26, 256)  1024        conv2d_82[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)      (None, 26, 26, 256)  0           batch_normalization_82[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_83 (Conv2D)              (None, 26, 26, 512)  1179648     leaky_re_lu_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_83 (BatchNo (None, 26, 26, 512)  2048        conv2d_83[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)      (None, 26, 26, 512)  0           batch_normalization_83[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_84 (Conv2D)              (None, 26, 26, 256)  131072      leaky_re_lu_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_84 (BatchNo (None, 26, 26, 256)  1024        conv2d_84[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)      (None, 26, 26, 256)  0           batch_normalization_84[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_85 (Conv2D)              (None, 26, 26, 128)  32768       leaky_re_lu_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_86 (Conv2D)              (None, 52, 52, 128)  32768       mish_37[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_85 (BatchNo (None, 26, 26, 128)  512         conv2d_85[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_86 (BatchNo (None, 52, 52, 128)  512         conv2d_86[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_13 (LeakyReLU)      (None, 26, 26, 128)  0           batch_normalization_85[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_14 (LeakyReLU)      (None, 52, 52, 128)  0           batch_normalization_86[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2D)  (None, 52, 52, 128)  0           leaky_re_lu_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 52, 52, 256)  0           leaky_re_lu_14[0][0]             \n",
      "                                                                 up_sampling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_87 (Conv2D)              (None, 52, 52, 128)  32768       concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_87 (BatchNo (None, 52, 52, 128)  512         conv2d_87[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_15 (LeakyReLU)      (None, 52, 52, 128)  0           batch_normalization_87[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_88 (Conv2D)              (None, 52, 52, 256)  294912      leaky_re_lu_15[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_88 (BatchNo (None, 52, 52, 256)  1024        conv2d_88[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_16 (LeakyReLU)      (None, 52, 52, 256)  0           batch_normalization_88[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_89 (Conv2D)              (None, 52, 52, 128)  32768       leaky_re_lu_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_89 (BatchNo (None, 52, 52, 128)  512         conv2d_89[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_17 (LeakyReLU)      (None, 52, 52, 128)  0           batch_normalization_89[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_90 (Conv2D)              (None, 52, 52, 256)  294912      leaky_re_lu_17[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_90 (BatchNo (None, 52, 52, 256)  1024        conv2d_90[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_18 (LeakyReLU)      (None, 52, 52, 256)  0           batch_normalization_90[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_91 (Conv2D)              (None, 52, 52, 128)  32768       leaky_re_lu_18[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_91 (BatchNo (None, 52, 52, 128)  512         conv2d_91[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_19 (LeakyReLU)      (None, 52, 52, 128)  0           batch_normalization_91[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_5 (ZeroPadding2D (None, 53, 53, 128)  0           leaky_re_lu_19[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_94 (Conv2D)              (None, 26, 26, 256)  294912      zero_padding2d_5[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_93 (BatchNo (None, 26, 26, 256)  1024        conv2d_94[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_21 (LeakyReLU)      (None, 26, 26, 256)  0           batch_normalization_93[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 26, 26, 512)  0           leaky_re_lu_21[0][0]             \n",
      "                                                                 leaky_re_lu_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_95 (Conv2D)              (None, 26, 26, 256)  131072      concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_94 (BatchNo (None, 26, 26, 256)  1024        conv2d_95[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_22 (LeakyReLU)      (None, 26, 26, 256)  0           batch_normalization_94[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_96 (Conv2D)              (None, 26, 26, 512)  1179648     leaky_re_lu_22[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_95 (BatchNo (None, 26, 26, 512)  2048        conv2d_96[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_23 (LeakyReLU)      (None, 26, 26, 512)  0           batch_normalization_95[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_97 (Conv2D)              (None, 26, 26, 256)  131072      leaky_re_lu_23[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_96 (BatchNo (None, 26, 26, 256)  1024        conv2d_97[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_24 (LeakyReLU)      (None, 26, 26, 256)  0           batch_normalization_96[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_98 (Conv2D)              (None, 26, 26, 512)  1179648     leaky_re_lu_24[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_97 (BatchNo (None, 26, 26, 512)  2048        conv2d_98[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_25 (LeakyReLU)      (None, 26, 26, 512)  0           batch_normalization_97[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_99 (Conv2D)              (None, 26, 26, 256)  131072      leaky_re_lu_25[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_98 (BatchNo (None, 26, 26, 256)  1024        conv2d_99[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_26 (LeakyReLU)      (None, 26, 26, 256)  0           batch_normalization_98[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_6 (ZeroPadding2D (None, 27, 27, 256)  0           leaky_re_lu_26[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_102 (Conv2D)             (None, 13, 13, 512)  1179648     zero_padding2d_6[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_100 (BatchN (None, 13, 13, 512)  2048        conv2d_102[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_28 (LeakyReLU)      (None, 13, 13, 512)  0           batch_normalization_100[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 13, 13, 1024) 0           leaky_re_lu_28[0][0]             \n",
      "                                                                 leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_103 (Conv2D)             (None, 13, 13, 512)  524288      concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_101 (BatchN (None, 13, 13, 512)  2048        conv2d_103[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_29 (LeakyReLU)      (None, 13, 13, 512)  0           batch_normalization_101[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_104 (Conv2D)             (None, 13, 13, 1024) 4718592     leaky_re_lu_29[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_102 (BatchN (None, 13, 13, 1024) 4096        conv2d_104[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_30 (LeakyReLU)      (None, 13, 13, 1024) 0           batch_normalization_102[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_105 (Conv2D)             (None, 13, 13, 512)  524288      leaky_re_lu_30[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_103 (BatchN (None, 13, 13, 512)  2048        conv2d_105[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_31 (LeakyReLU)      (None, 13, 13, 512)  0           batch_normalization_103[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_106 (Conv2D)             (None, 13, 13, 1024) 4718592     leaky_re_lu_31[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_104 (BatchN (None, 13, 13, 1024) 4096        conv2d_106[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_32 (LeakyReLU)      (None, 13, 13, 1024) 0           batch_normalization_104[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_107 (Conv2D)             (None, 13, 13, 512)  524288      leaky_re_lu_32[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_105 (BatchN (None, 13, 13, 512)  2048        conv2d_107[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_33 (LeakyReLU)      (None, 13, 13, 512)  0           batch_normalization_105[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_108 (Conv2D)             (None, 13, 13, 1024) 4718592     leaky_re_lu_33[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_100 (Conv2D)             (None, 26, 26, 512)  1179648     leaky_re_lu_26[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_92 (Conv2D)              (None, 52, 52, 256)  294912      leaky_re_lu_19[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_106 (BatchN (None, 13, 13, 1024) 4096        conv2d_108[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_99 (BatchNo (None, 26, 26, 512)  2048        conv2d_100[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_92 (BatchNo (None, 52, 52, 256)  1024        conv2d_92[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_34 (LeakyReLU)      (None, 13, 13, 1024) 0           batch_normalization_106[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_27 (LeakyReLU)      (None, 26, 26, 512)  0           batch_normalization_99[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_20 (LeakyReLU)      (None, 52, 52, 256)  0           batch_normalization_92[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_109 (Conv2D)             (None, 13, 13, 18)   18450       leaky_re_lu_34[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_101 (Conv2D)             (None, 26, 26, 18)   9234        leaky_re_lu_27[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_93 (Conv2D)              (None, 52, 52, 18)   4626        leaky_re_lu_20[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 64,003,990\n",
      "Trainable params: 63,937,686\n",
      "Non-trainable params: 66,304\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from nets.yolo import yolo_body, get_train_model\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "model_body  = yolo_body((416, 416, 3), anchors_mask, num_classes)\n",
    "model_body.load_weights(model_path, by_name=True, skip_mismatch=True)\n",
    "\n",
    "model_body.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D, DepthwiseConv2D, Dense\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "for layer in model_body.layers:\n",
    "    if isinstance(layer, DepthwiseConv2D):\n",
    "            layer.add_loss(lambda: l2(0)(layer.depthwise_kernel))\n",
    "    elif isinstance(layer, Conv2D) or isinstance(layer, Dense):\n",
    "            layer.add_loss(lambda: l2(0)(layer.kernel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freeze the first 249 layers of total 370 layers.\n"
     ]
    }
   ],
   "source": [
    "freeze_layers = 249\n",
    "\n",
    "for i in range(freeze_layers):\n",
    "    model_body.layers[i].trainable = False\n",
    "    \n",
    "print('Freeze the first {} layers of total {} layers.'.format(freeze_layers, len(model_body.layers)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 建立訓練模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 416, 416, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 416, 416, 32) 864         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 416, 416, 32) 128         conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "mish (Mish)                     (None, 416, 416, 32) 0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d (ZeroPadding2D)  (None, 417, 417, 32) 0           mish[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 208, 208, 64) 18432       zero_padding2d[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 208, 208, 64) 256         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "mish_1 (Mish)                   (None, 208, 208, 64) 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 208, 208, 64) 4096        mish_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 208, 208, 64) 256         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "mish_3 (Mish)                   (None, 208, 208, 64) 0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 208, 208, 32) 2048        mish_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 208, 208, 32) 128         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "mish_4 (Mish)                   (None, 208, 208, 32) 0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 208, 208, 64) 18432       mish_4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 208, 208, 64) 256         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "mish_5 (Mish)                   (None, 208, 208, 64) 0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 208, 208, 64) 0           mish_3[0][0]                     \n",
      "                                                                 mish_5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 208, 208, 64) 4096        add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 208, 208, 64) 4096        mish_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 208, 208, 64) 256         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 208, 208, 64) 256         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "mish_6 (Mish)                   (None, 208, 208, 64) 0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "mish_2 (Mish)                   (None, 208, 208, 64) 0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 208, 208, 128 0           mish_6[0][0]                     \n",
      "                                                                 mish_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 208, 208, 64) 8192        concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 208, 208, 64) 256         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "mish_7 (Mish)                   (None, 208, 208, 64) 0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_1 (ZeroPadding2D (None, 209, 209, 64) 0           mish_7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 104, 104, 128 73728       zero_padding2d_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 104, 104, 128 512         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "mish_8 (Mish)                   (None, 104, 104, 128 0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 104, 104, 64) 8192        mish_8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 104, 104, 64) 256         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_10 (Mish)                  (None, 104, 104, 64) 0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 104, 104, 64) 4096        mish_10[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 104, 104, 64) 256         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_11 (Mish)                  (None, 104, 104, 64) 0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 104, 104, 64) 36864       mish_11[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 104, 104, 64) 256         conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_12 (Mish)                  (None, 104, 104, 64) 0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 104, 104, 64) 0           mish_10[0][0]                    \n",
      "                                                                 mish_12[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 104, 104, 64) 4096        add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 104, 104, 64) 256         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_13 (Mish)                  (None, 104, 104, 64) 0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 104, 104, 64) 36864       mish_13[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 104, 104, 64) 256         conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_14 (Mish)                  (None, 104, 104, 64) 0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 104, 104, 64) 0           add_1[0][0]                      \n",
      "                                                                 mish_14[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 104, 104, 64) 4096        add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 104, 104, 64) 8192        mish_8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 104, 104, 64) 256         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 104, 104, 64) 256         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "mish_15 (Mish)                  (None, 104, 104, 64) 0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mish_9 (Mish)                   (None, 104, 104, 64) 0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 104, 104, 128 0           mish_15[0][0]                    \n",
      "                                                                 mish_9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 104, 104, 128 16384       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 104, 104, 128 512         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_16 (Mish)                  (None, 104, 104, 128 0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_2 (ZeroPadding2D (None, 105, 105, 128 0           mish_16[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 52, 52, 256)  294912      zero_padding2d_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 52, 52, 256)  1024        conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_17 (Mish)                  (None, 52, 52, 256)  0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 52, 52, 128)  32768       mish_17[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 52, 52, 128)  512         conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_19 (Mish)                  (None, 52, 52, 128)  0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 52, 52, 128)  16384       mish_19[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 52, 52, 128)  512         conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_20 (Mish)                  (None, 52, 52, 128)  0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 52, 52, 128)  147456      mish_20[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 52, 52, 128)  512         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_21 (Mish)                  (None, 52, 52, 128)  0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 52, 52, 128)  0           mish_19[0][0]                    \n",
      "                                                                 mish_21[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 52, 52, 128)  16384       add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 52, 52, 128)  512         conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_22 (Mish)                  (None, 52, 52, 128)  0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 52, 52, 128)  147456      mish_22[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 52, 52, 128)  512         conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_23 (Mish)                  (None, 52, 52, 128)  0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 52, 52, 128)  0           add_3[0][0]                      \n",
      "                                                                 mish_23[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 52, 52, 128)  16384       add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 52, 52, 128)  512         conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_24 (Mish)                  (None, 52, 52, 128)  0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 52, 52, 128)  147456      mish_24[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 52, 52, 128)  512         conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_25 (Mish)                  (None, 52, 52, 128)  0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 52, 52, 128)  0           add_4[0][0]                      \n",
      "                                                                 mish_25[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 52, 52, 128)  16384       add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 52, 52, 128)  512         conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_26 (Mish)                  (None, 52, 52, 128)  0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 52, 52, 128)  147456      mish_26[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 52, 52, 128)  512         conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_27 (Mish)                  (None, 52, 52, 128)  0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 52, 52, 128)  0           add_5[0][0]                      \n",
      "                                                                 mish_27[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 52, 52, 128)  16384       add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 52, 52, 128)  512         conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_28 (Mish)                  (None, 52, 52, 128)  0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 52, 52, 128)  147456      mish_28[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 52, 52, 128)  512         conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_29 (Mish)                  (None, 52, 52, 128)  0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 52, 52, 128)  0           add_6[0][0]                      \n",
      "                                                                 mish_29[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 52, 52, 128)  16384       add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 52, 52, 128)  512         conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_30 (Mish)                  (None, 52, 52, 128)  0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 52, 52, 128)  147456      mish_30[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 52, 52, 128)  512         conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_31 (Mish)                  (None, 52, 52, 128)  0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 52, 52, 128)  0           add_7[0][0]                      \n",
      "                                                                 mish_31[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 52, 52, 128)  16384       add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 52, 52, 128)  512         conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_32 (Mish)                  (None, 52, 52, 128)  0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 52, 52, 128)  147456      mish_32[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 52, 52, 128)  512         conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_33 (Mish)                  (None, 52, 52, 128)  0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 52, 52, 128)  0           add_8[0][0]                      \n",
      "                                                                 mish_33[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 52, 52, 128)  16384       add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 52, 52, 128)  512         conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_34 (Mish)                  (None, 52, 52, 128)  0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 52, 52, 128)  147456      mish_34[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 52, 52, 128)  512         conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_35 (Mish)                  (None, 52, 52, 128)  0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 52, 52, 128)  0           add_9[0][0]                      \n",
      "                                                                 mish_35[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 52, 52, 128)  16384       add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 52, 52, 128)  32768       mish_17[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 52, 52, 128)  512         conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 52, 52, 128)  512         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_36 (Mish)                  (None, 52, 52, 128)  0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mish_18 (Mish)                  (None, 52, 52, 128)  0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 52, 52, 256)  0           mish_36[0][0]                    \n",
      "                                                                 mish_18[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 52, 52, 256)  65536       concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 52, 52, 256)  1024        conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_37 (Mish)                  (None, 52, 52, 256)  0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_3 (ZeroPadding2D (None, 53, 53, 256)  0           mish_37[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 26, 26, 512)  1179648     zero_padding2d_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 26, 26, 512)  2048        conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_38 (Mish)                  (None, 26, 26, 512)  0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, 26, 26, 256)  131072      mish_38[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 26, 26, 256)  1024        conv2d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_40 (Mish)                  (None, 26, 26, 256)  0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, 26, 26, 256)  65536       mish_40[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 26, 26, 256)  1024        conv2d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_41 (Mish)                  (None, 26, 26, 256)  0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)              (None, 26, 26, 256)  589824      mish_41[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 26, 26, 256)  1024        conv2d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_42 (Mish)                  (None, 26, 26, 256)  0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 26, 26, 256)  0           mish_40[0][0]                    \n",
      "                                                                 mish_42[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_43 (Conv2D)              (None, 26, 26, 256)  65536       add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_43 (BatchNo (None, 26, 26, 256)  1024        conv2d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_43 (Mish)                  (None, 26, 26, 256)  0           batch_normalization_43[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)              (None, 26, 26, 256)  589824      mish_43[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_44 (BatchNo (None, 26, 26, 256)  1024        conv2d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_44 (Mish)                  (None, 26, 26, 256)  0           batch_normalization_44[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 26, 26, 256)  0           add_11[0][0]                     \n",
      "                                                                 mish_44[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_45 (Conv2D)              (None, 26, 26, 256)  65536       add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_45 (BatchNo (None, 26, 26, 256)  1024        conv2d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_45 (Mish)                  (None, 26, 26, 256)  0           batch_normalization_45[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_46 (Conv2D)              (None, 26, 26, 256)  589824      mish_45[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_46 (BatchNo (None, 26, 26, 256)  1024        conv2d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_46 (Mish)                  (None, 26, 26, 256)  0           batch_normalization_46[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, 26, 26, 256)  0           add_12[0][0]                     \n",
      "                                                                 mish_46[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_47 (Conv2D)              (None, 26, 26, 256)  65536       add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_47 (BatchNo (None, 26, 26, 256)  1024        conv2d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_47 (Mish)                  (None, 26, 26, 256)  0           batch_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_48 (Conv2D)              (None, 26, 26, 256)  589824      mish_47[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_48 (BatchNo (None, 26, 26, 256)  1024        conv2d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_48 (Mish)                  (None, 26, 26, 256)  0           batch_normalization_48[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_14 (Add)                    (None, 26, 26, 256)  0           add_13[0][0]                     \n",
      "                                                                 mish_48[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_49 (Conv2D)              (None, 26, 26, 256)  65536       add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_49 (BatchNo (None, 26, 26, 256)  1024        conv2d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_49 (Mish)                  (None, 26, 26, 256)  0           batch_normalization_49[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_50 (Conv2D)              (None, 26, 26, 256)  589824      mish_49[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_50 (BatchNo (None, 26, 26, 256)  1024        conv2d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_50 (Mish)                  (None, 26, 26, 256)  0           batch_normalization_50[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_15 (Add)                    (None, 26, 26, 256)  0           add_14[0][0]                     \n",
      "                                                                 mish_50[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_51 (Conv2D)              (None, 26, 26, 256)  65536       add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_51 (BatchNo (None, 26, 26, 256)  1024        conv2d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_51 (Mish)                  (None, 26, 26, 256)  0           batch_normalization_51[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_52 (Conv2D)              (None, 26, 26, 256)  589824      mish_51[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_52 (BatchNo (None, 26, 26, 256)  1024        conv2d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_52 (Mish)                  (None, 26, 26, 256)  0           batch_normalization_52[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_16 (Add)                    (None, 26, 26, 256)  0           add_15[0][0]                     \n",
      "                                                                 mish_52[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_53 (Conv2D)              (None, 26, 26, 256)  65536       add_16[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_53 (BatchNo (None, 26, 26, 256)  1024        conv2d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_53 (Mish)                  (None, 26, 26, 256)  0           batch_normalization_53[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_54 (Conv2D)              (None, 26, 26, 256)  589824      mish_53[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_54 (BatchNo (None, 26, 26, 256)  1024        conv2d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_54 (Mish)                  (None, 26, 26, 256)  0           batch_normalization_54[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_17 (Add)                    (None, 26, 26, 256)  0           add_16[0][0]                     \n",
      "                                                                 mish_54[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_55 (Conv2D)              (None, 26, 26, 256)  65536       add_17[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_55 (BatchNo (None, 26, 26, 256)  1024        conv2d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_55 (Mish)                  (None, 26, 26, 256)  0           batch_normalization_55[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_56 (Conv2D)              (None, 26, 26, 256)  589824      mish_55[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_56 (BatchNo (None, 26, 26, 256)  1024        conv2d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_56 (Mish)                  (None, 26, 26, 256)  0           batch_normalization_56[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_18 (Add)                    (None, 26, 26, 256)  0           add_17[0][0]                     \n",
      "                                                                 mish_56[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_57 (Conv2D)              (None, 26, 26, 256)  65536       add_18[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 26, 26, 256)  131072      mish_38[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_57 (BatchNo (None, 26, 26, 256)  1024        conv2d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 26, 26, 256)  1024        conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_57 (Mish)                  (None, 26, 26, 256)  0           batch_normalization_57[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mish_39 (Mish)                  (None, 26, 26, 256)  0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 26, 26, 512)  0           mish_57[0][0]                    \n",
      "                                                                 mish_39[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_58 (Conv2D)              (None, 26, 26, 512)  262144      concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_58 (BatchNo (None, 26, 26, 512)  2048        conv2d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_58 (Mish)                  (None, 26, 26, 512)  0           batch_normalization_58[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_4 (ZeroPadding2D (None, 27, 27, 512)  0           mish_58[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_59 (Conv2D)              (None, 13, 13, 1024) 4718592     zero_padding2d_4[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_59 (BatchNo (None, 13, 13, 1024) 4096        conv2d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_59 (Mish)                  (None, 13, 13, 1024) 0           batch_normalization_59[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_61 (Conv2D)              (None, 13, 13, 512)  524288      mish_59[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_61 (BatchNo (None, 13, 13, 512)  2048        conv2d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_61 (Mish)                  (None, 13, 13, 512)  0           batch_normalization_61[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_62 (Conv2D)              (None, 13, 13, 512)  262144      mish_61[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_62 (BatchNo (None, 13, 13, 512)  2048        conv2d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_62 (Mish)                  (None, 13, 13, 512)  0           batch_normalization_62[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_63 (Conv2D)              (None, 13, 13, 512)  2359296     mish_62[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_63 (BatchNo (None, 13, 13, 512)  2048        conv2d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_63 (Mish)                  (None, 13, 13, 512)  0           batch_normalization_63[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_19 (Add)                    (None, 13, 13, 512)  0           mish_61[0][0]                    \n",
      "                                                                 mish_63[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_64 (Conv2D)              (None, 13, 13, 512)  262144      add_19[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_64 (BatchNo (None, 13, 13, 512)  2048        conv2d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_64 (Mish)                  (None, 13, 13, 512)  0           batch_normalization_64[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_65 (Conv2D)              (None, 13, 13, 512)  2359296     mish_64[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_65 (BatchNo (None, 13, 13, 512)  2048        conv2d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_65 (Mish)                  (None, 13, 13, 512)  0           batch_normalization_65[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_20 (Add)                    (None, 13, 13, 512)  0           add_19[0][0]                     \n",
      "                                                                 mish_65[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_66 (Conv2D)              (None, 13, 13, 512)  262144      add_20[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_66 (BatchNo (None, 13, 13, 512)  2048        conv2d_66[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_66 (Mish)                  (None, 13, 13, 512)  0           batch_normalization_66[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_67 (Conv2D)              (None, 13, 13, 512)  2359296     mish_66[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_67 (BatchNo (None, 13, 13, 512)  2048        conv2d_67[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_67 (Mish)                  (None, 13, 13, 512)  0           batch_normalization_67[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_21 (Add)                    (None, 13, 13, 512)  0           add_20[0][0]                     \n",
      "                                                                 mish_67[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_68 (Conv2D)              (None, 13, 13, 512)  262144      add_21[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_68 (BatchNo (None, 13, 13, 512)  2048        conv2d_68[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_68 (Mish)                  (None, 13, 13, 512)  0           batch_normalization_68[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_69 (Conv2D)              (None, 13, 13, 512)  2359296     mish_68[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_69 (BatchNo (None, 13, 13, 512)  2048        conv2d_69[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_69 (Mish)                  (None, 13, 13, 512)  0           batch_normalization_69[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_22 (Add)                    (None, 13, 13, 512)  0           add_21[0][0]                     \n",
      "                                                                 mish_69[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_70 (Conv2D)              (None, 13, 13, 512)  262144      add_22[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_60 (Conv2D)              (None, 13, 13, 512)  524288      mish_59[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_70 (BatchNo (None, 13, 13, 512)  2048        conv2d_70[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_60 (BatchNo (None, 13, 13, 512)  2048        conv2d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_70 (Mish)                  (None, 13, 13, 512)  0           batch_normalization_70[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mish_60 (Mish)                  (None, 13, 13, 512)  0           batch_normalization_60[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 13, 13, 1024) 0           mish_70[0][0]                    \n",
      "                                                                 mish_60[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_71 (Conv2D)              (None, 13, 13, 1024) 1048576     concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_71 (BatchNo (None, 13, 13, 1024) 4096        conv2d_71[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mish_71 (Mish)                  (None, 13, 13, 1024) 0           batch_normalization_71[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_72 (Conv2D)              (None, 13, 13, 512)  524288      mish_71[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_72 (BatchNo (None, 13, 13, 512)  2048        conv2d_72[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)         (None, 13, 13, 512)  0           batch_normalization_72[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_73 (Conv2D)              (None, 13, 13, 1024) 4718592     leaky_re_lu[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_73 (BatchNo (None, 13, 13, 1024) 4096        conv2d_73[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, 13, 13, 1024) 0           batch_normalization_73[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_74 (Conv2D)              (None, 13, 13, 512)  524288      leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_74 (BatchNo (None, 13, 13, 512)  2048        conv2d_74[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, 13, 13, 512)  0           batch_normalization_74[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 13, 13, 512)  0           leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 13, 13, 512)  0           leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 13, 13, 512)  0           leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 13, 13, 2048) 0           max_pooling2d[0][0]              \n",
      "                                                                 max_pooling2d_1[0][0]            \n",
      "                                                                 max_pooling2d_2[0][0]            \n",
      "                                                                 leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_75 (Conv2D)              (None, 13, 13, 512)  1048576     concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_75 (BatchNo (None, 13, 13, 512)  2048        conv2d_75[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)       (None, 13, 13, 512)  0           batch_normalization_75[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_76 (Conv2D)              (None, 13, 13, 1024) 4718592     leaky_re_lu_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_76 (BatchNo (None, 13, 13, 1024) 4096        conv2d_76[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)       (None, 13, 13, 1024) 0           batch_normalization_76[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_77 (Conv2D)              (None, 13, 13, 512)  524288      leaky_re_lu_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_77 (BatchNo (None, 13, 13, 512)  2048        conv2d_77[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)       (None, 13, 13, 512)  0           batch_normalization_77[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_78 (Conv2D)              (None, 13, 13, 256)  131072      leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_79 (Conv2D)              (None, 26, 26, 256)  131072      mish_58[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_78 (BatchNo (None, 13, 13, 256)  1024        conv2d_78[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_79 (BatchNo (None, 26, 26, 256)  1024        conv2d_79[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)       (None, 13, 13, 256)  0           batch_normalization_78[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)       (None, 26, 26, 256)  0           batch_normalization_79[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d (UpSampling2D)    (None, 26, 26, 256)  0           leaky_re_lu_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 26, 26, 512)  0           leaky_re_lu_7[0][0]              \n",
      "                                                                 up_sampling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_80 (Conv2D)              (None, 26, 26, 256)  131072      concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_80 (BatchNo (None, 26, 26, 256)  1024        conv2d_80[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)       (None, 26, 26, 256)  0           batch_normalization_80[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_81 (Conv2D)              (None, 26, 26, 512)  1179648     leaky_re_lu_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_81 (BatchNo (None, 26, 26, 512)  2048        conv2d_81[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)       (None, 26, 26, 512)  0           batch_normalization_81[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_82 (Conv2D)              (None, 26, 26, 256)  131072      leaky_re_lu_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_82 (BatchNo (None, 26, 26, 256)  1024        conv2d_82[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)      (None, 26, 26, 256)  0           batch_normalization_82[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_83 (Conv2D)              (None, 26, 26, 512)  1179648     leaky_re_lu_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_83 (BatchNo (None, 26, 26, 512)  2048        conv2d_83[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)      (None, 26, 26, 512)  0           batch_normalization_83[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_84 (Conv2D)              (None, 26, 26, 256)  131072      leaky_re_lu_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_84 (BatchNo (None, 26, 26, 256)  1024        conv2d_84[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)      (None, 26, 26, 256)  0           batch_normalization_84[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_85 (Conv2D)              (None, 26, 26, 128)  32768       leaky_re_lu_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_86 (Conv2D)              (None, 52, 52, 128)  32768       mish_37[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_85 (BatchNo (None, 26, 26, 128)  512         conv2d_85[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_86 (BatchNo (None, 52, 52, 128)  512         conv2d_86[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_13 (LeakyReLU)      (None, 26, 26, 128)  0           batch_normalization_85[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_14 (LeakyReLU)      (None, 52, 52, 128)  0           batch_normalization_86[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2D)  (None, 52, 52, 128)  0           leaky_re_lu_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 52, 52, 256)  0           leaky_re_lu_14[0][0]             \n",
      "                                                                 up_sampling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_87 (Conv2D)              (None, 52, 52, 128)  32768       concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_87 (BatchNo (None, 52, 52, 128)  512         conv2d_87[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_15 (LeakyReLU)      (None, 52, 52, 128)  0           batch_normalization_87[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_88 (Conv2D)              (None, 52, 52, 256)  294912      leaky_re_lu_15[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_88 (BatchNo (None, 52, 52, 256)  1024        conv2d_88[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_16 (LeakyReLU)      (None, 52, 52, 256)  0           batch_normalization_88[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_89 (Conv2D)              (None, 52, 52, 128)  32768       leaky_re_lu_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_89 (BatchNo (None, 52, 52, 128)  512         conv2d_89[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_17 (LeakyReLU)      (None, 52, 52, 128)  0           batch_normalization_89[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_90 (Conv2D)              (None, 52, 52, 256)  294912      leaky_re_lu_17[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_90 (BatchNo (None, 52, 52, 256)  1024        conv2d_90[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_18 (LeakyReLU)      (None, 52, 52, 256)  0           batch_normalization_90[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_91 (Conv2D)              (None, 52, 52, 128)  32768       leaky_re_lu_18[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_91 (BatchNo (None, 52, 52, 128)  512         conv2d_91[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_19 (LeakyReLU)      (None, 52, 52, 128)  0           batch_normalization_91[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_5 (ZeroPadding2D (None, 53, 53, 128)  0           leaky_re_lu_19[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_94 (Conv2D)              (None, 26, 26, 256)  294912      zero_padding2d_5[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_93 (BatchNo (None, 26, 26, 256)  1024        conv2d_94[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_21 (LeakyReLU)      (None, 26, 26, 256)  0           batch_normalization_93[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 26, 26, 512)  0           leaky_re_lu_21[0][0]             \n",
      "                                                                 leaky_re_lu_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_95 (Conv2D)              (None, 26, 26, 256)  131072      concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_94 (BatchNo (None, 26, 26, 256)  1024        conv2d_95[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_22 (LeakyReLU)      (None, 26, 26, 256)  0           batch_normalization_94[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_96 (Conv2D)              (None, 26, 26, 512)  1179648     leaky_re_lu_22[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_95 (BatchNo (None, 26, 26, 512)  2048        conv2d_96[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_23 (LeakyReLU)      (None, 26, 26, 512)  0           batch_normalization_95[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_97 (Conv2D)              (None, 26, 26, 256)  131072      leaky_re_lu_23[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_96 (BatchNo (None, 26, 26, 256)  1024        conv2d_97[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_24 (LeakyReLU)      (None, 26, 26, 256)  0           batch_normalization_96[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_98 (Conv2D)              (None, 26, 26, 512)  1179648     leaky_re_lu_24[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_97 (BatchNo (None, 26, 26, 512)  2048        conv2d_98[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_25 (LeakyReLU)      (None, 26, 26, 512)  0           batch_normalization_97[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_99 (Conv2D)              (None, 26, 26, 256)  131072      leaky_re_lu_25[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_98 (BatchNo (None, 26, 26, 256)  1024        conv2d_99[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_26 (LeakyReLU)      (None, 26, 26, 256)  0           batch_normalization_98[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_6 (ZeroPadding2D (None, 27, 27, 256)  0           leaky_re_lu_26[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_102 (Conv2D)             (None, 13, 13, 512)  1179648     zero_padding2d_6[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_100 (BatchN (None, 13, 13, 512)  2048        conv2d_102[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_28 (LeakyReLU)      (None, 13, 13, 512)  0           batch_normalization_100[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 13, 13, 1024) 0           leaky_re_lu_28[0][0]             \n",
      "                                                                 leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_103 (Conv2D)             (None, 13, 13, 512)  524288      concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_101 (BatchN (None, 13, 13, 512)  2048        conv2d_103[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_29 (LeakyReLU)      (None, 13, 13, 512)  0           batch_normalization_101[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_104 (Conv2D)             (None, 13, 13, 1024) 4718592     leaky_re_lu_29[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_102 (BatchN (None, 13, 13, 1024) 4096        conv2d_104[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_30 (LeakyReLU)      (None, 13, 13, 1024) 0           batch_normalization_102[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_105 (Conv2D)             (None, 13, 13, 512)  524288      leaky_re_lu_30[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_103 (BatchN (None, 13, 13, 512)  2048        conv2d_105[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_31 (LeakyReLU)      (None, 13, 13, 512)  0           batch_normalization_103[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_106 (Conv2D)             (None, 13, 13, 1024) 4718592     leaky_re_lu_31[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_104 (BatchN (None, 13, 13, 1024) 4096        conv2d_106[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_32 (LeakyReLU)      (None, 13, 13, 1024) 0           batch_normalization_104[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_107 (Conv2D)             (None, 13, 13, 512)  524288      leaky_re_lu_32[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_105 (BatchN (None, 13, 13, 512)  2048        conv2d_107[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_33 (LeakyReLU)      (None, 13, 13, 512)  0           batch_normalization_105[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_108 (Conv2D)             (None, 13, 13, 1024) 4718592     leaky_re_lu_33[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_100 (Conv2D)             (None, 26, 26, 512)  1179648     leaky_re_lu_26[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_92 (Conv2D)              (None, 52, 52, 256)  294912      leaky_re_lu_19[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_106 (BatchN (None, 13, 13, 1024) 4096        conv2d_108[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_99 (BatchNo (None, 26, 26, 512)  2048        conv2d_100[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_92 (BatchNo (None, 52, 52, 256)  1024        conv2d_92[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_34 (LeakyReLU)      (None, 13, 13, 1024) 0           batch_normalization_106[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_27 (LeakyReLU)      (None, 26, 26, 512)  0           batch_normalization_99[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_20 (LeakyReLU)      (None, 52, 52, 256)  0           batch_normalization_92[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_109 (Conv2D)             (None, 13, 13, 18)   18450       leaky_re_lu_34[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_101 (Conv2D)             (None, 26, 26, 18)   9234        leaky_re_lu_27[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_93 (Conv2D)              (None, 52, 52, 18)   4626        leaky_re_lu_20[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 13, 13, 3, 6 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, 26, 26, 3, 6 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            [(None, 52, 52, 3, 6 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "yolo_loss (Lambda)              ()                   0           conv2d_109[0][0]                 \n",
      "                                                                 conv2d_101[0][0]                 \n",
      "                                                                 conv2d_93[0][0]                  \n",
      "                                                                 input_2[0][0]                    \n",
      "                                                                 input_3[0][0]                    \n",
      "                                                                 input_4[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 64,003,990\n",
      "Trainable params: 37,320,502\n",
      "Non-trainable params: 26,683,488\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = get_train_model(model_body, input_shape, num_classes, anchors, anchors_mask, label_smoothing=0, focal_loss=False, alpha=0.25, gamma=2)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import optimizers\n",
    "opt = optimizers.Adam(lr = lr, beta_1 = 0.937)\n",
    "\n",
    "model.compile(optimizer = opt, loss={'yolo_loss': lambda y_true, y_pred: y_pred})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, History, TensorBoard, LearningRateScheduler\n",
    "from nets.yolo_training import get_lr_scheduler\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint('logs/best_epoch_weights.h5', monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "early_stopping  = EarlyStopping(monitor='val_loss', min_delta = 0, patience = 100, verbose = 1)\n",
    "\n",
    "lr_scheduler_func = get_lr_scheduler('cos', Init_lr_fit, Min_lr_fit, total_iters=Max_epoch)\n",
    "lr_scheduler    = LearningRateScheduler(lr_scheduler_func, verbose = 1)\n",
    "\n",
    "callbacks = [checkpoint_callback, lr_scheduler, early_stopping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 2.9999999999999997e-05.\n",
      "Epoch 1/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 19.3015\n",
      "Epoch 00001: val_loss improved from inf to 18.76326, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 8s 3s/step - loss: 19.3015 - val_loss: 18.7633\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 5.9999999999999995e-05.\n",
      "Epoch 2/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 19.0726\n",
      "Epoch 00002: val_loss improved from 18.76326 to 18.75138, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 19.0726 - val_loss: 18.7514\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.00014999999999999996.\n",
      "Epoch 3/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 18.5694\n",
      "Epoch 00003: val_loss improved from 18.75138 to 18.69430, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 18.5694 - val_loss: 18.6943\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.0002999999999999999.\n",
      "Epoch 4/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 17.4547\n",
      "Epoch 00004: val_loss improved from 18.69430 to 18.54377, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 17.4547 - val_loss: 18.5438\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.00029999999264536593.\n",
      "Epoch 5/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 15.8519\n",
      "Epoch 00005: val_loss improved from 18.54377 to 18.40425, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 15.8519 - val_loss: 18.4043\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.00029999997058146444.\n",
      "Epoch 6/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 14.1836\n",
      "Epoch 00006: val_loss improved from 18.40425 to 18.24412, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 14.1836 - val_loss: 18.2441\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.0002999999338082978.\n",
      "Epoch 7/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 12.6093\n",
      "Epoch 00007: val_loss improved from 18.24412 to 18.05679, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 12.6093 - val_loss: 18.0568\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.0002999998823258696.\n",
      "Epoch 8/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 11.2294\n",
      "Epoch 00008: val_loss improved from 18.05679 to 17.86584, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 11.2294 - val_loss: 17.8658\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.0002999998161341849.\n",
      "Epoch 9/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 10.0927\n",
      "Epoch 00009: val_loss improved from 17.86584 to 17.67826, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 10.0927 - val_loss: 17.6783\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.00029999973523325027.\n",
      "Epoch 10/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 9.1658\n",
      "Epoch 00010: val_loss improved from 17.67826 to 17.48627, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 9.1658 - val_loss: 17.4863\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 0.00029999963962307376.\n",
      "Epoch 11/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 8.3888\n",
      "Epoch 00011: val_loss improved from 17.48627 to 17.29699, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 8.3888 - val_loss: 17.2970\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 0.0002999995293036648.\n",
      "Epoch 12/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 7.7065\n",
      "Epoch 00012: val_loss improved from 17.29699 to 17.10353, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 7.7065 - val_loss: 17.1035\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 0.0002999994042750344.\n",
      "Epoch 13/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 7.1268\n",
      "Epoch 00013: val_loss improved from 17.10353 to 16.90597, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 7.1268 - val_loss: 16.9060\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 0.0002999992645371949.\n",
      "Epoch 14/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 6.6015\n",
      "Epoch 00014: val_loss improved from 16.90597 to 16.69410, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 6.6015 - val_loss: 16.6941\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 0.0002999991100901601.\n",
      "Epoch 15/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 6.1582\n",
      "Epoch 00015: val_loss improved from 16.69410 to 16.46752, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 6.1582 - val_loss: 16.4675\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 0.0002999989409339453.\n",
      "Epoch 16/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 5.7145\n",
      "Epoch 00016: val_loss improved from 16.46752 to 16.21968, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 5.7145 - val_loss: 16.2197\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 0.0002999987570685673.\n",
      "Epoch 17/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 5.3659\n",
      "Epoch 00017: val_loss improved from 16.21968 to 15.95502, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 5.3659 - val_loss: 15.9550\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 0.0002999985584940443.\n",
      "Epoch 18/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 4.9813\n",
      "Epoch 00018: val_loss improved from 15.95502 to 15.67383, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 4.9813 - val_loss: 15.6738\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 0.0002999983452103959.\n",
      "Epoch 19/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 4.6803\n",
      "Epoch 00019: val_loss improved from 15.67383 to 15.37584, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 4.6803 - val_loss: 15.3758\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to 0.00029999811721764336.\n",
      "Epoch 20/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 4.3737\n",
      "Epoch 00020: val_loss improved from 15.37584 to 15.06657, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 4.3737 - val_loss: 15.0666\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to 0.0002999978745158092.\n",
      "Epoch 21/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 4.0894\n",
      "Epoch 00021: val_loss improved from 15.06657 to 14.75306, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 4.0894 - val_loss: 14.7531\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to 0.0002999976171049174.\n",
      "Epoch 22/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 3.8562\n",
      "Epoch 00022: val_loss improved from 14.75306 to 14.43169, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 3.8562 - val_loss: 14.4317\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to 0.00029999734498499357.\n",
      "Epoch 23/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 3.6339\n",
      "Epoch 00023: val_loss improved from 14.43169 to 14.10128, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 3.6339 - val_loss: 14.1013\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to 0.0002999970581560646.\n",
      "Epoch 24/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 3.4211\n",
      "Epoch 00024: val_loss improved from 14.10128 to 13.76552, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 3.4211 - val_loss: 13.7655\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to 0.0002999967566181588.\n",
      "Epoch 25/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 3.2151\n",
      "Epoch 00025: val_loss improved from 13.76552 to 13.42180, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 3.2151 - val_loss: 13.4218\n",
      "\n",
      "Epoch 00026: LearningRateScheduler reducing learning rate to 0.0002999964403713063.\n",
      "Epoch 26/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 3.0800\n",
      "Epoch 00026: val_loss improved from 13.42180 to 13.07640, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 3.0800 - val_loss: 13.0764\n",
      "\n",
      "Epoch 00027: LearningRateScheduler reducing learning rate to 0.0002999961094155381.\n",
      "Epoch 27/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 2.9047\n",
      "Epoch 00027: val_loss improved from 13.07640 to 12.71708, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 2.9047 - val_loss: 12.7171\n",
      "\n",
      "Epoch 00028: LearningRateScheduler reducing learning rate to 0.00029999576375088724.\n",
      "Epoch 28/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 2.7418\n",
      "Epoch 00028: val_loss improved from 12.71708 to 12.34754, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 2.7418 - val_loss: 12.3475\n",
      "\n",
      "Epoch 00029: LearningRateScheduler reducing learning rate to 0.00029999540337738786.\n",
      "Epoch 29/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 2.6164\n",
      "Epoch 00029: val_loss improved from 12.34754 to 11.98621, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 2.6164 - val_loss: 11.9862\n",
      "\n",
      "Epoch 00030: LearningRateScheduler reducing learning rate to 0.0002999950282950757.\n",
      "Epoch 30/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 2.4650\n",
      "Epoch 00030: val_loss improved from 11.98621 to 11.62487, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 2.4650 - val_loss: 11.6249\n",
      "\n",
      "Epoch 00031: LearningRateScheduler reducing learning rate to 0.00029999463850398784.\n",
      "Epoch 31/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 2.3559\n",
      "Epoch 00031: val_loss improved from 11.62487 to 11.26233, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 2.3559 - val_loss: 11.2623\n",
      "\n",
      "Epoch 00032: LearningRateScheduler reducing learning rate to 0.00029999423400416293.\n",
      "Epoch 32/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 2.2608\n",
      "Epoch 00032: val_loss improved from 11.26233 to 10.90999, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 2.2608 - val_loss: 10.9100\n",
      "\n",
      "Epoch 00033: LearningRateScheduler reducing learning rate to 0.00029999381479564103.\n",
      "Epoch 33/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 2.1305\n",
      "Epoch 00033: val_loss improved from 10.90999 to 10.56721, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 2.1305 - val_loss: 10.5672\n",
      "\n",
      "Epoch 00034: LearningRateScheduler reducing learning rate to 0.00029999338087846367.\n",
      "Epoch 34/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 2.0420\n",
      "Epoch 00034: val_loss improved from 10.56721 to 10.22378, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 2.0420 - val_loss: 10.2238\n",
      "\n",
      "Epoch 00035: LearningRateScheduler reducing learning rate to 0.00029999293225267383.\n",
      "Epoch 35/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 1.9621\n",
      "Epoch 00035: val_loss improved from 10.22378 to 9.88439, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 1.9621 - val_loss: 9.8844\n",
      "\n",
      "Epoch 00036: LearningRateScheduler reducing learning rate to 0.0002999924689183159.\n",
      "Epoch 36/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 1.8636\n",
      "Epoch 00036: val_loss improved from 9.88439 to 9.55724, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 1.8636 - val_loss: 9.5572\n",
      "\n",
      "Epoch 00037: LearningRateScheduler reducing learning rate to 0.0002999919908754359.\n",
      "Epoch 37/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 1.7884\n",
      "Epoch 00037: val_loss improved from 9.55724 to 9.23759, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 1.7884 - val_loss: 9.2376\n",
      "\n",
      "Epoch 00038: LearningRateScheduler reducing learning rate to 0.0002999914981240811.\n",
      "Epoch 38/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 1.7148\n",
      "Epoch 00038: val_loss improved from 9.23759 to 8.93518, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 1.7148 - val_loss: 8.9352\n",
      "\n",
      "Epoch 00039: LearningRateScheduler reducing learning rate to 0.00029999099066430025.\n",
      "Epoch 39/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 1.6611\n",
      "Epoch 00039: val_loss improved from 8.93518 to 8.63773, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 1.6611 - val_loss: 8.6377\n",
      "\n",
      "Epoch 00040: LearningRateScheduler reducing learning rate to 0.00029999046849614366.\n",
      "Epoch 40/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 1.6035\n",
      "Epoch 00040: val_loss improved from 8.63773 to 8.35491, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 1.6035 - val_loss: 8.3549\n",
      "\n",
      "Epoch 00041: LearningRateScheduler reducing learning rate to 0.0002999899316196631.\n",
      "Epoch 41/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 1.5234\n",
      "Epoch 00041: val_loss improved from 8.35491 to 8.08027, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 1.5234 - val_loss: 8.0803\n",
      "\n",
      "Epoch 00042: LearningRateScheduler reducing learning rate to 0.00029998938003491176.\n",
      "Epoch 42/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 1.4702\n",
      "Epoch 00042: val_loss improved from 8.08027 to 7.80966, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 1.4702 - val_loss: 7.8097\n",
      "\n",
      "Epoch 00043: LearningRateScheduler reducing learning rate to 0.00029998881374194416.\n",
      "Epoch 43/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 1.4007\n",
      "Epoch 00043: val_loss improved from 7.80966 to 7.55187, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 1.4007 - val_loss: 7.5519\n",
      "\n",
      "Epoch 00044: LearningRateScheduler reducing learning rate to 0.00029998823274081653.\n",
      "Epoch 44/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 1.3873\n",
      "Epoch 00044: val_loss improved from 7.55187 to 7.29657, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 1.3873 - val_loss: 7.2966\n",
      "\n",
      "Epoch 00045: LearningRateScheduler reducing learning rate to 0.00029998763703158627.\n",
      "Epoch 45/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 1.3373\n",
      "Epoch 00045: val_loss improved from 7.29657 to 7.04761, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 1.3373 - val_loss: 7.0476\n",
      "\n",
      "Epoch 00046: LearningRateScheduler reducing learning rate to 0.0002999870266143125.\n",
      "Epoch 46/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 1.2930\n",
      "Epoch 00046: val_loss improved from 7.04761 to 6.81568, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 1.2930 - val_loss: 6.8157\n",
      "\n",
      "Epoch 00047: LearningRateScheduler reducing learning rate to 0.0002999864014890557.\n",
      "Epoch 47/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 1.2412\n",
      "Epoch 00047: val_loss improved from 6.81568 to 6.59122, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 1.2412 - val_loss: 6.5912\n",
      "\n",
      "Epoch 00048: LearningRateScheduler reducing learning rate to 0.00029998576165587765.\n",
      "Epoch 48/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 1.2073\n",
      "Epoch 00048: val_loss improved from 6.59122 to 6.38073, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 1.2073 - val_loss: 6.3807\n",
      "\n",
      "Epoch 00049: LearningRateScheduler reducing learning rate to 0.0002999851071148419.\n",
      "Epoch 49/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 1.1729\n",
      "Epoch 00049: val_loss improved from 6.38073 to 6.18266, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 1.1729 - val_loss: 6.1827\n",
      "\n",
      "Epoch 00050: LearningRateScheduler reducing learning rate to 0.0002999844378660132.\n",
      "Epoch 50/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 1.1335\n",
      "Epoch 00050: val_loss improved from 6.18266 to 5.98941, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 1.1335 - val_loss: 5.9894\n",
      "\n",
      "Epoch 00051: LearningRateScheduler reducing learning rate to 0.0002999837539094578.\n",
      "Epoch 51/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 1.0826\n",
      "Epoch 00051: val_loss improved from 5.98941 to 5.80246, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 1.0826 - val_loss: 5.8025\n",
      "\n",
      "Epoch 00052: LearningRateScheduler reducing learning rate to 0.0002999830552452435.\n",
      "Epoch 52/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 1.0689\n",
      "Epoch 00052: val_loss improved from 5.80246 to 5.61548, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 1.0689 - val_loss: 5.6155\n",
      "\n",
      "Epoch 00053: LearningRateScheduler reducing learning rate to 0.0002999823418734395.\n",
      "Epoch 53/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 1.0397\n",
      "Epoch 00053: val_loss improved from 5.61548 to 5.42701, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 1.0397 - val_loss: 5.4270\n",
      "\n",
      "Epoch 00054: LearningRateScheduler reducing learning rate to 0.0002999816137941165.\n",
      "Epoch 54/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 1.0024\n",
      "Epoch 00054: val_loss improved from 5.42701 to 5.25358, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 1.0024 - val_loss: 5.2536\n",
      "\n",
      "Epoch 00055: LearningRateScheduler reducing learning rate to 0.00029998087100734647.\n",
      "Epoch 55/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.9985\n",
      "Epoch 00055: val_loss improved from 5.25358 to 5.08533, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.9985 - val_loss: 5.0853\n",
      "\n",
      "Epoch 00056: LearningRateScheduler reducing learning rate to 0.0002999801135132032.\n",
      "Epoch 56/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.9478\n",
      "Epoch 00056: val_loss improved from 5.08533 to 4.91860, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.9478 - val_loss: 4.9186\n",
      "\n",
      "Epoch 00057: LearningRateScheduler reducing learning rate to 0.00029997934131176154.\n",
      "Epoch 57/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.9220\n",
      "Epoch 00057: val_loss improved from 4.91860 to 4.75444, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.9220 - val_loss: 4.7544\n",
      "\n",
      "Epoch 00058: LearningRateScheduler reducing learning rate to 0.0002999785544030981.\n",
      "Epoch 58/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.8753\n",
      "Epoch 00058: val_loss improved from 4.75444 to 4.60002, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.8753 - val_loss: 4.6000\n",
      "\n",
      "Epoch 00059: LearningRateScheduler reducing learning rate to 0.0002999777527872907.\n",
      "Epoch 59/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.8817\n",
      "Epoch 00059: val_loss improved from 4.60002 to 4.45432, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.8817 - val_loss: 4.4543\n",
      "\n",
      "Epoch 00060: LearningRateScheduler reducing learning rate to 0.0002999769364644189.\n",
      "Epoch 60/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.8686\n",
      "Epoch 00060: val_loss improved from 4.45432 to 4.31416, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.8686 - val_loss: 4.3142\n",
      "\n",
      "Epoch 00061: LearningRateScheduler reducing learning rate to 0.00029997610543456345.\n",
      "Epoch 61/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.8378\n",
      "Epoch 00061: val_loss improved from 4.31416 to 4.16398, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.8378 - val_loss: 4.1640\n",
      "\n",
      "Epoch 00062: LearningRateScheduler reducing learning rate to 0.0002999752596978067.\n",
      "Epoch 62/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.8255\n",
      "Epoch 00062: val_loss improved from 4.16398 to 4.01929, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.8255 - val_loss: 4.0193\n",
      "\n",
      "Epoch 00063: LearningRateScheduler reducing learning rate to 0.0002999743992542324.\n",
      "Epoch 63/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.8027\n",
      "Epoch 00063: val_loss improved from 4.01929 to 3.89270, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.8027 - val_loss: 3.8927\n",
      "\n",
      "Epoch 00064: LearningRateScheduler reducing learning rate to 0.00029997352410392577.\n",
      "Epoch 64/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7703\n",
      "Epoch 00064: val_loss improved from 3.89270 to 3.76881, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.7703 - val_loss: 3.7688\n",
      "\n",
      "Epoch 00065: LearningRateScheduler reducing learning rate to 0.00029997263424697354.\n",
      "Epoch 65/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7752\n",
      "Epoch 00065: val_loss improved from 3.76881 to 3.64507, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.7752 - val_loss: 3.6451\n",
      "\n",
      "Epoch 00066: LearningRateScheduler reducing learning rate to 0.0002999717296834638.\n",
      "Epoch 66/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7437\n",
      "Epoch 00066: val_loss improved from 3.64507 to 3.52010, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.7437 - val_loss: 3.5201\n",
      "\n",
      "Epoch 00067: LearningRateScheduler reducing learning rate to 0.00029997081041348615.\n",
      "Epoch 67/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7332\n",
      "Epoch 00067: val_loss improved from 3.52010 to 3.39867, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.7332 - val_loss: 3.3987\n",
      "\n",
      "Epoch 00068: LearningRateScheduler reducing learning rate to 0.0002999698764371317.\n",
      "Epoch 68/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7060\n",
      "Epoch 00068: val_loss improved from 3.39867 to 3.28550, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.7060 - val_loss: 3.2855\n",
      "\n",
      "Epoch 00069: LearningRateScheduler reducing learning rate to 0.000299968927754493.\n",
      "Epoch 69/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7053\n",
      "Epoch 00069: val_loss improved from 3.28550 to 3.18700, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.7053 - val_loss: 3.1870\n",
      "\n",
      "Epoch 00070: LearningRateScheduler reducing learning rate to 0.0002999679643656639.\n",
      "Epoch 70/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6814\n",
      "Epoch 00070: val_loss improved from 3.18700 to 3.08809, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.6814 - val_loss: 3.0881\n",
      "\n",
      "Epoch 00071: LearningRateScheduler reducing learning rate to 0.00029996698627073986.\n",
      "Epoch 71/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6489\n",
      "Epoch 00071: val_loss improved from 3.08809 to 2.99056, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.6489 - val_loss: 2.9906\n",
      "\n",
      "Epoch 00072: LearningRateScheduler reducing learning rate to 0.0002999659934698179.\n",
      "Epoch 72/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6303\n",
      "Epoch 00072: val_loss improved from 2.99056 to 2.89133, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.6303 - val_loss: 2.8913\n",
      "\n",
      "Epoch 00073: LearningRateScheduler reducing learning rate to 0.00029996498596299615.\n",
      "Epoch 73/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6437\n",
      "Epoch 00073: val_loss improved from 2.89133 to 2.79829, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.6437 - val_loss: 2.7983\n",
      "\n",
      "Epoch 00074: LearningRateScheduler reducing learning rate to 0.0002999639637503745.\n",
      "Epoch 74/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6317\n",
      "Epoch 00074: val_loss improved from 2.79829 to 2.71861, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.6317 - val_loss: 2.7186\n",
      "\n",
      "Epoch 00075: LearningRateScheduler reducing learning rate to 0.0002999629268320542.\n",
      "Epoch 75/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6117\n",
      "Epoch 00075: val_loss improved from 2.71861 to 2.63319, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.6117 - val_loss: 2.6332\n",
      "\n",
      "Epoch 00076: LearningRateScheduler reducing learning rate to 0.00029996187520813796.\n",
      "Epoch 76/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6070\n",
      "Epoch 00076: val_loss improved from 2.63319 to 2.55452, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.6070 - val_loss: 2.5545\n",
      "\n",
      "Epoch 00077: LearningRateScheduler reducing learning rate to 0.00029996080887873.\n",
      "Epoch 77/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5877\n",
      "Epoch 00077: val_loss improved from 2.55452 to 2.48548, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.5877 - val_loss: 2.4855\n",
      "\n",
      "Epoch 00078: LearningRateScheduler reducing learning rate to 0.0002999597278439358.\n",
      "Epoch 78/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5637\n",
      "Epoch 00078: val_loss improved from 2.48548 to 2.42681, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.5637 - val_loss: 2.4268\n",
      "\n",
      "Epoch 00079: LearningRateScheduler reducing learning rate to 0.0002999586321038626.\n",
      "Epoch 79/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5876\n",
      "Epoch 00079: val_loss improved from 2.42681 to 2.37423, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.5876 - val_loss: 2.3742\n",
      "\n",
      "Epoch 00080: LearningRateScheduler reducing learning rate to 0.00029995752165861886.\n",
      "Epoch 80/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5941\n",
      "Epoch 00080: val_loss improved from 2.37423 to 2.30498, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.5941 - val_loss: 2.3050\n",
      "\n",
      "Epoch 00081: LearningRateScheduler reducing learning rate to 0.00029995639650831456.\n",
      "Epoch 81/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5456\n",
      "Epoch 00081: val_loss improved from 2.30498 to 2.22881, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.5456 - val_loss: 2.2288\n",
      "\n",
      "Epoch 00082: LearningRateScheduler reducing learning rate to 0.0002999552566530612.\n",
      "Epoch 82/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5572\n",
      "Epoch 00082: val_loss improved from 2.22881 to 2.16285, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.5572 - val_loss: 2.1629\n",
      "\n",
      "Epoch 00083: LearningRateScheduler reducing learning rate to 0.00029995410209297156.\n",
      "Epoch 83/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5416\n",
      "Epoch 00083: val_loss improved from 2.16285 to 2.09523, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.5416 - val_loss: 2.0952\n",
      "\n",
      "Epoch 00084: LearningRateScheduler reducing learning rate to 0.0002999529328281602.\n",
      "Epoch 84/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5301\n",
      "Epoch 00084: val_loss improved from 2.09523 to 2.04108, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.5301 - val_loss: 2.0411\n",
      "\n",
      "Epoch 00085: LearningRateScheduler reducing learning rate to 0.00029995174885874277.\n",
      "Epoch 85/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5099\n",
      "Epoch 00085: val_loss improved from 2.04108 to 1.99636, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.5099 - val_loss: 1.9964\n",
      "\n",
      "Epoch 00086: LearningRateScheduler reducing learning rate to 0.00029995055018483655.\n",
      "Epoch 86/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5088\n",
      "Epoch 00086: val_loss improved from 1.99636 to 1.95891, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.5088 - val_loss: 1.9589\n",
      "\n",
      "Epoch 00087: LearningRateScheduler reducing learning rate to 0.0002999493368065604.\n",
      "Epoch 87/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4948\n",
      "Epoch 00087: val_loss improved from 1.95891 to 1.91696, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.4948 - val_loss: 1.9170\n",
      "\n",
      "Epoch 00088: LearningRateScheduler reducing learning rate to 0.00029994810872403446.\n",
      "Epoch 88/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4696\n",
      "Epoch 00088: val_loss improved from 1.91696 to 1.86984, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.4696 - val_loss: 1.8698\n",
      "\n",
      "Epoch 00089: LearningRateScheduler reducing learning rate to 0.0002999468659373803.\n",
      "Epoch 89/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4834\n",
      "Epoch 00089: val_loss improved from 1.86984 to 1.82882, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.4834 - val_loss: 1.8288\n",
      "\n",
      "Epoch 00090: LearningRateScheduler reducing learning rate to 0.0002999456084467211.\n",
      "Epoch 90/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4791\n",
      "Epoch 00090: val_loss improved from 1.82882 to 1.78430, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.4791 - val_loss: 1.7843\n",
      "\n",
      "Epoch 00091: LearningRateScheduler reducing learning rate to 0.00029994433625218133.\n",
      "Epoch 91/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4668\n",
      "Epoch 00091: val_loss improved from 1.78430 to 1.73902, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.4668 - val_loss: 1.7390\n",
      "\n",
      "Epoch 00092: LearningRateScheduler reducing learning rate to 0.0002999430493538871.\n",
      "Epoch 92/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4565\n",
      "Epoch 00092: val_loss improved from 1.73902 to 1.69259, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.4565 - val_loss: 1.6926\n",
      "\n",
      "Epoch 00093: LearningRateScheduler reducing learning rate to 0.00029994174775196584.\n",
      "Epoch 93/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4724\n",
      "Epoch 00093: val_loss improved from 1.69259 to 1.65506, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.4724 - val_loss: 1.6551\n",
      "\n",
      "Epoch 00094: LearningRateScheduler reducing learning rate to 0.0002999404314465465.\n",
      "Epoch 94/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4490\n",
      "Epoch 00094: val_loss improved from 1.65506 to 1.62383, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.4490 - val_loss: 1.6238\n",
      "\n",
      "Epoch 00095: LearningRateScheduler reducing learning rate to 0.00029993910043775944.\n",
      "Epoch 95/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4489\n",
      "Epoch 00095: val_loss improved from 1.62383 to 1.58999, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.4489 - val_loss: 1.5900\n",
      "\n",
      "Epoch 00096: LearningRateScheduler reducing learning rate to 0.00029993775472573645.\n",
      "Epoch 96/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4367\n",
      "Epoch 00096: val_loss improved from 1.58999 to 1.58249, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.4367 - val_loss: 1.5825\n",
      "\n",
      "Epoch 00097: LearningRateScheduler reducing learning rate to 0.0002999363943106109.\n",
      "Epoch 97/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4231\n",
      "Epoch 00097: val_loss improved from 1.58249 to 1.57515, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.4231 - val_loss: 1.5752\n",
      "\n",
      "Epoch 00098: LearningRateScheduler reducing learning rate to 0.00029993501919251756.\n",
      "Epoch 98/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4337\n",
      "Epoch 00098: val_loss improved from 1.57515 to 1.55538, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.4337 - val_loss: 1.5554\n",
      "\n",
      "Epoch 00099: LearningRateScheduler reducing learning rate to 0.00029993362937159255.\n",
      "Epoch 99/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4385\n",
      "Epoch 00099: val_loss improved from 1.55538 to 1.53028, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.4385 - val_loss: 1.5303\n",
      "\n",
      "Epoch 00100: LearningRateScheduler reducing learning rate to 0.0002999322248479736.\n",
      "Epoch 100/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4180\n",
      "Epoch 00100: val_loss improved from 1.53028 to 1.50086, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.4180 - val_loss: 1.5009\n",
      "\n",
      "Epoch 00101: LearningRateScheduler reducing learning rate to 0.0002999308056217998.\n",
      "Epoch 101/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4153\n",
      "Epoch 00101: val_loss improved from 1.50086 to 1.48239, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.4153 - val_loss: 1.4824\n",
      "\n",
      "Epoch 00102: LearningRateScheduler reducing learning rate to 0.00029992937169321174.\n",
      "Epoch 102/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4176\n",
      "Epoch 00102: val_loss improved from 1.48239 to 1.44538, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.4176 - val_loss: 1.4454\n",
      "\n",
      "Epoch 00103: LearningRateScheduler reducing learning rate to 0.00029992792306235145.\n",
      "Epoch 103/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.3904\n",
      "Epoch 00103: val_loss improved from 1.44538 to 1.40945, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.3904 - val_loss: 1.4095\n",
      "\n",
      "Epoch 00104: LearningRateScheduler reducing learning rate to 0.00029992645972936244.\n",
      "Epoch 104/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.3953\n",
      "Epoch 00104: val_loss improved from 1.40945 to 1.39584, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.3953 - val_loss: 1.3958\n",
      "\n",
      "Epoch 00105: LearningRateScheduler reducing learning rate to 0.00029992498169438965.\n",
      "Epoch 105/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.3871\n",
      "Epoch 00105: val_loss improved from 1.39584 to 1.37575, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.3871 - val_loss: 1.3758\n",
      "\n",
      "Epoch 00106: LearningRateScheduler reducing learning rate to 0.00029992348895757946.\n",
      "Epoch 106/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4014\n",
      "Epoch 00106: val_loss improved from 1.37575 to 1.34866, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.4014 - val_loss: 1.3487\n",
      "\n",
      "Epoch 00107: LearningRateScheduler reducing learning rate to 0.00029992198151907975.\n",
      "Epoch 107/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.3700\n",
      "Epoch 00107: val_loss improved from 1.34866 to 1.33256, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.3700 - val_loss: 1.3326\n",
      "\n",
      "Epoch 00108: LearningRateScheduler reducing learning rate to 0.00029992045937903987.\n",
      "Epoch 108/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.3525\n",
      "Epoch 00108: val_loss did not improve from 1.33256\n",
      "3/3 [==============================] - 0s 131ms/step - loss: 0.3525 - val_loss: 1.3406\n",
      "\n",
      "Epoch 00109: LearningRateScheduler reducing learning rate to 0.0002999189225376105.\n",
      "Epoch 109/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.3628\n",
      "Epoch 00109: val_loss did not improve from 1.33256\n",
      "3/3 [==============================] - 0s 128ms/step - loss: 0.3628 - val_loss: 1.3651\n",
      "\n",
      "Epoch 00110: LearningRateScheduler reducing learning rate to 0.00029991737099494393.\n",
      "Epoch 110/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.3693\n",
      "Epoch 00110: val_loss did not improve from 1.33256\n",
      "3/3 [==============================] - 0s 126ms/step - loss: 0.3693 - val_loss: 1.3809\n",
      "\n",
      "Epoch 00111: LearningRateScheduler reducing learning rate to 0.00029991580475119384.\n",
      "Epoch 111/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.3594\n",
      "Epoch 00111: val_loss did not improve from 1.33256\n",
      "3/3 [==============================] - 0s 131ms/step - loss: 0.3594 - val_loss: 1.3766\n",
      "\n",
      "Epoch 00112: LearningRateScheduler reducing learning rate to 0.00029991422380651534.\n",
      "Epoch 112/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.3426\n",
      "Epoch 00112: val_loss did not improve from 1.33256\n",
      "3/3 [==============================] - 0s 133ms/step - loss: 0.3426 - val_loss: 1.3668\n",
      "\n",
      "Epoch 00113: LearningRateScheduler reducing learning rate to 0.00029991262816106505.\n",
      "Epoch 113/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.3458\n",
      "Epoch 00113: val_loss improved from 1.33256 to 1.32962, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.3458 - val_loss: 1.3296\n",
      "\n",
      "Epoch 00114: LearningRateScheduler reducing learning rate to 0.00029991101781500103.\n",
      "Epoch 114/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.3585\n",
      "Epoch 00114: val_loss improved from 1.32962 to 1.29139, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.3585 - val_loss: 1.2914\n",
      "\n",
      "Epoch 00115: LearningRateScheduler reducing learning rate to 0.0002999093927684828.\n",
      "Epoch 115/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.3352\n",
      "Epoch 00115: val_loss improved from 1.29139 to 1.27123, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.3352 - val_loss: 1.2712\n",
      "\n",
      "Epoch 00116: LearningRateScheduler reducing learning rate to 0.00029990775302167125.\n",
      "Epoch 116/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.3405\n",
      "Epoch 00116: val_loss improved from 1.27123 to 1.25010, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.3405 - val_loss: 1.2501\n",
      "\n",
      "Epoch 00117: LearningRateScheduler reducing learning rate to 0.0002999060985747289.\n",
      "Epoch 117/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.3213\n",
      "Epoch 00117: val_loss improved from 1.25010 to 1.22484, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.3213 - val_loss: 1.2248\n",
      "\n",
      "Epoch 00118: LearningRateScheduler reducing learning rate to 0.00029990442942781956.\n",
      "Epoch 118/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.3168\n",
      "Epoch 00118: val_loss did not improve from 1.22484\n",
      "3/3 [==============================] - 0s 132ms/step - loss: 0.3168 - val_loss: 1.2308\n",
      "\n",
      "Epoch 00119: LearningRateScheduler reducing learning rate to 0.0002999027455811086.\n",
      "Epoch 119/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.3271\n",
      "Epoch 00119: val_loss did not improve from 1.22484\n",
      "3/3 [==============================] - 0s 126ms/step - loss: 0.3271 - val_loss: 1.2531\n",
      "\n",
      "Epoch 00120: LearningRateScheduler reducing learning rate to 0.0002999010470347628.\n",
      "Epoch 120/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.3294\n",
      "Epoch 00120: val_loss did not improve from 1.22484\n",
      "3/3 [==============================] - 0s 128ms/step - loss: 0.3294 - val_loss: 1.2695\n",
      "\n",
      "Epoch 00121: LearningRateScheduler reducing learning rate to 0.0002998993337889504.\n",
      "Epoch 121/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.3079\n",
      "Epoch 00121: val_loss did not improve from 1.22484\n",
      "3/3 [==============================] - 0s 130ms/step - loss: 0.3079 - val_loss: 1.2613\n",
      "\n",
      "Epoch 00122: LearningRateScheduler reducing learning rate to 0.00029989760584384106.\n",
      "Epoch 122/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.3075\n",
      "Epoch 00122: val_loss did not improve from 1.22484\n",
      "3/3 [==============================] - 0s 123ms/step - loss: 0.3075 - val_loss: 1.2417\n",
      "\n",
      "Epoch 00123: LearningRateScheduler reducing learning rate to 0.00029989586319960603.\n",
      "Epoch 123/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.3171\n",
      "Epoch 00123: val_loss did not improve from 1.22484\n",
      "3/3 [==============================] - 0s 127ms/step - loss: 0.3171 - val_loss: 1.2292\n",
      "\n",
      "Epoch 00124: LearningRateScheduler reducing learning rate to 0.0002998941058564178.\n",
      "Epoch 124/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.3132\n",
      "Epoch 00124: val_loss did not improve from 1.22484\n",
      "3/3 [==============================] - 0s 136ms/step - loss: 0.3132 - val_loss: 1.2324\n",
      "\n",
      "Epoch 00125: LearningRateScheduler reducing learning rate to 0.0002998923338144506.\n",
      "Epoch 125/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.3052\n",
      "Epoch 00125: val_loss did not improve from 1.22484\n",
      "3/3 [==============================] - 0s 129ms/step - loss: 0.3052 - val_loss: 1.2337\n",
      "\n",
      "Epoch 00126: LearningRateScheduler reducing learning rate to 0.00029989054707387977.\n",
      "Epoch 126/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.3274\n",
      "Epoch 00126: val_loss improved from 1.22484 to 1.21780, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.3274 - val_loss: 1.2178\n",
      "\n",
      "Epoch 00127: LearningRateScheduler reducing learning rate to 0.00029988874563488245.\n",
      "Epoch 127/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.2935\n",
      "Epoch 00127: val_loss improved from 1.21780 to 1.20005, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.2935 - val_loss: 1.2001\n",
      "\n",
      "Epoch 00128: LearningRateScheduler reducing learning rate to 0.000299886929497637.\n",
      "Epoch 128/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.2882\n",
      "Epoch 00128: val_loss improved from 1.20005 to 1.18360, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.2882 - val_loss: 1.1836\n",
      "\n",
      "Epoch 00129: LearningRateScheduler reducing learning rate to 0.0002998850986623232.\n",
      "Epoch 129/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.2716\n",
      "Epoch 00129: val_loss improved from 1.18360 to 1.16989, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.2716 - val_loss: 1.1699\n",
      "\n",
      "Epoch 00130: LearningRateScheduler reducing learning rate to 0.00029988325312912267.\n",
      "Epoch 130/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.3207\n",
      "Epoch 00130: val_loss improved from 1.16989 to 1.16303, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.3207 - val_loss: 1.1630\n",
      "\n",
      "Epoch 00131: LearningRateScheduler reducing learning rate to 0.0002998813928982181.\n",
      "Epoch 131/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.2697\n",
      "Epoch 00131: val_loss improved from 1.16303 to 1.15203, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.2697 - val_loss: 1.1520\n",
      "\n",
      "Epoch 00132: LearningRateScheduler reducing learning rate to 0.0002998795179697936.\n",
      "Epoch 132/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.2784\n",
      "Epoch 00132: val_loss improved from 1.15203 to 1.13953, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.2784 - val_loss: 1.1395\n",
      "\n",
      "Epoch 00133: LearningRateScheduler reducing learning rate to 0.000299877628344035.\n",
      "Epoch 133/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.2894\n",
      "Epoch 00133: val_loss improved from 1.13953 to 1.12610, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.2894 - val_loss: 1.1261\n",
      "\n",
      "Epoch 00134: LearningRateScheduler reducing learning rate to 0.0002998757240211295.\n",
      "Epoch 134/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.3046\n",
      "Epoch 00134: val_loss improved from 1.12610 to 1.11155, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.3046 - val_loss: 1.1115\n",
      "\n",
      "Epoch 00135: LearningRateScheduler reducing learning rate to 0.00029987380500126577.\n",
      "Epoch 135/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.2762\n",
      "Epoch 00135: val_loss improved from 1.11155 to 1.09738, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.2762 - val_loss: 1.0974\n",
      "\n",
      "Epoch 00136: LearningRateScheduler reducing learning rate to 0.00029987187128463374.\n",
      "Epoch 136/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.2678\n",
      "Epoch 00136: val_loss improved from 1.09738 to 1.08389, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.2678 - val_loss: 1.0839\n",
      "\n",
      "Epoch 00137: LearningRateScheduler reducing learning rate to 0.00029986992287142503.\n",
      "Epoch 137/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.2658\n",
      "Epoch 00137: val_loss improved from 1.08389 to 1.06983, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.2658 - val_loss: 1.0698\n",
      "\n",
      "Epoch 00138: LearningRateScheduler reducing learning rate to 0.0002998679597618327.\n",
      "Epoch 138/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.2917\n",
      "Epoch 00138: val_loss improved from 1.06983 to 1.04731, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.2917 - val_loss: 1.0473\n",
      "\n",
      "Epoch 00139: LearningRateScheduler reducing learning rate to 0.0002998659819560511.\n",
      "Epoch 139/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.3002\n",
      "Epoch 00139: val_loss improved from 1.04731 to 1.02452, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.3002 - val_loss: 1.0245\n",
      "\n",
      "Epoch 00140: LearningRateScheduler reducing learning rate to 0.0002998639894542762.\n",
      "Epoch 140/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.2792\n",
      "Epoch 00140: val_loss improved from 1.02452 to 1.00453, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.2792 - val_loss: 1.0045\n",
      "\n",
      "Epoch 00141: LearningRateScheduler reducing learning rate to 0.0002998619822567053.\n",
      "Epoch 141/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.2609\n",
      "Epoch 00141: val_loss improved from 1.00453 to 0.97858, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.2609 - val_loss: 0.9786\n",
      "\n",
      "Epoch 00142: LearningRateScheduler reducing learning rate to 0.0002998599603635373.\n",
      "Epoch 142/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.2656\n",
      "Epoch 00142: val_loss improved from 0.97858 to 0.95269, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.2656 - val_loss: 0.9527\n",
      "\n",
      "Epoch 00143: LearningRateScheduler reducing learning rate to 0.0002998579237749724.\n",
      "Epoch 143/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.2494\n",
      "Epoch 00143: val_loss improved from 0.95269 to 0.93512, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.2494 - val_loss: 0.9351\n",
      "\n",
      "Epoch 00144: LearningRateScheduler reducing learning rate to 0.00029985587249121233.\n",
      "Epoch 144/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.2484\n",
      "Epoch 00144: val_loss improved from 0.93512 to 0.92328, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.2484 - val_loss: 0.9233\n",
      "\n",
      "Epoch 00145: LearningRateScheduler reducing learning rate to 0.0002998538065124603.\n",
      "Epoch 145/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.2745\n",
      "Epoch 00145: val_loss improved from 0.92328 to 0.90819, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.2745 - val_loss: 0.9082\n",
      "\n",
      "Epoch 00146: LearningRateScheduler reducing learning rate to 0.00029985172583892104.\n",
      "Epoch 146/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.2562\n",
      "Epoch 00146: val_loss improved from 0.90819 to 0.88492, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.2562 - val_loss: 0.8849\n",
      "\n",
      "Epoch 00147: LearningRateScheduler reducing learning rate to 0.0002998496304708005.\n",
      "Epoch 147/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.2476\n",
      "Epoch 00147: val_loss improved from 0.88492 to 0.85445, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.2476 - val_loss: 0.8545\n",
      "\n",
      "Epoch 00148: LearningRateScheduler reducing learning rate to 0.0002998475204083063.\n",
      "Epoch 148/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.2635\n",
      "Epoch 00148: val_loss improved from 0.85445 to 0.84057, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.2635 - val_loss: 0.8406\n",
      "\n",
      "Epoch 00149: LearningRateScheduler reducing learning rate to 0.0002998453956516474.\n",
      "Epoch 149/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.2374\n",
      "Epoch 00149: val_loss did not improve from 0.84057\n",
      "3/3 [==============================] - 0s 132ms/step - loss: 0.2374 - val_loss: 0.8419\n",
      "\n",
      "Epoch 00150: LearningRateScheduler reducing learning rate to 0.0002998432562010343.\n",
      "Epoch 150/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.2494\n",
      "Epoch 00150: val_loss improved from 0.84057 to 0.83609, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.2494 - val_loss: 0.8361\n",
      "\n",
      "Epoch 00151: LearningRateScheduler reducing learning rate to 0.00029984110205667894.\n",
      "Epoch 151/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.2744\n",
      "Epoch 00151: val_loss improved from 0.83609 to 0.81288, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.2744 - val_loss: 0.8129\n",
      "\n",
      "Epoch 00152: LearningRateScheduler reducing learning rate to 0.0002998389332187947.\n",
      "Epoch 152/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.2386\n",
      "Epoch 00152: val_loss improved from 0.81288 to 0.79216, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.2386 - val_loss: 0.7922\n",
      "\n",
      "Epoch 00153: LearningRateScheduler reducing learning rate to 0.0002998367496875963.\n",
      "Epoch 153/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.2463\n",
      "Epoch 00153: val_loss improved from 0.79216 to 0.77706, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.2463 - val_loss: 0.7771\n",
      "\n",
      "Epoch 00154: LearningRateScheduler reducing learning rate to 0.0002998345514633001.\n",
      "Epoch 154/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.2284\n",
      "Epoch 00154: val_loss improved from 0.77706 to 0.75693, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.2284 - val_loss: 0.7569\n",
      "\n",
      "Epoch 00155: LearningRateScheduler reducing learning rate to 0.0002998323385461239.\n",
      "Epoch 155/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.2457\n",
      "Epoch 00155: val_loss improved from 0.75693 to 0.73710, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.2457 - val_loss: 0.7371\n",
      "\n",
      "Epoch 00156: LearningRateScheduler reducing learning rate to 0.0002998301109362868.\n",
      "Epoch 156/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.2312\n",
      "Epoch 00156: val_loss improved from 0.73710 to 0.72830, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.2312 - val_loss: 0.7283\n",
      "\n",
      "Epoch 00157: LearningRateScheduler reducing learning rate to 0.00029982786863400946.\n",
      "Epoch 157/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.2520\n",
      "Epoch 00157: val_loss improved from 0.72830 to 0.71840, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.2520 - val_loss: 0.7184\n",
      "\n",
      "Epoch 00158: LearningRateScheduler reducing learning rate to 0.00029982561163951403.\n",
      "Epoch 158/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.2173\n",
      "Epoch 00158: val_loss improved from 0.71840 to 0.70081, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.2173 - val_loss: 0.7008\n",
      "\n",
      "Epoch 00159: LearningRateScheduler reducing learning rate to 0.00029982333995302407.\n",
      "Epoch 159/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.2228\n",
      "Epoch 00159: val_loss improved from 0.70081 to 0.69094, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.2228 - val_loss: 0.6909\n",
      "\n",
      "Epoch 00160: LearningRateScheduler reducing learning rate to 0.0002998210535747645.\n",
      "Epoch 160/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.2184\n",
      "Epoch 00160: val_loss improved from 0.69094 to 0.69051, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.2184 - val_loss: 0.6905\n",
      "\n",
      "Epoch 00161: LearningRateScheduler reducing learning rate to 0.0002998187525049619.\n",
      "Epoch 161/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.2209\n",
      "Epoch 00161: val_loss improved from 0.69051 to 0.68851, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.2209 - val_loss: 0.6885\n",
      "\n",
      "Epoch 00162: LearningRateScheduler reducing learning rate to 0.0002998164367438442.\n",
      "Epoch 162/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.2312\n",
      "Epoch 00162: val_loss improved from 0.68851 to 0.67875, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.2312 - val_loss: 0.6787\n",
      "\n",
      "Epoch 00163: LearningRateScheduler reducing learning rate to 0.0002998141062916407.\n",
      "Epoch 163/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.2235\n",
      "Epoch 00163: val_loss improved from 0.67875 to 0.67163, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.2235 - val_loss: 0.6716\n",
      "\n",
      "Epoch 00164: LearningRateScheduler reducing learning rate to 0.00029981176114858235.\n",
      "Epoch 164/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.2045\n",
      "Epoch 00164: val_loss did not improve from 0.67163\n",
      "3/3 [==============================] - 0s 132ms/step - loss: 0.2045 - val_loss: 0.6762\n",
      "\n",
      "Epoch 00165: LearningRateScheduler reducing learning rate to 0.0002998094013149013.\n",
      "Epoch 165/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.2296\n",
      "Epoch 00165: val_loss did not improve from 0.67163\n",
      "3/3 [==============================] - 0s 134ms/step - loss: 0.2296 - val_loss: 0.6808\n",
      "\n",
      "Epoch 00166: LearningRateScheduler reducing learning rate to 0.0002998070267908314.\n",
      "Epoch 166/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.2275\n",
      "Epoch 00166: val_loss did not improve from 0.67163\n",
      "3/3 [==============================] - 0s 133ms/step - loss: 0.2275 - val_loss: 0.6757\n",
      "\n",
      "Epoch 00167: LearningRateScheduler reducing learning rate to 0.0002998046375766078.\n",
      "Epoch 167/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.2278\n",
      "Epoch 00167: val_loss improved from 0.67163 to 0.66339, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.2278 - val_loss: 0.6634\n",
      "\n",
      "Epoch 00168: LearningRateScheduler reducing learning rate to 0.0002998022336724672.\n",
      "Epoch 168/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.2331\n",
      "Epoch 00168: val_loss improved from 0.66339 to 0.65629, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.2331 - val_loss: 0.6563\n",
      "\n",
      "Epoch 00169: LearningRateScheduler reducing learning rate to 0.0002997998150786477.\n",
      "Epoch 169/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.2048\n",
      "Epoch 00169: val_loss improved from 0.65629 to 0.65431, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.2048 - val_loss: 0.6543\n",
      "\n",
      "Epoch 00170: LearningRateScheduler reducing learning rate to 0.00029979738179538885.\n",
      "Epoch 170/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.2131\n",
      "Epoch 00170: val_loss improved from 0.65431 to 0.64480, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.2131 - val_loss: 0.6448\n",
      "\n",
      "Epoch 00171: LearningRateScheduler reducing learning rate to 0.00029979493382293167.\n",
      "Epoch 171/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.2045\n",
      "Epoch 00171: val_loss improved from 0.64480 to 0.62770, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.2045 - val_loss: 0.6277\n",
      "\n",
      "Epoch 00172: LearningRateScheduler reducing learning rate to 0.0002997924711615186.\n",
      "Epoch 172/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.2357\n",
      "Epoch 00172: val_loss improved from 0.62770 to 0.60917, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.2357 - val_loss: 0.6092\n",
      "\n",
      "Epoch 00173: LearningRateScheduler reducing learning rate to 0.00029978999381139365.\n",
      "Epoch 173/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1974\n",
      "Epoch 00173: val_loss improved from 0.60917 to 0.59722, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.1974 - val_loss: 0.5972\n",
      "\n",
      "Epoch 00174: LearningRateScheduler reducing learning rate to 0.0002997875017728022.\n",
      "Epoch 174/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.2307\n",
      "Epoch 00174: val_loss improved from 0.59722 to 0.58659, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.2307 - val_loss: 0.5866\n",
      "\n",
      "Epoch 00175: LearningRateScheduler reducing learning rate to 0.000299784995045991.\n",
      "Epoch 175/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.2016\n",
      "Epoch 00175: val_loss improved from 0.58659 to 0.58249, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.2016 - val_loss: 0.5825\n",
      "\n",
      "Epoch 00176: LearningRateScheduler reducing learning rate to 0.00029978247363120854.\n",
      "Epoch 176/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.2114\n",
      "Epoch 00176: val_loss did not improve from 0.58249\n",
      "3/3 [==============================] - 0s 137ms/step - loss: 0.2114 - val_loss: 0.5849\n",
      "\n",
      "Epoch 00177: LearningRateScheduler reducing learning rate to 0.0002997799375287044.\n",
      "Epoch 177/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.2058\n",
      "Epoch 00177: val_loss did not improve from 0.58249\n",
      "3/3 [==============================] - 0s 138ms/step - loss: 0.2058 - val_loss: 0.5921\n",
      "\n",
      "Epoch 00178: LearningRateScheduler reducing learning rate to 0.0002997773867387298.\n",
      "Epoch 178/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.2123\n",
      "Epoch 00178: val_loss did not improve from 0.58249\n",
      "3/3 [==============================] - 0s 132ms/step - loss: 0.2123 - val_loss: 0.5935\n",
      "\n",
      "Epoch 00179: LearningRateScheduler reducing learning rate to 0.00029977482126153745.\n",
      "Epoch 179/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.2050\n",
      "Epoch 00179: val_loss did not improve from 0.58249\n",
      "3/3 [==============================] - 0s 131ms/step - loss: 0.2050 - val_loss: 0.5844\n",
      "\n",
      "Epoch 00180: LearningRateScheduler reducing learning rate to 0.00029977224109738144.\n",
      "Epoch 180/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.2119\n",
      "Epoch 00180: val_loss improved from 0.58249 to 0.57505, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.2119 - val_loss: 0.5750\n",
      "\n",
      "Epoch 00181: LearningRateScheduler reducing learning rate to 0.00029976964624651737.\n",
      "Epoch 181/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.2200\n",
      "Epoch 00181: val_loss improved from 0.57505 to 0.56741, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.2200 - val_loss: 0.5674\n",
      "\n",
      "Epoch 00182: LearningRateScheduler reducing learning rate to 0.00029976703670920224.\n",
      "Epoch 182/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.2107\n",
      "Epoch 00182: val_loss improved from 0.56741 to 0.56111, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.2107 - val_loss: 0.5611\n",
      "\n",
      "Epoch 00183: LearningRateScheduler reducing learning rate to 0.0002997644124856945.\n",
      "Epoch 183/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.2046\n",
      "Epoch 00183: val_loss improved from 0.56111 to 0.55943, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.2046 - val_loss: 0.5594\n",
      "\n",
      "Epoch 00184: LearningRateScheduler reducing learning rate to 0.0002997617735762542.\n",
      "Epoch 184/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.2073\n",
      "Epoch 00184: val_loss improved from 0.55943 to 0.55695, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.2073 - val_loss: 0.5569\n",
      "\n",
      "Epoch 00185: LearningRateScheduler reducing learning rate to 0.0002997591199811426.\n",
      "Epoch 185/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1935\n",
      "Epoch 00185: val_loss improved from 0.55695 to 0.55358, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.1935 - val_loss: 0.5536\n",
      "\n",
      "Epoch 00186: LearningRateScheduler reducing learning rate to 0.0002997564517006227.\n",
      "Epoch 186/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1849\n",
      "Epoch 00186: val_loss improved from 0.55358 to 0.55109, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1849 - val_loss: 0.5511\n",
      "\n",
      "Epoch 00187: LearningRateScheduler reducing learning rate to 0.0002997537687349586.\n",
      "Epoch 187/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.2037\n",
      "Epoch 00187: val_loss improved from 0.55109 to 0.54597, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.2037 - val_loss: 0.5460\n",
      "\n",
      "Epoch 00188: LearningRateScheduler reducing learning rate to 0.00029975107108441614.\n",
      "Epoch 188/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1670\n",
      "Epoch 00188: val_loss improved from 0.54597 to 0.53825, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1670 - val_loss: 0.5382\n",
      "\n",
      "Epoch 00189: LearningRateScheduler reducing learning rate to 0.00029974835874926263.\n",
      "Epoch 189/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1897\n",
      "Epoch 00189: val_loss improved from 0.53825 to 0.53261, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1897 - val_loss: 0.5326\n",
      "\n",
      "Epoch 00190: LearningRateScheduler reducing learning rate to 0.00029974563172976656.\n",
      "Epoch 190/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1996\n",
      "Epoch 00190: val_loss improved from 0.53261 to 0.53039, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1996 - val_loss: 0.5304\n",
      "\n",
      "Epoch 00191: LearningRateScheduler reducing learning rate to 0.00029974289002619817.\n",
      "Epoch 191/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.2243\n",
      "Epoch 00191: val_loss improved from 0.53039 to 0.52981, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.2243 - val_loss: 0.5298\n",
      "\n",
      "Epoch 00192: LearningRateScheduler reducing learning rate to 0.000299740133638829.\n",
      "Epoch 192/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1985\n",
      "Epoch 00192: val_loss improved from 0.52981 to 0.52179, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1985 - val_loss: 0.5218\n",
      "\n",
      "Epoch 00193: LearningRateScheduler reducing learning rate to 0.00029973736256793206.\n",
      "Epoch 193/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1967\n",
      "Epoch 00193: val_loss improved from 0.52179 to 0.51418, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1967 - val_loss: 0.5142\n",
      "\n",
      "Epoch 00194: LearningRateScheduler reducing learning rate to 0.00029973457681378187.\n",
      "Epoch 194/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1900\n",
      "Epoch 00194: val_loss improved from 0.51418 to 0.50882, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1900 - val_loss: 0.5088\n",
      "\n",
      "Epoch 00195: LearningRateScheduler reducing learning rate to 0.00029973177637665434.\n",
      "Epoch 195/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1659\n",
      "Epoch 00195: val_loss improved from 0.50882 to 0.50338, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1659 - val_loss: 0.5034\n",
      "\n",
      "Epoch 00196: LearningRateScheduler reducing learning rate to 0.0002997289612568268.\n",
      "Epoch 196/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1758\n",
      "Epoch 00196: val_loss improved from 0.50338 to 0.49479, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1758 - val_loss: 0.4948\n",
      "\n",
      "Epoch 00197: LearningRateScheduler reducing learning rate to 0.00029972613145457823.\n",
      "Epoch 197/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1775\n",
      "Epoch 00197: val_loss improved from 0.49479 to 0.48563, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1775 - val_loss: 0.4856\n",
      "\n",
      "Epoch 00198: LearningRateScheduler reducing learning rate to 0.0002997232869701888.\n",
      "Epoch 198/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1765\n",
      "Epoch 00198: val_loss improved from 0.48563 to 0.48084, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1765 - val_loss: 0.4808\n",
      "\n",
      "Epoch 00199: LearningRateScheduler reducing learning rate to 0.00029972042780394034.\n",
      "Epoch 199/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1889\n",
      "Epoch 00199: val_loss improved from 0.48084 to 0.47673, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1889 - val_loss: 0.4767\n",
      "\n",
      "Epoch 00200: LearningRateScheduler reducing learning rate to 0.00029971755395611597.\n",
      "Epoch 200/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1744\n",
      "Epoch 00200: val_loss improved from 0.47673 to 0.47193, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1744 - val_loss: 0.4719\n",
      "\n",
      "Epoch 00201: LearningRateScheduler reducing learning rate to 0.00029971466542700047.\n",
      "Epoch 201/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1951\n",
      "Epoch 00201: val_loss improved from 0.47193 to 0.46939, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.1951 - val_loss: 0.4694\n",
      "\n",
      "Epoch 00202: LearningRateScheduler reducing learning rate to 0.0002997117622168799.\n",
      "Epoch 202/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1918\n",
      "Epoch 00202: val_loss improved from 0.46939 to 0.46747, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1918 - val_loss: 0.4675\n",
      "\n",
      "Epoch 00203: LearningRateScheduler reducing learning rate to 0.0002997088443260418.\n",
      "Epoch 203/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1806\n",
      "Epoch 00203: val_loss improved from 0.46747 to 0.46580, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1806 - val_loss: 0.4658\n",
      "\n",
      "Epoch 00204: LearningRateScheduler reducing learning rate to 0.00029970591175477524.\n",
      "Epoch 204/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1770\n",
      "Epoch 00204: val_loss improved from 0.46580 to 0.46526, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1770 - val_loss: 0.4653\n",
      "\n",
      "Epoch 00205: LearningRateScheduler reducing learning rate to 0.00029970296450337067.\n",
      "Epoch 205/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1773\n",
      "Epoch 00205: val_loss did not improve from 0.46526\n",
      "3/3 [==============================] - 0s 130ms/step - loss: 0.1773 - val_loss: 0.4670\n",
      "\n",
      "Epoch 00206: LearningRateScheduler reducing learning rate to 0.00029970000257212005.\n",
      "Epoch 206/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1806\n",
      "Epoch 00206: val_loss did not improve from 0.46526\n",
      "3/3 [==============================] - 0s 142ms/step - loss: 0.1806 - val_loss: 0.4689\n",
      "\n",
      "Epoch 00207: LearningRateScheduler reducing learning rate to 0.0002996970259613167.\n",
      "Epoch 207/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1682\n",
      "Epoch 00207: val_loss did not improve from 0.46526\n",
      "3/3 [==============================] - 0s 155ms/step - loss: 0.1682 - val_loss: 0.4659\n",
      "\n",
      "Epoch 00208: LearningRateScheduler reducing learning rate to 0.00029969403467125554.\n",
      "Epoch 208/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1688\n",
      "Epoch 00208: val_loss improved from 0.46526 to 0.45860, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.1688 - val_loss: 0.4586\n",
      "\n",
      "Epoch 00209: LearningRateScheduler reducing learning rate to 0.0002996910287022328.\n",
      "Epoch 209/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1579\n",
      "Epoch 00209: val_loss improved from 0.45860 to 0.44901, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1579 - val_loss: 0.4490\n",
      "\n",
      "Epoch 00210: LearningRateScheduler reducing learning rate to 0.0002996880080545463.\n",
      "Epoch 210/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.2065\n",
      "Epoch 00210: val_loss improved from 0.44901 to 0.44231, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.2065 - val_loss: 0.4423\n",
      "\n",
      "Epoch 00211: LearningRateScheduler reducing learning rate to 0.0002996849727284952.\n",
      "Epoch 211/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1599\n",
      "Epoch 00211: val_loss improved from 0.44231 to 0.43622, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1599 - val_loss: 0.4362\n",
      "\n",
      "Epoch 00212: LearningRateScheduler reducing learning rate to 0.0002996819227243801.\n",
      "Epoch 212/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1680\n",
      "Epoch 00212: val_loss improved from 0.43622 to 0.42512, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1680 - val_loss: 0.4251\n",
      "\n",
      "Epoch 00213: LearningRateScheduler reducing learning rate to 0.0002996788580425032.\n",
      "Epoch 213/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1651\n",
      "Epoch 00213: val_loss improved from 0.42512 to 0.40797, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1651 - val_loss: 0.4080\n",
      "\n",
      "Epoch 00214: LearningRateScheduler reducing learning rate to 0.00029967577868316803.\n",
      "Epoch 214/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1505\n",
      "Epoch 00214: val_loss improved from 0.40797 to 0.39484, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1505 - val_loss: 0.3948\n",
      "\n",
      "Epoch 00215: LearningRateScheduler reducing learning rate to 0.00029967268464667964.\n",
      "Epoch 215/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1750\n",
      "Epoch 00215: val_loss improved from 0.39484 to 0.38658, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.1750 - val_loss: 0.3866\n",
      "\n",
      "Epoch 00216: LearningRateScheduler reducing learning rate to 0.00029966957593334444.\n",
      "Epoch 216/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1795\n",
      "Epoch 00216: val_loss improved from 0.38658 to 0.37957, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1795 - val_loss: 0.3796\n",
      "\n",
      "Epoch 00217: LearningRateScheduler reducing learning rate to 0.0002996664525434704.\n",
      "Epoch 217/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1783\n",
      "Epoch 00217: val_loss improved from 0.37957 to 0.37531, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1783 - val_loss: 0.3753\n",
      "\n",
      "Epoch 00218: LearningRateScheduler reducing learning rate to 0.00029966331447736687.\n",
      "Epoch 218/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1880\n",
      "Epoch 00218: val_loss improved from 0.37531 to 0.37065, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1880 - val_loss: 0.3706\n",
      "\n",
      "Epoch 00219: LearningRateScheduler reducing learning rate to 0.0002996601617353447.\n",
      "Epoch 219/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1677\n",
      "Epoch 00219: val_loss improved from 0.37065 to 0.36677, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1677 - val_loss: 0.3668\n",
      "\n",
      "Epoch 00220: LearningRateScheduler reducing learning rate to 0.0002996569943177162.\n",
      "Epoch 220/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1870\n",
      "Epoch 00220: val_loss improved from 0.36677 to 0.36185, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1870 - val_loss: 0.3618\n",
      "\n",
      "Epoch 00221: LearningRateScheduler reducing learning rate to 0.00029965381222479506.\n",
      "Epoch 221/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1878\n",
      "Epoch 00221: val_loss improved from 0.36185 to 0.35530, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1878 - val_loss: 0.3553\n",
      "\n",
      "Epoch 00222: LearningRateScheduler reducing learning rate to 0.0002996506154568965.\n",
      "Epoch 222/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1443\n",
      "Epoch 00222: val_loss improved from 0.35530 to 0.34789, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1443 - val_loss: 0.3479\n",
      "\n",
      "Epoch 00223: LearningRateScheduler reducing learning rate to 0.0002996474040143372.\n",
      "Epoch 223/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1648\n",
      "Epoch 00223: val_loss improved from 0.34789 to 0.33975, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1648 - val_loss: 0.3398\n",
      "\n",
      "Epoch 00224: LearningRateScheduler reducing learning rate to 0.0002996441778974351.\n",
      "Epoch 224/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1560\n",
      "Epoch 00224: val_loss improved from 0.33975 to 0.32853, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1560 - val_loss: 0.3285\n",
      "\n",
      "Epoch 00225: LearningRateScheduler reducing learning rate to 0.00029964093710650996.\n",
      "Epoch 225/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1611\n",
      "Epoch 00225: val_loss improved from 0.32853 to 0.31958, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1611 - val_loss: 0.3196\n",
      "\n",
      "Epoch 00226: LearningRateScheduler reducing learning rate to 0.00029963768164188275.\n",
      "Epoch 226/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1627\n",
      "Epoch 00226: val_loss improved from 0.31958 to 0.31441, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.1627 - val_loss: 0.3144\n",
      "\n",
      "Epoch 00227: LearningRateScheduler reducing learning rate to 0.00029963441150387585.\n",
      "Epoch 227/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1552\n",
      "Epoch 00227: val_loss improved from 0.31441 to 0.31145, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1552 - val_loss: 0.3114\n",
      "\n",
      "Epoch 00228: LearningRateScheduler reducing learning rate to 0.0002996311266928132.\n",
      "Epoch 228/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1452\n",
      "Epoch 00228: val_loss improved from 0.31145 to 0.30229, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1452 - val_loss: 0.3023\n",
      "\n",
      "Epoch 00229: LearningRateScheduler reducing learning rate to 0.0002996278272090202.\n",
      "Epoch 229/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1623\n",
      "Epoch 00229: val_loss improved from 0.30229 to 0.28731, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1623 - val_loss: 0.2873\n",
      "\n",
      "Epoch 00230: LearningRateScheduler reducing learning rate to 0.00029962451305282366.\n",
      "Epoch 230/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1757\n",
      "Epoch 00230: val_loss improved from 0.28731 to 0.28015, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1757 - val_loss: 0.2802\n",
      "\n",
      "Epoch 00231: LearningRateScheduler reducing learning rate to 0.00029962118422455187.\n",
      "Epoch 231/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1614\n",
      "Epoch 00231: val_loss improved from 0.28015 to 0.27433, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1614 - val_loss: 0.2743\n",
      "\n",
      "Epoch 00232: LearningRateScheduler reducing learning rate to 0.0002996178407245345.\n",
      "Epoch 232/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1779\n",
      "Epoch 00232: val_loss improved from 0.27433 to 0.26965, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1779 - val_loss: 0.2696\n",
      "\n",
      "Epoch 00233: LearningRateScheduler reducing learning rate to 0.00029961448255310275.\n",
      "Epoch 233/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1551\n",
      "Epoch 00233: val_loss improved from 0.26965 to 0.26459, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1551 - val_loss: 0.2646\n",
      "\n",
      "Epoch 00234: LearningRateScheduler reducing learning rate to 0.00029961110971058933.\n",
      "Epoch 234/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1536\n",
      "Epoch 00234: val_loss improved from 0.26459 to 0.25942, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1536 - val_loss: 0.2594\n",
      "\n",
      "Epoch 00235: LearningRateScheduler reducing learning rate to 0.00029960772219732816.\n",
      "Epoch 235/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1470\n",
      "Epoch 00235: val_loss improved from 0.25942 to 0.25621, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1470 - val_loss: 0.2562\n",
      "\n",
      "Epoch 00236: LearningRateScheduler reducing learning rate to 0.000299604320013655.\n",
      "Epoch 236/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1583\n",
      "Epoch 00236: val_loss improved from 0.25621 to 0.25491, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1583 - val_loss: 0.2549\n",
      "\n",
      "Epoch 00237: LearningRateScheduler reducing learning rate to 0.0002996009031599067.\n",
      "Epoch 237/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1704\n",
      "Epoch 00237: val_loss improved from 0.25491 to 0.25479, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1704 - val_loss: 0.2548\n",
      "\n",
      "Epoch 00238: LearningRateScheduler reducing learning rate to 0.00029959747163642173.\n",
      "Epoch 238/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1523\n",
      "Epoch 00238: val_loss improved from 0.25479 to 0.25183, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1523 - val_loss: 0.2518\n",
      "\n",
      "Epoch 00239: LearningRateScheduler reducing learning rate to 0.00029959402544354005.\n",
      "Epoch 239/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1640\n",
      "Epoch 00239: val_loss improved from 0.25183 to 0.24729, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1640 - val_loss: 0.2473\n",
      "\n",
      "Epoch 00240: LearningRateScheduler reducing learning rate to 0.00029959056458160295.\n",
      "Epoch 240/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1634\n",
      "Epoch 00240: val_loss improved from 0.24729 to 0.24651, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1634 - val_loss: 0.2465\n",
      "\n",
      "Epoch 00241: LearningRateScheduler reducing learning rate to 0.00029958708905095317.\n",
      "Epoch 241/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1552\n",
      "Epoch 00241: val_loss improved from 0.24651 to 0.24477, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1552 - val_loss: 0.2448\n",
      "\n",
      "Epoch 00242: LearningRateScheduler reducing learning rate to 0.0002995835988519352.\n",
      "Epoch 242/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1438\n",
      "Epoch 00242: val_loss improved from 0.24477 to 0.23852, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1438 - val_loss: 0.2385\n",
      "\n",
      "Epoch 00243: LearningRateScheduler reducing learning rate to 0.0002995800939848945.\n",
      "Epoch 243/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1491\n",
      "Epoch 00243: val_loss improved from 0.23852 to 0.22984, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1491 - val_loss: 0.2298\n",
      "\n",
      "Epoch 00244: LearningRateScheduler reducing learning rate to 0.0002995765744501783.\n",
      "Epoch 244/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1396\n",
      "Epoch 00244: val_loss improved from 0.22984 to 0.22029, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1396 - val_loss: 0.2203\n",
      "\n",
      "Epoch 00245: LearningRateScheduler reducing learning rate to 0.0002995730402481353.\n",
      "Epoch 245/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1754\n",
      "Epoch 00245: val_loss improved from 0.22029 to 0.21316, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1754 - val_loss: 0.2132\n",
      "\n",
      "Epoch 00246: LearningRateScheduler reducing learning rate to 0.0002995694913791156.\n",
      "Epoch 246/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1458\n",
      "Epoch 00246: val_loss improved from 0.21316 to 0.20859, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1458 - val_loss: 0.2086\n",
      "\n",
      "Epoch 00247: LearningRateScheduler reducing learning rate to 0.0002995659278434706.\n",
      "Epoch 247/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1704\n",
      "Epoch 00247: val_loss improved from 0.20859 to 0.20562, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1704 - val_loss: 0.2056\n",
      "\n",
      "Epoch 00248: LearningRateScheduler reducing learning rate to 0.0002995623496415533.\n",
      "Epoch 248/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1529\n",
      "Epoch 00248: val_loss improved from 0.20562 to 0.20257, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1529 - val_loss: 0.2026\n",
      "\n",
      "Epoch 00249: LearningRateScheduler reducing learning rate to 0.0002995587567737182.\n",
      "Epoch 249/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1503\n",
      "Epoch 00249: val_loss improved from 0.20257 to 0.20126, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1503 - val_loss: 0.2013\n",
      "\n",
      "Epoch 00250: LearningRateScheduler reducing learning rate to 0.0002995551492403211.\n",
      "Epoch 250/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1365\n",
      "Epoch 00250: val_loss improved from 0.20126 to 0.19724, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1365 - val_loss: 0.1972\n",
      "\n",
      "Epoch 00251: LearningRateScheduler reducing learning rate to 0.00029955152704171935.\n",
      "Epoch 251/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1632\n",
      "Epoch 00251: val_loss improved from 0.19724 to 0.19284, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1632 - val_loss: 0.1928\n",
      "\n",
      "Epoch 00252: LearningRateScheduler reducing learning rate to 0.00029954789017827186.\n",
      "Epoch 252/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1512\n",
      "Epoch 00252: val_loss improved from 0.19284 to 0.19174, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1512 - val_loss: 0.1917\n",
      "\n",
      "Epoch 00253: LearningRateScheduler reducing learning rate to 0.0002995442386503387.\n",
      "Epoch 253/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1475\n",
      "Epoch 00253: val_loss did not improve from 0.19174\n",
      "3/3 [==============================] - 0s 141ms/step - loss: 0.1475 - val_loss: 0.1939\n",
      "\n",
      "Epoch 00254: LearningRateScheduler reducing learning rate to 0.0002995405724582817.\n",
      "Epoch 254/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1762\n",
      "Epoch 00254: val_loss did not improve from 0.19174\n",
      "3/3 [==============================] - 0s 136ms/step - loss: 0.1762 - val_loss: 0.1979\n",
      "\n",
      "Epoch 00255: LearningRateScheduler reducing learning rate to 0.00029953689160246383.\n",
      "Epoch 255/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1483\n",
      "Epoch 00255: val_loss did not improve from 0.19174\n",
      "3/3 [==============================] - 0s 132ms/step - loss: 0.1483 - val_loss: 0.1993\n",
      "\n",
      "Epoch 00256: LearningRateScheduler reducing learning rate to 0.00029953319608324984.\n",
      "Epoch 256/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1729\n",
      "Epoch 00256: val_loss did not improve from 0.19174\n",
      "3/3 [==============================] - 0s 138ms/step - loss: 0.1729 - val_loss: 0.1993\n",
      "\n",
      "Epoch 00257: LearningRateScheduler reducing learning rate to 0.0002995294859010058.\n",
      "Epoch 257/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1370\n",
      "Epoch 00257: val_loss did not improve from 0.19174\n",
      "3/3 [==============================] - 0s 132ms/step - loss: 0.1370 - val_loss: 0.1971\n",
      "\n",
      "Epoch 00258: LearningRateScheduler reducing learning rate to 0.0002995257610560991.\n",
      "Epoch 258/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1600\n",
      "Epoch 00258: val_loss did not improve from 0.19174\n",
      "3/3 [==============================] - 0s 136ms/step - loss: 0.1600 - val_loss: 0.1926\n",
      "\n",
      "Epoch 00259: LearningRateScheduler reducing learning rate to 0.00029952202154889866.\n",
      "Epoch 259/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1506- ETA: 0s - loss: 0.157\n",
      "Epoch 00259: val_loss improved from 0.19174 to 0.19046, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1506 - val_loss: 0.1905\n",
      "\n",
      "Epoch 00260: LearningRateScheduler reducing learning rate to 0.0002995182673797751.\n",
      "Epoch 260/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1623\n",
      "Epoch 00260: val_loss improved from 0.19046 to 0.18715, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1623 - val_loss: 0.1872\n",
      "\n",
      "Epoch 00261: LearningRateScheduler reducing learning rate to 0.0002995144985491001.\n",
      "Epoch 261/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1434\n",
      "Epoch 00261: val_loss improved from 0.18715 to 0.18288, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1434 - val_loss: 0.1829\n",
      "\n",
      "Epoch 00262: LearningRateScheduler reducing learning rate to 0.000299510715057247.\n",
      "Epoch 262/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1401\n",
      "Epoch 00262: val_loss improved from 0.18288 to 0.17931, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1401 - val_loss: 0.1793\n",
      "\n",
      "Epoch 00263: LearningRateScheduler reducing learning rate to 0.0002995069169045906.\n",
      "Epoch 263/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1326\n",
      "Epoch 00263: val_loss improved from 0.17931 to 0.17627, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.1326 - val_loss: 0.1763\n",
      "\n",
      "Epoch 00264: LearningRateScheduler reducing learning rate to 0.00029950310409150707.\n",
      "Epoch 264/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1346\n",
      "Epoch 00264: val_loss improved from 0.17627 to 0.17502, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1346 - val_loss: 0.1750\n",
      "\n",
      "Epoch 00265: LearningRateScheduler reducing learning rate to 0.0002994992766183741.\n",
      "Epoch 265/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1348\n",
      "Epoch 00265: val_loss did not improve from 0.17502\n",
      "3/3 [==============================] - 0s 130ms/step - loss: 0.1348 - val_loss: 0.1755\n",
      "\n",
      "Epoch 00266: LearningRateScheduler reducing learning rate to 0.00029949543448557083.\n",
      "Epoch 266/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1466\n",
      "Epoch 00266: val_loss improved from 0.17502 to 0.17403, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1466 - val_loss: 0.1740\n",
      "\n",
      "Epoch 00267: LearningRateScheduler reducing learning rate to 0.00029949157769347784.\n",
      "Epoch 267/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1337\n",
      "Epoch 00267: val_loss improved from 0.17403 to 0.17182, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1337 - val_loss: 0.1718\n",
      "\n",
      "Epoch 00268: LearningRateScheduler reducing learning rate to 0.0002994877062424771.\n",
      "Epoch 268/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1402\n",
      "Epoch 00268: val_loss improved from 0.17182 to 0.16878, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.1402 - val_loss: 0.1688\n",
      "\n",
      "Epoch 00269: LearningRateScheduler reducing learning rate to 0.00029948382013295213.\n",
      "Epoch 269/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1561\n",
      "Epoch 00269: val_loss improved from 0.16878 to 0.16568, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1561 - val_loss: 0.1657\n",
      "\n",
      "Epoch 00270: LearningRateScheduler reducing learning rate to 0.00029947991936528785.\n",
      "Epoch 270/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1502\n",
      "Epoch 00270: val_loss improved from 0.16568 to 0.16291, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1502 - val_loss: 0.1629\n",
      "\n",
      "Epoch 00271: LearningRateScheduler reducing learning rate to 0.0002994760039398707.\n",
      "Epoch 271/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1361\n",
      "Epoch 00271: val_loss improved from 0.16291 to 0.16227, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1361 - val_loss: 0.1623\n",
      "\n",
      "Epoch 00272: LearningRateScheduler reducing learning rate to 0.00029947207385708837.\n",
      "Epoch 272/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1390\n",
      "Epoch 00272: val_loss improved from 0.16227 to 0.16108, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1390 - val_loss: 0.1611\n",
      "\n",
      "Epoch 00273: LearningRateScheduler reducing learning rate to 0.00029946812911733027.\n",
      "Epoch 273/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1389\n",
      "Epoch 00273: val_loss improved from 0.16108 to 0.15979, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1389 - val_loss: 0.1598\n",
      "\n",
      "Epoch 00274: LearningRateScheduler reducing learning rate to 0.00029946416972098704.\n",
      "Epoch 274/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1655\n",
      "Epoch 00274: val_loss improved from 0.15979 to 0.15870, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1655 - val_loss: 0.1587\n",
      "\n",
      "Epoch 00275: LearningRateScheduler reducing learning rate to 0.00029946019566845093.\n",
      "Epoch 275/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1462\n",
      "Epoch 00275: val_loss improved from 0.15870 to 0.15618, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.1462 - val_loss: 0.1562\n",
      "\n",
      "Epoch 00276: LearningRateScheduler reducing learning rate to 0.0002994562069601156.\n",
      "Epoch 276/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1324\n",
      "Epoch 00276: val_loss improved from 0.15618 to 0.15573, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1324 - val_loss: 0.1557\n",
      "\n",
      "Epoch 00277: LearningRateScheduler reducing learning rate to 0.00029945220359637605.\n",
      "Epoch 277/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1296\n",
      "Epoch 00277: val_loss improved from 0.15573 to 0.15419, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1296 - val_loss: 0.1542\n",
      "\n",
      "Epoch 00278: LearningRateScheduler reducing learning rate to 0.0002994481855776289.\n",
      "Epoch 278/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1352\n",
      "Epoch 00278: val_loss improved from 0.15419 to 0.15087, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.1352 - val_loss: 0.1509\n",
      "\n",
      "Epoch 00279: LearningRateScheduler reducing learning rate to 0.00029944415290427214.\n",
      "Epoch 279/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1507\n",
      "Epoch 00279: val_loss improved from 0.15087 to 0.14833, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1507 - val_loss: 0.1483\n",
      "\n",
      "Epoch 00280: LearningRateScheduler reducing learning rate to 0.0002994401055767052.\n",
      "Epoch 280/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1464\n",
      "Epoch 00280: val_loss improved from 0.14833 to 0.14816, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1464 - val_loss: 0.1482\n",
      "\n",
      "Epoch 00281: LearningRateScheduler reducing learning rate to 0.00029943604359532894.\n",
      "Epoch 281/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1150\n",
      "Epoch 00281: val_loss did not improve from 0.14816\n",
      "3/3 [==============================] - 0s 131ms/step - loss: 0.1150 - val_loss: 0.1495\n",
      "\n",
      "Epoch 00282: LearningRateScheduler reducing learning rate to 0.0002994319669605458.\n",
      "Epoch 282/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1405\n",
      "Epoch 00282: val_loss did not improve from 0.14816\n",
      "3/3 [==============================] - 0s 146ms/step - loss: 0.1405 - val_loss: 0.1520\n",
      "\n",
      "Epoch 00283: LearningRateScheduler reducing learning rate to 0.00029942787567275946.\n",
      "Epoch 283/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1157\n",
      "Epoch 00283: val_loss did not improve from 0.14816\n",
      "3/3 [==============================] - 0s 131ms/step - loss: 0.1157 - val_loss: 0.1525\n",
      "\n",
      "Epoch 00284: LearningRateScheduler reducing learning rate to 0.0002994237697323752.\n",
      "Epoch 284/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1380\n",
      "Epoch 00284: val_loss did not improve from 0.14816\n",
      "3/3 [==============================] - 0s 134ms/step - loss: 0.1380 - val_loss: 0.1509\n",
      "\n",
      "Epoch 00285: LearningRateScheduler reducing learning rate to 0.00029941964913979984.\n",
      "Epoch 285/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1424\n",
      "Epoch 00285: val_loss did not improve from 0.14816\n",
      "3/3 [==============================] - 0s 136ms/step - loss: 0.1424 - val_loss: 0.1489\n",
      "\n",
      "Epoch 00286: LearningRateScheduler reducing learning rate to 0.0002994155138954415.\n",
      "Epoch 286/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1354\n",
      "Epoch 00286: val_loss did not improve from 0.14816\n",
      "3/3 [==============================] - 0s 135ms/step - loss: 0.1354 - val_loss: 0.1484\n",
      "\n",
      "Epoch 00287: LearningRateScheduler reducing learning rate to 0.00029941136399970963.\n",
      "Epoch 287/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1272\n",
      "Epoch 00287: val_loss did not improve from 0.14816\n",
      "3/3 [==============================] - 0s 131ms/step - loss: 0.1272 - val_loss: 0.1487\n",
      "\n",
      "Epoch 00288: LearningRateScheduler reducing learning rate to 0.0002994071994530154.\n",
      "Epoch 288/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1370\n",
      "Epoch 00288: val_loss did not improve from 0.14816\n",
      "3/3 [==============================] - 0s 132ms/step - loss: 0.1370 - val_loss: 0.1504\n",
      "\n",
      "Epoch 00289: LearningRateScheduler reducing learning rate to 0.00029940302025577135.\n",
      "Epoch 289/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1285\n",
      "Epoch 00289: val_loss did not improve from 0.14816\n",
      "3/3 [==============================] - 0s 131ms/step - loss: 0.1285 - val_loss: 0.1508\n",
      "\n",
      "Epoch 00290: LearningRateScheduler reducing learning rate to 0.0002993988264083914.\n",
      "Epoch 290/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1431\n",
      "Epoch 00290: val_loss did not improve from 0.14816\n",
      "3/3 [==============================] - 0s 131ms/step - loss: 0.1431 - val_loss: 0.1495\n",
      "\n",
      "Epoch 00291: LearningRateScheduler reducing learning rate to 0.00029939461791129096.\n",
      "Epoch 291/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1378\n",
      "Epoch 00291: val_loss did not improve from 0.14816\n",
      "3/3 [==============================] - 0s 131ms/step - loss: 0.1378 - val_loss: 0.1484\n",
      "\n",
      "Epoch 00292: LearningRateScheduler reducing learning rate to 0.00029939039476488687.\n",
      "Epoch 292/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1301\n",
      "Epoch 00292: val_loss improved from 0.14816 to 0.14677, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1301 - val_loss: 0.1468\n",
      "\n",
      "Epoch 00293: LearningRateScheduler reducing learning rate to 0.0002993861569695975.\n",
      "Epoch 293/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1181\n",
      "Epoch 00293: val_loss improved from 0.14677 to 0.14343, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1181 - val_loss: 0.1434\n",
      "\n",
      "Epoch 00294: LearningRateScheduler reducing learning rate to 0.00029938190452584256.\n",
      "Epoch 294/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1313\n",
      "Epoch 00294: val_loss improved from 0.14343 to 0.13989, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1313 - val_loss: 0.1399\n",
      "\n",
      "Epoch 00295: LearningRateScheduler reducing learning rate to 0.0002993776374340433.\n",
      "Epoch 295/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1298\n",
      "Epoch 00295: val_loss improved from 0.13989 to 0.13589, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1298 - val_loss: 0.1359\n",
      "\n",
      "Epoch 00296: LearningRateScheduler reducing learning rate to 0.0002993733556946223.\n",
      "Epoch 296/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1423\n",
      "Epoch 00296: val_loss improved from 0.13589 to 0.13509, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1423 - val_loss: 0.1351\n",
      "\n",
      "Epoch 00297: LearningRateScheduler reducing learning rate to 0.00029936905930800383.\n",
      "Epoch 297/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1055\n",
      "Epoch 00297: val_loss improved from 0.13509 to 0.13422, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1055 - val_loss: 0.1342\n",
      "\n",
      "Epoch 00298: LearningRateScheduler reducing learning rate to 0.0002993647482746134.\n",
      "Epoch 298/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1549\n",
      "Epoch 00298: val_loss improved from 0.13422 to 0.13159, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1549 - val_loss: 0.1316\n",
      "\n",
      "Epoch 00299: LearningRateScheduler reducing learning rate to 0.0002993604225948779.\n",
      "Epoch 299/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1136\n",
      "Epoch 00299: val_loss improved from 0.13159 to 0.12864, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1136 - val_loss: 0.1286\n",
      "\n",
      "Epoch 00300: LearningRateScheduler reducing learning rate to 0.0002993560822692259.\n",
      "Epoch 300/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1418\n",
      "Epoch 00300: val_loss improved from 0.12864 to 0.12559, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1418 - val_loss: 0.1256\n",
      "\n",
      "Epoch 00301: LearningRateScheduler reducing learning rate to 0.00029935172729808737.\n",
      "Epoch 301/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1536\n",
      "Epoch 00301: val_loss improved from 0.12559 to 0.12462, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1536 - val_loss: 0.1246\n",
      "\n",
      "Epoch 00302: LearningRateScheduler reducing learning rate to 0.00029934735768189364.\n",
      "Epoch 302/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1336\n",
      "Epoch 00302: val_loss did not improve from 0.12462\n",
      "3/3 [==============================] - 0s 138ms/step - loss: 0.1336 - val_loss: 0.1279\n",
      "\n",
      "Epoch 00303: LearningRateScheduler reducing learning rate to 0.0002993429734210775.\n",
      "Epoch 303/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1235\n",
      "Epoch 00303: val_loss did not improve from 0.12462\n",
      "3/3 [==============================] - 0s 136ms/step - loss: 0.1235 - val_loss: 0.1311\n",
      "\n",
      "Epoch 00304: LearningRateScheduler reducing learning rate to 0.0002993385745160732.\n",
      "Epoch 304/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1358\n",
      "Epoch 00304: val_loss did not improve from 0.12462\n",
      "3/3 [==============================] - 0s 131ms/step - loss: 0.1358 - val_loss: 0.1300\n",
      "\n",
      "Epoch 00305: LearningRateScheduler reducing learning rate to 0.0002993341609673165.\n",
      "Epoch 305/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1439\n",
      "Epoch 00305: val_loss did not improve from 0.12462\n",
      "3/3 [==============================] - 0s 141ms/step - loss: 0.1439 - val_loss: 0.1291\n",
      "\n",
      "Epoch 00306: LearningRateScheduler reducing learning rate to 0.0002993297327752446.\n",
      "Epoch 306/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1359\n",
      "Epoch 00306: val_loss did not improve from 0.12462\n",
      "3/3 [==============================] - 0s 136ms/step - loss: 0.1359 - val_loss: 0.1254\n",
      "\n",
      "Epoch 00307: LearningRateScheduler reducing learning rate to 0.00029932528994029614.\n",
      "Epoch 307/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1354\n",
      "Epoch 00307: val_loss improved from 0.12462 to 0.12224, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1354 - val_loss: 0.1222\n",
      "\n",
      "Epoch 00308: LearningRateScheduler reducing learning rate to 0.0002993208324629111.\n",
      "Epoch 308/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1321\n",
      "Epoch 00308: val_loss improved from 0.12224 to 0.12087, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.1321 - val_loss: 0.1209\n",
      "\n",
      "Epoch 00309: LearningRateScheduler reducing learning rate to 0.00029931636034353105.\n",
      "Epoch 309/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1363\n",
      "Epoch 00309: val_loss improved from 0.12087 to 0.12022, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1363 - val_loss: 0.1202\n",
      "\n",
      "Epoch 00310: LearningRateScheduler reducing learning rate to 0.0002993118735825989.\n",
      "Epoch 310/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1411\n",
      "Epoch 00310: val_loss did not improve from 0.12022\n",
      "3/3 [==============================] - 0s 131ms/step - loss: 0.1411 - val_loss: 0.1203\n",
      "\n",
      "Epoch 00311: LearningRateScheduler reducing learning rate to 0.0002993073721805593.\n",
      "Epoch 311/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1253\n",
      "Epoch 00311: val_loss did not improve from 0.12022\n",
      "3/3 [==============================] - 0s 130ms/step - loss: 0.1253 - val_loss: 0.1211\n",
      "\n",
      "Epoch 00312: LearningRateScheduler reducing learning rate to 0.00029930285613785786.\n",
      "Epoch 312/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1267\n",
      "Epoch 00312: val_loss did not improve from 0.12022\n",
      "3/3 [==============================] - 0s 133ms/step - loss: 0.1267 - val_loss: 0.1235\n",
      "\n",
      "Epoch 00313: LearningRateScheduler reducing learning rate to 0.000299298325454942.\n",
      "Epoch 313/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1209\n",
      "Epoch 00313: val_loss did not improve from 0.12022\n",
      "3/3 [==============================] - 0s 131ms/step - loss: 0.1209 - val_loss: 0.1261\n",
      "\n",
      "Epoch 00314: LearningRateScheduler reducing learning rate to 0.00029929378013226056.\n",
      "Epoch 314/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1361\n",
      "Epoch 00314: val_loss did not improve from 0.12022\n",
      "3/3 [==============================] - 0s 131ms/step - loss: 0.1361 - val_loss: 0.1265\n",
      "\n",
      "Epoch 00315: LearningRateScheduler reducing learning rate to 0.00029928922017026364.\n",
      "Epoch 315/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1220\n",
      "Epoch 00315: val_loss did not improve from 0.12022\n",
      "3/3 [==============================] - 0s 143ms/step - loss: 0.1220 - val_loss: 0.1250\n",
      "\n",
      "Epoch 00316: LearningRateScheduler reducing learning rate to 0.00029928464556940303.\n",
      "Epoch 316/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1489\n",
      "Epoch 00316: val_loss did not improve from 0.12022\n",
      "3/3 [==============================] - 0s 131ms/step - loss: 0.1489 - val_loss: 0.1246\n",
      "\n",
      "Epoch 00317: LearningRateScheduler reducing learning rate to 0.00029928005633013177.\n",
      "Epoch 317/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1177\n",
      "Epoch 00317: val_loss did not improve from 0.12022\n",
      "3/3 [==============================] - 0s 144ms/step - loss: 0.1177 - val_loss: 0.1222\n",
      "\n",
      "Epoch 00318: LearningRateScheduler reducing learning rate to 0.00029927545245290457.\n",
      "Epoch 318/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1211\n",
      "Epoch 00318: val_loss did not improve from 0.12022\n",
      "3/3 [==============================] - 0s 131ms/step - loss: 0.1211 - val_loss: 0.1205\n",
      "\n",
      "Epoch 00319: LearningRateScheduler reducing learning rate to 0.0002992708339381772.\n",
      "Epoch 319/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1350\n",
      "Epoch 00319: val_loss did not improve from 0.12022\n",
      "3/3 [==============================] - 0s 131ms/step - loss: 0.1350 - val_loss: 0.1207\n",
      "\n",
      "Epoch 00320: LearningRateScheduler reducing learning rate to 0.00029926620078640743.\n",
      "Epoch 320/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1191\n",
      "Epoch 00320: val_loss did not improve from 0.12022\n",
      "3/3 [==============================] - 0s 129ms/step - loss: 0.1191 - val_loss: 0.1220\n",
      "\n",
      "Epoch 00321: LearningRateScheduler reducing learning rate to 0.00029926155299805397.\n",
      "Epoch 321/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1333\n",
      "Epoch 00321: val_loss did not improve from 0.12022\n",
      "3/3 [==============================] - 0s 131ms/step - loss: 0.1333 - val_loss: 0.1229\n",
      "\n",
      "Epoch 00322: LearningRateScheduler reducing learning rate to 0.0002992568905735773.\n",
      "Epoch 322/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1149\n",
      "Epoch 00322: val_loss did not improve from 0.12022\n",
      "3/3 [==============================] - 0s 132ms/step - loss: 0.1149 - val_loss: 0.1243\n",
      "\n",
      "Epoch 00323: LearningRateScheduler reducing learning rate to 0.0002992522135134392.\n",
      "Epoch 323/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1335\n",
      "Epoch 00323: val_loss did not improve from 0.12022\n",
      "3/3 [==============================] - 0s 166ms/step - loss: 0.1335 - val_loss: 0.1254\n",
      "\n",
      "Epoch 00324: LearningRateScheduler reducing learning rate to 0.00029924752181810293.\n",
      "Epoch 324/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1274\n",
      "Epoch 00324: val_loss did not improve from 0.12022\n",
      "3/3 [==============================] - 0s 134ms/step - loss: 0.1274 - val_loss: 0.1240\n",
      "\n",
      "Epoch 00325: LearningRateScheduler reducing learning rate to 0.0002992428154880333.\n",
      "Epoch 325/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1210\n",
      "Epoch 00325: val_loss did not improve from 0.12022\n",
      "3/3 [==============================] - 0s 134ms/step - loss: 0.1210 - val_loss: 0.1208\n",
      "\n",
      "Epoch 00326: LearningRateScheduler reducing learning rate to 0.0002992380945236964.\n",
      "Epoch 326/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1333\n",
      "Epoch 00326: val_loss improved from 0.12022 to 0.11671, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1333 - val_loss: 0.1167\n",
      "\n",
      "Epoch 00327: LearningRateScheduler reducing learning rate to 0.0002992333589255599.\n",
      "Epoch 327/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1311\n",
      "Epoch 00327: val_loss improved from 0.11671 to 0.11329, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1311 - val_loss: 0.1133\n",
      "\n",
      "Epoch 00328: LearningRateScheduler reducing learning rate to 0.0002992286086940928.\n",
      "Epoch 328/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1097\n",
      "Epoch 00328: val_loss improved from 0.11329 to 0.11137, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1097 - val_loss: 0.1114\n",
      "\n",
      "Epoch 00329: LearningRateScheduler reducing learning rate to 0.0002992238438297657.\n",
      "Epoch 329/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1063\n",
      "Epoch 00329: val_loss improved from 0.11137 to 0.11043, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1063 - val_loss: 0.1104\n",
      "\n",
      "Epoch 00330: LearningRateScheduler reducing learning rate to 0.00029921906433305053.\n",
      "Epoch 330/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1304\n",
      "Epoch 00330: val_loss did not improve from 0.11043\n",
      "3/3 [==============================] - 0s 138ms/step - loss: 0.1304 - val_loss: 0.1109\n",
      "\n",
      "Epoch 00331: LearningRateScheduler reducing learning rate to 0.0002992142702044207.\n",
      "Epoch 331/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1097\n",
      "Epoch 00331: val_loss did not improve from 0.11043\n",
      "3/3 [==============================] - 0s 132ms/step - loss: 0.1097 - val_loss: 0.1108\n",
      "\n",
      "Epoch 00332: LearningRateScheduler reducing learning rate to 0.00029920946144435114.\n",
      "Epoch 332/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1515\n",
      "Epoch 00332: val_loss did not improve from 0.11043\n",
      "3/3 [==============================] - 0s 151ms/step - loss: 0.1515 - val_loss: 0.1122\n",
      "\n",
      "Epoch 00333: LearningRateScheduler reducing learning rate to 0.0002992046380533181.\n",
      "Epoch 333/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1205\n",
      "Epoch 00333: val_loss did not improve from 0.11043\n",
      "3/3 [==============================] - 0s 130ms/step - loss: 0.1205 - val_loss: 0.1120\n",
      "\n",
      "Epoch 00334: LearningRateScheduler reducing learning rate to 0.00029919980003179943.\n",
      "Epoch 334/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1191\n",
      "Epoch 00334: val_loss did not improve from 0.11043\n",
      "3/3 [==============================] - 0s 131ms/step - loss: 0.1191 - val_loss: 0.1114\n",
      "\n",
      "Epoch 00335: LearningRateScheduler reducing learning rate to 0.00029919494738027424.\n",
      "Epoch 335/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1208\n",
      "Epoch 00335: val_loss did not improve from 0.11043\n",
      "3/3 [==============================] - 0s 131ms/step - loss: 0.1208 - val_loss: 0.1114\n",
      "\n",
      "Epoch 00336: LearningRateScheduler reducing learning rate to 0.0002991900800992232.\n",
      "Epoch 336/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1396\n",
      "Epoch 00336: val_loss did not improve from 0.11043\n",
      "3/3 [==============================] - 0s 135ms/step - loss: 0.1396 - val_loss: 0.1137\n",
      "\n",
      "Epoch 00337: LearningRateScheduler reducing learning rate to 0.00029918519818912856.\n",
      "Epoch 337/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1051\n",
      "Epoch 00337: val_loss did not improve from 0.11043\n",
      "3/3 [==============================] - 0s 133ms/step - loss: 0.1051 - val_loss: 0.1180\n",
      "\n",
      "Epoch 00338: LearningRateScheduler reducing learning rate to 0.00029918030165047377.\n",
      "Epoch 338/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1156\n",
      "Epoch 00338: val_loss did not improve from 0.11043\n",
      "3/3 [==============================] - 0s 135ms/step - loss: 0.1156 - val_loss: 0.1217\n",
      "\n",
      "Epoch 00339: LearningRateScheduler reducing learning rate to 0.0002991753904837439.\n",
      "Epoch 339/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1127\n",
      "Epoch 00339: val_loss did not improve from 0.11043\n",
      "3/3 [==============================] - 0s 133ms/step - loss: 0.1127 - val_loss: 0.1241\n",
      "\n",
      "Epoch 00340: LearningRateScheduler reducing learning rate to 0.00029917046468942536.\n",
      "Epoch 340/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1474\n",
      "Epoch 00340: val_loss did not improve from 0.11043\n",
      "3/3 [==============================] - 0s 130ms/step - loss: 0.1474 - val_loss: 0.1249\n",
      "\n",
      "Epoch 00341: LearningRateScheduler reducing learning rate to 0.00029916552426800607.\n",
      "Epoch 341/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1164\n",
      "Epoch 00341: val_loss did not improve from 0.11043\n",
      "3/3 [==============================] - 0s 146ms/step - loss: 0.1164 - val_loss: 0.1247\n",
      "\n",
      "Epoch 00342: LearningRateScheduler reducing learning rate to 0.00029916056921997544.\n",
      "Epoch 342/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1648\n",
      "Epoch 00342: val_loss did not improve from 0.11043\n",
      "3/3 [==============================] - 0s 131ms/step - loss: 0.1648 - val_loss: 0.1208\n",
      "\n",
      "Epoch 00343: LearningRateScheduler reducing learning rate to 0.00029915559954582423.\n",
      "Epoch 343/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1310\n",
      "Epoch 00343: val_loss did not improve from 0.11043\n",
      "3/3 [==============================] - 0s 131ms/step - loss: 0.1310 - val_loss: 0.1166\n",
      "\n",
      "Epoch 00344: LearningRateScheduler reducing learning rate to 0.00029915061524604474.\n",
      "Epoch 344/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1122\n",
      "Epoch 00344: val_loss improved from 0.11043 to 0.10933, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1122 - val_loss: 0.1093\n",
      "\n",
      "Epoch 00345: LearningRateScheduler reducing learning rate to 0.00029914561632113064.\n",
      "Epoch 345/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1336\n",
      "Epoch 00345: val_loss improved from 0.10933 to 0.10476, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1336 - val_loss: 0.1048\n",
      "\n",
      "Epoch 00346: LearningRateScheduler reducing learning rate to 0.00029914060277157705.\n",
      "Epoch 346/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1238\n",
      "Epoch 00346: val_loss did not improve from 0.10476\n",
      "3/3 [==============================] - 0s 138ms/step - loss: 0.1238 - val_loss: 0.1056\n",
      "\n",
      "Epoch 00347: LearningRateScheduler reducing learning rate to 0.0002991355745978807.\n",
      "Epoch 347/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1379\n",
      "Epoch 00347: val_loss did not improve from 0.10476\n",
      "3/3 [==============================] - 0s 133ms/step - loss: 0.1379 - val_loss: 0.1070\n",
      "\n",
      "Epoch 00348: LearningRateScheduler reducing learning rate to 0.0002991305318005395.\n",
      "Epoch 348/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1185\n",
      "Epoch 00348: val_loss did not improve from 0.10476\n",
      "3/3 [==============================] - 0s 137ms/step - loss: 0.1185 - val_loss: 0.1062\n",
      "\n",
      "Epoch 00349: LearningRateScheduler reducing learning rate to 0.00029912547438005305.\n",
      "Epoch 349/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1128\n",
      "Epoch 00349: val_loss did not improve from 0.10476\n",
      "3/3 [==============================] - 0s 138ms/step - loss: 0.1128 - val_loss: 0.1059\n",
      "\n",
      "Epoch 00350: LearningRateScheduler reducing learning rate to 0.00029912040233692224.\n",
      "Epoch 350/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1242\n",
      "Epoch 00350: val_loss did not improve from 0.10476\n",
      "3/3 [==============================] - 0s 155ms/step - loss: 0.1242 - val_loss: 0.1085\n",
      "\n",
      "Epoch 00351: LearningRateScheduler reducing learning rate to 0.0002991153156716495.\n",
      "Epoch 351/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1283\n",
      "Epoch 00351: val_loss did not improve from 0.10476\n",
      "3/3 [==============================] - 0s 138ms/step - loss: 0.1283 - val_loss: 0.1124\n",
      "\n",
      "Epoch 00352: LearningRateScheduler reducing learning rate to 0.00029911021438473864.\n",
      "Epoch 352/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1430\n",
      "Epoch 00352: val_loss did not improve from 0.10476\n",
      "3/3 [==============================] - 0s 130ms/step - loss: 0.1430 - val_loss: 0.1159\n",
      "\n",
      "Epoch 00353: LearningRateScheduler reducing learning rate to 0.000299105098476695.\n",
      "Epoch 353/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1284\n",
      "Epoch 00353: val_loss did not improve from 0.10476\n",
      "3/3 [==============================] - 0s 136ms/step - loss: 0.1284 - val_loss: 0.1187\n",
      "\n",
      "Epoch 00354: LearningRateScheduler reducing learning rate to 0.00029909996794802527.\n",
      "Epoch 354/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1129\n",
      "Epoch 00354: val_loss did not improve from 0.10476\n",
      "3/3 [==============================] - 0s 134ms/step - loss: 0.1129 - val_loss: 0.1203\n",
      "\n",
      "Epoch 00355: LearningRateScheduler reducing learning rate to 0.0002990948227992377.\n",
      "Epoch 355/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1131\n",
      "Epoch 00355: val_loss did not improve from 0.10476\n",
      "3/3 [==============================] - 0s 150ms/step - loss: 0.1131 - val_loss: 0.1214\n",
      "\n",
      "Epoch 00356: LearningRateScheduler reducing learning rate to 0.0002990896630308419.\n",
      "Epoch 356/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1240\n",
      "Epoch 00356: val_loss did not improve from 0.10476\n",
      "3/3 [==============================] - 0s 130ms/step - loss: 0.1240 - val_loss: 0.1222\n",
      "\n",
      "Epoch 00357: LearningRateScheduler reducing learning rate to 0.000299084488643349.\n",
      "Epoch 357/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1158\n",
      "Epoch 00357: val_loss did not improve from 0.10476\n",
      "3/3 [==============================] - 0s 133ms/step - loss: 0.1158 - val_loss: 0.1222\n",
      "\n",
      "Epoch 00358: LearningRateScheduler reducing learning rate to 0.00029907929963727143.\n",
      "Epoch 358/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1281\n",
      "Epoch 00358: val_loss did not improve from 0.10476\n",
      "3/3 [==============================] - 0s 135ms/step - loss: 0.1281 - val_loss: 0.1208\n",
      "\n",
      "Epoch 00359: LearningRateScheduler reducing learning rate to 0.00029907409601312326.\n",
      "Epoch 359/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0975\n",
      "Epoch 00359: val_loss did not improve from 0.10476\n",
      "3/3 [==============================] - 0s 131ms/step - loss: 0.0975 - val_loss: 0.1184\n",
      "\n",
      "Epoch 00360: LearningRateScheduler reducing learning rate to 0.00029906887777141994.\n",
      "Epoch 360/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1095\n",
      "Epoch 00360: val_loss did not improve from 0.10476\n",
      "3/3 [==============================] - 0s 137ms/step - loss: 0.1095 - val_loss: 0.1162\n",
      "\n",
      "Epoch 00361: LearningRateScheduler reducing learning rate to 0.00029906364491267824.\n",
      "Epoch 361/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1127\n",
      "Epoch 00361: val_loss did not improve from 0.10476\n",
      "3/3 [==============================] - 0s 127ms/step - loss: 0.1127 - val_loss: 0.1145\n",
      "\n",
      "Epoch 00362: LearningRateScheduler reducing learning rate to 0.0002990583974374166.\n",
      "Epoch 362/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1174\n",
      "Epoch 00362: val_loss did not improve from 0.10476\n",
      "3/3 [==============================] - 0s 134ms/step - loss: 0.1174 - val_loss: 0.1117\n",
      "\n",
      "Epoch 00363: LearningRateScheduler reducing learning rate to 0.00029905313534615466.\n",
      "Epoch 363/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1086\n",
      "Epoch 00363: val_loss did not improve from 0.10476\n",
      "3/3 [==============================] - 0s 129ms/step - loss: 0.1086 - val_loss: 0.1059\n",
      "\n",
      "Epoch 00364: LearningRateScheduler reducing learning rate to 0.00029904785863941384.\n",
      "Epoch 364/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1081\n",
      "Epoch 00364: val_loss improved from 0.10476 to 0.10182, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1081 - val_loss: 0.1018\n",
      "\n",
      "Epoch 00365: LearningRateScheduler reducing learning rate to 0.00029904256731771664.\n",
      "Epoch 365/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1203\n",
      "Epoch 00365: val_loss improved from 0.10182 to 0.10037, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1203 - val_loss: 0.1004\n",
      "\n",
      "Epoch 00366: LearningRateScheduler reducing learning rate to 0.00029903726138158726.\n",
      "Epoch 366/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1438\n",
      "Epoch 00366: val_loss improved from 0.10037 to 0.09986, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1438 - val_loss: 0.0999\n",
      "\n",
      "Epoch 00367: LearningRateScheduler reducing learning rate to 0.00029903194083155123.\n",
      "Epoch 367/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1025\n",
      "Epoch 00367: val_loss improved from 0.09986 to 0.09897, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1025 - val_loss: 0.0990\n",
      "\n",
      "Epoch 00368: LearningRateScheduler reducing learning rate to 0.0002990266056681356.\n",
      "Epoch 368/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1037\n",
      "Epoch 00368: val_loss did not improve from 0.09897\n",
      "3/3 [==============================] - 0s 131ms/step - loss: 0.1037 - val_loss: 0.0990\n",
      "\n",
      "Epoch 00369: LearningRateScheduler reducing learning rate to 0.0002990212558918688.\n",
      "Epoch 369/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1041\n",
      "Epoch 00369: val_loss did not improve from 0.09897\n",
      "3/3 [==============================] - 0s 130ms/step - loss: 0.1041 - val_loss: 0.0995\n",
      "\n",
      "Epoch 00370: LearningRateScheduler reducing learning rate to 0.00029901589150328067.\n",
      "Epoch 370/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1103\n",
      "Epoch 00370: val_loss did not improve from 0.09897\n",
      "3/3 [==============================] - 0s 128ms/step - loss: 0.1103 - val_loss: 0.1000\n",
      "\n",
      "Epoch 00371: LearningRateScheduler reducing learning rate to 0.00029901051250290275.\n",
      "Epoch 371/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1201\n",
      "Epoch 00371: val_loss did not improve from 0.09897\n",
      "3/3 [==============================] - 0s 132ms/step - loss: 0.1201 - val_loss: 0.1007\n",
      "\n",
      "Epoch 00372: LearningRateScheduler reducing learning rate to 0.00029900511889126763.\n",
      "Epoch 372/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1083\n",
      "Epoch 00372: val_loss did not improve from 0.09897\n",
      "3/3 [==============================] - 0s 130ms/step - loss: 0.1083 - val_loss: 0.1020\n",
      "\n",
      "Epoch 00373: LearningRateScheduler reducing learning rate to 0.0002989997106689097.\n",
      "Epoch 373/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1117\n",
      "Epoch 00373: val_loss did not improve from 0.09897\n",
      "3/3 [==============================] - 0s 132ms/step - loss: 0.1117 - val_loss: 0.1046\n",
      "\n",
      "Epoch 00374: LearningRateScheduler reducing learning rate to 0.0002989942878363647.\n",
      "Epoch 374/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1233\n",
      "Epoch 00374: val_loss did not improve from 0.09897\n",
      "3/3 [==============================] - 0s 126ms/step - loss: 0.1233 - val_loss: 0.1077\n",
      "\n",
      "Epoch 00375: LearningRateScheduler reducing learning rate to 0.00029898885039416956.\n",
      "Epoch 375/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1168\n",
      "Epoch 00375: val_loss did not improve from 0.09897\n",
      "3/3 [==============================] - 0s 133ms/step - loss: 0.1168 - val_loss: 0.1100\n",
      "\n",
      "Epoch 00376: LearningRateScheduler reducing learning rate to 0.0002989833983428631.\n",
      "Epoch 376/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1255\n",
      "Epoch 00376: val_loss did not improve from 0.09897\n",
      "3/3 [==============================] - 0s 132ms/step - loss: 0.1255 - val_loss: 0.1114\n",
      "\n",
      "Epoch 00377: LearningRateScheduler reducing learning rate to 0.00029897793168298523.\n",
      "Epoch 377/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1042\n",
      "Epoch 00377: val_loss did not improve from 0.09897\n",
      "3/3 [==============================] - 0s 148ms/step - loss: 0.1042 - val_loss: 0.1113\n",
      "\n",
      "Epoch 00378: LearningRateScheduler reducing learning rate to 0.00029897245041507746.\n",
      "Epoch 378/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1490\n",
      "Epoch 00378: val_loss did not improve from 0.09897\n",
      "3/3 [==============================] - 0s 133ms/step - loss: 0.1490 - val_loss: 0.1100\n",
      "\n",
      "Epoch 00379: LearningRateScheduler reducing learning rate to 0.0002989669545396827.\n",
      "Epoch 379/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1249\n",
      "Epoch 00379: val_loss did not improve from 0.09897\n",
      "3/3 [==============================] - 0s 131ms/step - loss: 0.1249 - val_loss: 0.1062\n",
      "\n",
      "Epoch 00380: LearningRateScheduler reducing learning rate to 0.0002989614440573454.\n",
      "Epoch 380/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1175\n",
      "Epoch 00380: val_loss did not improve from 0.09897\n",
      "3/3 [==============================] - 0s 130ms/step - loss: 0.1175 - val_loss: 0.1034\n",
      "\n",
      "Epoch 00381: LearningRateScheduler reducing learning rate to 0.00029895591896861133.\n",
      "Epoch 381/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1356\n",
      "Epoch 00381: val_loss did not improve from 0.09897\n",
      "3/3 [==============================] - 0s 136ms/step - loss: 0.1356 - val_loss: 0.1014\n",
      "\n",
      "Epoch 00382: LearningRateScheduler reducing learning rate to 0.0002989503792740278.\n",
      "Epoch 382/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1064\n",
      "Epoch 00382: val_loss did not improve from 0.09897\n",
      "3/3 [==============================] - 0s 130ms/step - loss: 0.1064 - val_loss: 0.1007\n",
      "\n",
      "Epoch 00383: LearningRateScheduler reducing learning rate to 0.00029894482497414346.\n",
      "Epoch 383/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1163\n",
      "Epoch 00383: val_loss did not improve from 0.09897\n",
      "3/3 [==============================] - 0s 132ms/step - loss: 0.1163 - val_loss: 0.1010\n",
      "\n",
      "Epoch 00384: LearningRateScheduler reducing learning rate to 0.0002989392560695085.\n",
      "Epoch 384/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1040\n",
      "Epoch 00384: val_loss did not improve from 0.09897\n",
      "3/3 [==============================] - 0s 137ms/step - loss: 0.1040 - val_loss: 0.1005\n",
      "\n",
      "Epoch 00385: LearningRateScheduler reducing learning rate to 0.0002989336725606746.\n",
      "Epoch 385/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1180\n",
      "Epoch 00385: val_loss did not improve from 0.09897\n",
      "3/3 [==============================] - 0s 133ms/step - loss: 0.1180 - val_loss: 0.0997\n",
      "\n",
      "Epoch 00386: LearningRateScheduler reducing learning rate to 0.0002989280744481948.\n",
      "Epoch 386/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1065\n",
      "Epoch 00386: val_loss did not improve from 0.09897\n",
      "3/3 [==============================] - 0s 129ms/step - loss: 0.1065 - val_loss: 0.0994\n",
      "\n",
      "Epoch 00387: LearningRateScheduler reducing learning rate to 0.00029892246173262357.\n",
      "Epoch 387/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1272\n",
      "Epoch 00387: val_loss did not improve from 0.09897\n",
      "3/3 [==============================] - 0s 126ms/step - loss: 0.1272 - val_loss: 0.1001\n",
      "\n",
      "Epoch 00388: LearningRateScheduler reducing learning rate to 0.00029891683441451685.\n",
      "Epoch 388/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1203\n",
      "Epoch 00388: val_loss improved from 0.09897 to 0.09794, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1203 - val_loss: 0.0979\n",
      "\n",
      "Epoch 00389: LearningRateScheduler reducing learning rate to 0.0002989111924944321.\n",
      "Epoch 389/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1046\n",
      "Epoch 00389: val_loss improved from 0.09794 to 0.09550, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1046 - val_loss: 0.0955\n",
      "\n",
      "Epoch 00390: LearningRateScheduler reducing learning rate to 0.00029890553597292807.\n",
      "Epoch 390/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1225\n",
      "Epoch 00390: val_loss improved from 0.09550 to 0.09470, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1225 - val_loss: 0.0947\n",
      "\n",
      "Epoch 00391: LearningRateScheduler reducing learning rate to 0.00029889986485056517.\n",
      "Epoch 391/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1169\n",
      "Epoch 00391: val_loss improved from 0.09470 to 0.09434, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1169 - val_loss: 0.0943\n",
      "\n",
      "Epoch 00392: LearningRateScheduler reducing learning rate to 0.000298894179127905.\n",
      "Epoch 392/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1222\n",
      "Epoch 00392: val_loss did not improve from 0.09434\n",
      "3/3 [==============================] - 0s 134ms/step - loss: 0.1222 - val_loss: 0.0944\n",
      "\n",
      "Epoch 00393: LearningRateScheduler reducing learning rate to 0.0002988884788055109.\n",
      "Epoch 393/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1229\n",
      "Epoch 00393: val_loss improved from 0.09434 to 0.09430, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1229 - val_loss: 0.0943\n",
      "\n",
      "Epoch 00394: LearningRateScheduler reducing learning rate to 0.0002988827638839474.\n",
      "Epoch 394/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1058\n",
      "Epoch 00394: val_loss did not improve from 0.09430\n",
      "3/3 [==============================] - 0s 125ms/step - loss: 0.1058 - val_loss: 0.0944\n",
      "\n",
      "Epoch 00395: LearningRateScheduler reducing learning rate to 0.00029887703436378056.\n",
      "Epoch 395/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1046\n",
      "Epoch 00395: val_loss did not improve from 0.09430\n",
      "3/3 [==============================] - 0s 147ms/step - loss: 0.1046 - val_loss: 0.0959\n",
      "\n",
      "Epoch 00396: LearningRateScheduler reducing learning rate to 0.00029887129024557793.\n",
      "Epoch 396/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1047\n",
      "Epoch 00396: val_loss did not improve from 0.09430\n",
      "3/3 [==============================] - 0s 130ms/step - loss: 0.1047 - val_loss: 0.0987\n",
      "\n",
      "Epoch 00397: LearningRateScheduler reducing learning rate to 0.00029886553152990857.\n",
      "Epoch 397/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1020\n",
      "Epoch 00397: val_loss did not improve from 0.09430\n",
      "3/3 [==============================] - 0s 133ms/step - loss: 0.1020 - val_loss: 0.1015\n",
      "\n",
      "Epoch 00398: LearningRateScheduler reducing learning rate to 0.00029885975821734276.\n",
      "Epoch 398/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1316\n",
      "Epoch 00398: val_loss did not improve from 0.09430\n",
      "3/3 [==============================] - 0s 135ms/step - loss: 0.1316 - val_loss: 0.1034\n",
      "\n",
      "Epoch 00399: LearningRateScheduler reducing learning rate to 0.0002988539703084524.\n",
      "Epoch 399/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1028\n",
      "Epoch 00399: val_loss did not improve from 0.09430\n",
      "3/3 [==============================] - 0s 133ms/step - loss: 0.1028 - val_loss: 0.1043\n",
      "\n",
      "Epoch 00400: LearningRateScheduler reducing learning rate to 0.0002988481678038108.\n",
      "Epoch 400/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0904\n",
      "Epoch 00400: val_loss did not improve from 0.09430\n",
      "3/3 [==============================] - 0s 137ms/step - loss: 0.0904 - val_loss: 0.1041\n",
      "\n",
      "Epoch 00401: LearningRateScheduler reducing learning rate to 0.00029884235070399277.\n",
      "Epoch 401/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1178\n",
      "Epoch 00401: val_loss did not improve from 0.09430\n",
      "3/3 [==============================] - 0s 132ms/step - loss: 0.1178 - val_loss: 0.1046\n",
      "\n",
      "Epoch 00402: LearningRateScheduler reducing learning rate to 0.00029883651900957444.\n",
      "Epoch 402/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1155\n",
      "Epoch 00402: val_loss did not improve from 0.09430\n",
      "3/3 [==============================] - 0s 131ms/step - loss: 0.1155 - val_loss: 0.1033\n",
      "\n",
      "Epoch 00403: LearningRateScheduler reducing learning rate to 0.00029883067272113345.\n",
      "Epoch 403/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1042\n",
      "Epoch 00403: val_loss did not improve from 0.09430\n",
      "3/3 [==============================] - 0s 134ms/step - loss: 0.1042 - val_loss: 0.1022\n",
      "\n",
      "Epoch 00404: LearningRateScheduler reducing learning rate to 0.0002988248118392489.\n",
      "Epoch 404/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1121\n",
      "Epoch 00404: val_loss did not improve from 0.09430\n",
      "3/3 [==============================] - 0s 131ms/step - loss: 0.1121 - val_loss: 0.1015\n",
      "\n",
      "Epoch 00405: LearningRateScheduler reducing learning rate to 0.0002988189363645013.\n",
      "Epoch 405/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1240\n",
      "Epoch 00405: val_loss did not improve from 0.09430\n",
      "3/3 [==============================] - 0s 129ms/step - loss: 0.1240 - val_loss: 0.1002\n",
      "\n",
      "Epoch 00406: LearningRateScheduler reducing learning rate to 0.00029881304629747273.\n",
      "Epoch 406/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0984\n",
      "Epoch 00406: val_loss did not improve from 0.09430\n",
      "3/3 [==============================] - 0s 130ms/step - loss: 0.0984 - val_loss: 0.0974\n",
      "\n",
      "Epoch 00407: LearningRateScheduler reducing learning rate to 0.0002988071416387465.\n",
      "Epoch 407/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1359\n",
      "Epoch 00407: val_loss did not improve from 0.09430\n",
      "3/3 [==============================] - 0s 129ms/step - loss: 0.1359 - val_loss: 0.0944\n",
      "\n",
      "Epoch 00408: LearningRateScheduler reducing learning rate to 0.00029880122238890754.\n",
      "Epoch 408/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1231\n",
      "Epoch 00408: val_loss did not improve from 0.09430\n",
      "3/3 [==============================] - 0s 145ms/step - loss: 0.1231 - val_loss: 0.0953\n",
      "\n",
      "Epoch 00409: LearningRateScheduler reducing learning rate to 0.00029879528854854213.\n",
      "Epoch 409/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1001\n",
      "Epoch 00409: val_loss did not improve from 0.09430\n",
      "3/3 [==============================] - 0s 138ms/step - loss: 0.1001 - val_loss: 0.0996\n",
      "\n",
      "Epoch 00410: LearningRateScheduler reducing learning rate to 0.0002987893401182381.\n",
      "Epoch 410/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1109\n",
      "Epoch 00410: val_loss did not improve from 0.09430\n",
      "3/3 [==============================] - 0s 131ms/step - loss: 0.1109 - val_loss: 0.1010\n",
      "\n",
      "Epoch 00411: LearningRateScheduler reducing learning rate to 0.00029878337709858457.\n",
      "Epoch 411/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1218\n",
      "Epoch 00411: val_loss did not improve from 0.09430\n",
      "3/3 [==============================] - 0s 130ms/step - loss: 0.1218 - val_loss: 0.1008\n",
      "\n",
      "Epoch 00412: LearningRateScheduler reducing learning rate to 0.00029877739949017224.\n",
      "Epoch 412/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1173\n",
      "Epoch 00412: val_loss did not improve from 0.09430\n",
      "3/3 [==============================] - 0s 148ms/step - loss: 0.1173 - val_loss: 0.0991\n",
      "\n",
      "Epoch 00413: LearningRateScheduler reducing learning rate to 0.0002987714072935932.\n",
      "Epoch 413/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1046\n",
      "Epoch 00413: val_loss did not improve from 0.09430\n",
      "3/3 [==============================] - 0s 131ms/step - loss: 0.1046 - val_loss: 0.0963\n",
      "\n",
      "Epoch 00414: LearningRateScheduler reducing learning rate to 0.00029876540050944103.\n",
      "Epoch 414/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1213\n",
      "Epoch 00414: val_loss improved from 0.09430 to 0.09413, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1213 - val_loss: 0.0941\n",
      "\n",
      "Epoch 00415: LearningRateScheduler reducing learning rate to 0.00029875937913831054.\n",
      "Epoch 415/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0961\n",
      "Epoch 00415: val_loss improved from 0.09413 to 0.09293, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.0961 - val_loss: 0.0929\n",
      "\n",
      "Epoch 00416: LearningRateScheduler reducing learning rate to 0.00029875334318079836.\n",
      "Epoch 416/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1025\n",
      "Epoch 00416: val_loss did not improve from 0.09293\n",
      "3/3 [==============================] - 0s 139ms/step - loss: 0.1025 - val_loss: 0.0933\n",
      "\n",
      "Epoch 00417: LearningRateScheduler reducing learning rate to 0.00029874729263750237.\n",
      "Epoch 417/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1028\n",
      "Epoch 00417: val_loss did not improve from 0.09293\n",
      "3/3 [==============================] - 0s 132ms/step - loss: 0.1028 - val_loss: 0.0938\n",
      "\n",
      "Epoch 00418: LearningRateScheduler reducing learning rate to 0.0002987412275090217.\n",
      "Epoch 418/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0907\n",
      "Epoch 00418: val_loss did not improve from 0.09293\n",
      "3/3 [==============================] - 0s 130ms/step - loss: 0.0907 - val_loss: 0.0945\n",
      "\n",
      "Epoch 00419: LearningRateScheduler reducing learning rate to 0.0002987351477959573.\n",
      "Epoch 419/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1073\n",
      "Epoch 00419: val_loss did not improve from 0.09293\n",
      "3/3 [==============================] - 0s 138ms/step - loss: 0.1073 - val_loss: 0.0943\n",
      "\n",
      "Epoch 00420: LearningRateScheduler reducing learning rate to 0.0002987290534989113.\n",
      "Epoch 420/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0959\n",
      "Epoch 00420: val_loss did not improve from 0.09293\n",
      "3/3 [==============================] - 0s 129ms/step - loss: 0.0959 - val_loss: 0.0943\n",
      "\n",
      "Epoch 00421: LearningRateScheduler reducing learning rate to 0.0002987229446184874.\n",
      "Epoch 421/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1010\n",
      "Epoch 00421: val_loss did not improve from 0.09293\n",
      "3/3 [==============================] - 0s 132ms/step - loss: 0.1010 - val_loss: 0.0947\n",
      "\n",
      "Epoch 00422: LearningRateScheduler reducing learning rate to 0.0002987168211552906.\n",
      "Epoch 422/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1376\n",
      "Epoch 00422: val_loss did not improve from 0.09293\n",
      "3/3 [==============================] - 0s 130ms/step - loss: 0.1376 - val_loss: 0.0947\n",
      "\n",
      "Epoch 00423: LearningRateScheduler reducing learning rate to 0.0002987106831099275.\n",
      "Epoch 423/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1058\n",
      "Epoch 00423: val_loss did not improve from 0.09293\n",
      "3/3 [==============================] - 0s 131ms/step - loss: 0.1058 - val_loss: 0.0940\n",
      "\n",
      "Epoch 00424: LearningRateScheduler reducing learning rate to 0.00029870453048300616.\n",
      "Epoch 424/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1277\n",
      "Epoch 00424: val_loss did not improve from 0.09293\n",
      "3/3 [==============================] - 0s 133ms/step - loss: 0.1277 - val_loss: 0.0938\n",
      "\n",
      "Epoch 00425: LearningRateScheduler reducing learning rate to 0.0002986983632751359.\n",
      "Epoch 425/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1149\n",
      "Epoch 00425: val_loss did not improve from 0.09293\n",
      "3/3 [==============================] - 0s 138ms/step - loss: 0.1149 - val_loss: 0.0930\n",
      "\n",
      "Epoch 00426: LearningRateScheduler reducing learning rate to 0.0002986921814869276.\n",
      "Epoch 426/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1340\n",
      "Epoch 00426: val_loss did not improve from 0.09293\n",
      "3/3 [==============================] - 0s 132ms/step - loss: 0.1340 - val_loss: 0.0935\n",
      "\n",
      "Epoch 00427: LearningRateScheduler reducing learning rate to 0.0002986859851189937.\n",
      "Epoch 427/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1120\n",
      "Epoch 00427: val_loss did not improve from 0.09293\n",
      "3/3 [==============================] - 0s 130ms/step - loss: 0.1120 - val_loss: 0.0946\n",
      "\n",
      "Epoch 00428: LearningRateScheduler reducing learning rate to 0.0002986797741719479.\n",
      "Epoch 428/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1002\n",
      "Epoch 00428: val_loss did not improve from 0.09293\n",
      "3/3 [==============================] - 0s 137ms/step - loss: 0.1002 - val_loss: 0.0957\n",
      "\n",
      "Epoch 00429: LearningRateScheduler reducing learning rate to 0.00029867354864640533.\n",
      "Epoch 429/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0957\n",
      "Epoch 00429: val_loss did not improve from 0.09293\n",
      "3/3 [==============================] - 0s 136ms/step - loss: 0.0957 - val_loss: 0.0974\n",
      "\n",
      "Epoch 00430: LearningRateScheduler reducing learning rate to 0.0002986673085429828.\n",
      "Epoch 430/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0923\n",
      "Epoch 00430: val_loss did not improve from 0.09293\n",
      "3/3 [==============================] - 0s 134ms/step - loss: 0.0923 - val_loss: 0.0984\n",
      "\n",
      "Epoch 00431: LearningRateScheduler reducing learning rate to 0.0002986610538622982.\n",
      "Epoch 431/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1024\n",
      "Epoch 00431: val_loss did not improve from 0.09293\n",
      "3/3 [==============================] - 0s 142ms/step - loss: 0.1024 - val_loss: 0.0981\n",
      "\n",
      "Epoch 00432: LearningRateScheduler reducing learning rate to 0.0002986547846049713.\n",
      "Epoch 432/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0885\n",
      "Epoch 00432: val_loss did not improve from 0.09293\n",
      "3/3 [==============================] - 0s 135ms/step - loss: 0.0885 - val_loss: 0.0980\n",
      "\n",
      "Epoch 00433: LearningRateScheduler reducing learning rate to 0.0002986485007716229.\n",
      "Epoch 433/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1106\n",
      "Epoch 00433: val_loss did not improve from 0.09293\n",
      "3/3 [==============================] - 0s 129ms/step - loss: 0.1106 - val_loss: 0.0969\n",
      "\n",
      "Epoch 00434: LearningRateScheduler reducing learning rate to 0.00029864220236287554.\n",
      "Epoch 434/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1089\n",
      "Epoch 00434: val_loss did not improve from 0.09293\n",
      "3/3 [==============================] - 0s 140ms/step - loss: 0.1089 - val_loss: 0.0951\n",
      "\n",
      "Epoch 00435: LearningRateScheduler reducing learning rate to 0.000298635889379353.\n",
      "Epoch 435/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1046\n",
      "Epoch 00435: val_loss did not improve from 0.09293\n",
      "3/3 [==============================] - 0s 143ms/step - loss: 0.1046 - val_loss: 0.0936\n",
      "\n",
      "Epoch 00436: LearningRateScheduler reducing learning rate to 0.00029862956182168067.\n",
      "Epoch 436/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1186\n",
      "Epoch 00436: val_loss did not improve from 0.09293\n",
      "3/3 [==============================] - 0s 129ms/step - loss: 0.1186 - val_loss: 0.0930\n",
      "\n",
      "Epoch 00437: LearningRateScheduler reducing learning rate to 0.0002986232196904853.\n",
      "Epoch 437/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0934\n",
      "Epoch 00437: val_loss improved from 0.09293 to 0.09179, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.0934 - val_loss: 0.0918\n",
      "\n",
      "Epoch 00438: LearningRateScheduler reducing learning rate to 0.0002986168629863951.\n",
      "Epoch 438/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1005\n",
      "Epoch 00438: val_loss improved from 0.09179 to 0.09067, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1005 - val_loss: 0.0907\n",
      "\n",
      "Epoch 00439: LearningRateScheduler reducing learning rate to 0.00029861049171003964.\n",
      "Epoch 439/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1134\n",
      "Epoch 00439: val_loss improved from 0.09067 to 0.08918, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1134 - val_loss: 0.0892\n",
      "\n",
      "Epoch 00440: LearningRateScheduler reducing learning rate to 0.00029860410586205014.\n",
      "Epoch 440/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1079\n",
      "Epoch 00440: val_loss improved from 0.08918 to 0.08786, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1079 - val_loss: 0.0879\n",
      "\n",
      "Epoch 00441: LearningRateScheduler reducing learning rate to 0.000298597705443059.\n",
      "Epoch 441/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1116\n",
      "Epoch 00441: val_loss improved from 0.08786 to 0.08664, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1116 - val_loss: 0.0866\n",
      "\n",
      "Epoch 00442: LearningRateScheduler reducing learning rate to 0.0002985912904537003.\n",
      "Epoch 442/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1043\n",
      "Epoch 00442: val_loss improved from 0.08664 to 0.08579, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.1043 - val_loss: 0.0858\n",
      "\n",
      "Epoch 00443: LearningRateScheduler reducing learning rate to 0.0002985848608946093.\n",
      "Epoch 443/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0952\n",
      "Epoch 00443: val_loss improved from 0.08579 to 0.08541, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.0952 - val_loss: 0.0854\n",
      "\n",
      "Epoch 00444: LearningRateScheduler reducing learning rate to 0.0002985784167664231.\n",
      "Epoch 444/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1460\n",
      "Epoch 00444: val_loss did not improve from 0.08541\n",
      "3/3 [==============================] - 0s 136ms/step - loss: 0.1460 - val_loss: 0.0872\n",
      "\n",
      "Epoch 00445: LearningRateScheduler reducing learning rate to 0.00029857195806977984.\n",
      "Epoch 445/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1139\n",
      "Epoch 00445: val_loss did not improve from 0.08541\n",
      "3/3 [==============================] - 0s 151ms/step - loss: 0.1139 - val_loss: 0.0894\n",
      "\n",
      "Epoch 00446: LearningRateScheduler reducing learning rate to 0.0002985654848053193.\n",
      "Epoch 446/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0965\n",
      "Epoch 00446: val_loss did not improve from 0.08541\n",
      "3/3 [==============================] - 0s 131ms/step - loss: 0.0965 - val_loss: 0.0915\n",
      "\n",
      "Epoch 00447: LearningRateScheduler reducing learning rate to 0.0002985589969736827.\n",
      "Epoch 447/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1034\n",
      "Epoch 00447: val_loss did not improve from 0.08541\n",
      "3/3 [==============================] - 0s 130ms/step - loss: 0.1034 - val_loss: 0.0940\n",
      "\n",
      "Epoch 00448: LearningRateScheduler reducing learning rate to 0.00029855249457551265.\n",
      "Epoch 448/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1079\n",
      "Epoch 00448: val_loss did not improve from 0.08541\n",
      "3/3 [==============================] - 0s 142ms/step - loss: 0.1079 - val_loss: 0.0966\n",
      "\n",
      "Epoch 00449: LearningRateScheduler reducing learning rate to 0.0002985459776114532.\n",
      "Epoch 449/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1087\n",
      "Epoch 00449: val_loss did not improve from 0.08541\n",
      "3/3 [==============================] - 0s 130ms/step - loss: 0.1087 - val_loss: 0.0977\n",
      "\n",
      "Epoch 00450: LearningRateScheduler reducing learning rate to 0.00029853944608214996.\n",
      "Epoch 450/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0939\n",
      "Epoch 00450: val_loss did not improve from 0.08541\n",
      "3/3 [==============================] - 0s 130ms/step - loss: 0.0939 - val_loss: 0.0963\n",
      "\n",
      "Epoch 00451: LearningRateScheduler reducing learning rate to 0.0002985328999882498.\n",
      "Epoch 451/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1280\n",
      "Epoch 00451: val_loss did not improve from 0.08541\n",
      "3/3 [==============================] - 0s 131ms/step - loss: 0.1280 - val_loss: 0.0945\n",
      "\n",
      "Epoch 00452: LearningRateScheduler reducing learning rate to 0.0002985263393304012.\n",
      "Epoch 452/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0901\n",
      "Epoch 00452: val_loss did not improve from 0.08541\n",
      "3/3 [==============================] - 0s 129ms/step - loss: 0.0901 - val_loss: 0.0920\n",
      "\n",
      "Epoch 00453: LearningRateScheduler reducing learning rate to 0.0002985197641092539.\n",
      "Epoch 453/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1157\n",
      "Epoch 00453: val_loss did not improve from 0.08541\n",
      "3/3 [==============================] - 0s 131ms/step - loss: 0.1157 - val_loss: 0.0886\n",
      "\n",
      "Epoch 00454: LearningRateScheduler reducing learning rate to 0.0002985131743254594.\n",
      "Epoch 454/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0989\n",
      "Epoch 00454: val_loss did not improve from 0.08541\n",
      "3/3 [==============================] - 0s 137ms/step - loss: 0.0989 - val_loss: 0.0860\n",
      "\n",
      "Epoch 00455: LearningRateScheduler reducing learning rate to 0.00029850656997967016.\n",
      "Epoch 455/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0980\n",
      "Epoch 00455: val_loss improved from 0.08541 to 0.08286, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.0980 - val_loss: 0.0829\n",
      "\n",
      "Epoch 00456: LearningRateScheduler reducing learning rate to 0.0002984999510725405.\n",
      "Epoch 456/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1024\n",
      "Epoch 00456: val_loss improved from 0.08286 to 0.08074, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.1024 - val_loss: 0.0807\n",
      "\n",
      "Epoch 00457: LearningRateScheduler reducing learning rate to 0.00029849331760472615.\n",
      "Epoch 457/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1201\n",
      "Epoch 00457: val_loss did not improve from 0.08074\n",
      "3/3 [==============================] - 0s 138ms/step - loss: 0.1201 - val_loss: 0.0819\n",
      "\n",
      "Epoch 00458: LearningRateScheduler reducing learning rate to 0.00029848666957688394.\n",
      "Epoch 458/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0874\n",
      "Epoch 00458: val_loss did not improve from 0.08074\n",
      "3/3 [==============================] - 0s 130ms/step - loss: 0.0874 - val_loss: 0.0827\n",
      "\n",
      "Epoch 00459: LearningRateScheduler reducing learning rate to 0.0002984800069896725.\n",
      "Epoch 459/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1132\n",
      "Epoch 00459: val_loss did not improve from 0.08074\n",
      "3/3 [==============================] - 0s 135ms/step - loss: 0.1132 - val_loss: 0.0831\n",
      "\n",
      "Epoch 00460: LearningRateScheduler reducing learning rate to 0.0002984733298437518.\n",
      "Epoch 460/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1067\n",
      "Epoch 00460: val_loss did not improve from 0.08074\n",
      "3/3 [==============================] - 0s 133ms/step - loss: 0.1067 - val_loss: 0.0828\n",
      "\n",
      "Epoch 00461: LearningRateScheduler reducing learning rate to 0.0002984666381397832.\n",
      "Epoch 461/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0953\n",
      "Epoch 00461: val_loss did not improve from 0.08074\n",
      "3/3 [==============================] - 0s 131ms/step - loss: 0.0953 - val_loss: 0.0829\n",
      "\n",
      "Epoch 00462: LearningRateScheduler reducing learning rate to 0.00029845993187842945.\n",
      "Epoch 462/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1028\n",
      "Epoch 00462: val_loss did not improve from 0.08074\n",
      "3/3 [==============================] - 0s 131ms/step - loss: 0.1028 - val_loss: 0.0832\n",
      "\n",
      "Epoch 00463: LearningRateScheduler reducing learning rate to 0.0002984532110603549.\n",
      "Epoch 463/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1030\n",
      "Epoch 00463: val_loss did not improve from 0.08074\n",
      "3/3 [==============================] - 0s 134ms/step - loss: 0.1030 - val_loss: 0.0835\n",
      "\n",
      "Epoch 00464: LearningRateScheduler reducing learning rate to 0.0002984464756862253.\n",
      "Epoch 464/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1105\n",
      "Epoch 00464: val_loss did not improve from 0.08074\n",
      "3/3 [==============================] - 0s 134ms/step - loss: 0.1105 - val_loss: 0.0841\n",
      "\n",
      "Epoch 00465: LearningRateScheduler reducing learning rate to 0.0002984397257567077.\n",
      "Epoch 465/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1206\n",
      "Epoch 00465: val_loss did not improve from 0.08074\n",
      "3/3 [==============================] - 0s 142ms/step - loss: 0.1206 - val_loss: 0.0844\n",
      "\n",
      "Epoch 00466: LearningRateScheduler reducing learning rate to 0.0002984329612724707.\n",
      "Epoch 466/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1128\n",
      "Epoch 00466: val_loss did not improve from 0.08074\n",
      "3/3 [==============================] - 0s 129ms/step - loss: 0.1128 - val_loss: 0.0862\n",
      "\n",
      "Epoch 00467: LearningRateScheduler reducing learning rate to 0.00029842618223418447.\n",
      "Epoch 467/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1077\n",
      "Epoch 00467: val_loss did not improve from 0.08074\n",
      "3/3 [==============================] - 0s 129ms/step - loss: 0.1077 - val_loss: 0.0885\n",
      "\n",
      "Epoch 00468: LearningRateScheduler reducing learning rate to 0.00029841938864252036.\n",
      "Epoch 468/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1028\n",
      "Epoch 00468: val_loss did not improve from 0.08074\n",
      "3/3 [==============================] - 0s 131ms/step - loss: 0.1028 - val_loss: 0.0907\n",
      "\n",
      "Epoch 00469: LearningRateScheduler reducing learning rate to 0.00029841258049815135.\n",
      "Epoch 469/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1073\n",
      "Epoch 00469: val_loss did not improve from 0.08074\n",
      "3/3 [==============================] - 0s 138ms/step - loss: 0.1073 - val_loss: 0.0925\n",
      "\n",
      "Epoch 00470: LearningRateScheduler reducing learning rate to 0.0002984057578017518.\n",
      "Epoch 470/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0977\n",
      "Epoch 00470: val_loss did not improve from 0.08074\n",
      "3/3 [==============================] - 0s 138ms/step - loss: 0.0977 - val_loss: 0.0929\n",
      "\n",
      "Epoch 00471: LearningRateScheduler reducing learning rate to 0.0002983989205539975.\n",
      "Epoch 471/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0979\n",
      "Epoch 00471: val_loss did not improve from 0.08074\n",
      "3/3 [==============================] - 0s 135ms/step - loss: 0.0979 - val_loss: 0.0932\n",
      "\n",
      "Epoch 00472: LearningRateScheduler reducing learning rate to 0.00029839206875556567.\n",
      "Epoch 472/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1171\n",
      "Epoch 00472: val_loss did not improve from 0.08074\n",
      "3/3 [==============================] - 0s 130ms/step - loss: 0.1171 - val_loss: 0.0926\n",
      "\n",
      "Epoch 00473: LearningRateScheduler reducing learning rate to 0.000298385202407135.\n",
      "Epoch 473/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1084\n",
      "Epoch 00473: val_loss did not improve from 0.08074\n",
      "3/3 [==============================] - 0s 132ms/step - loss: 0.1084 - val_loss: 0.0921\n",
      "\n",
      "Epoch 00474: LearningRateScheduler reducing learning rate to 0.0002983783215093857.\n",
      "Epoch 474/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0954\n",
      "Epoch 00474: val_loss did not improve from 0.08074\n",
      "3/3 [==============================] - 0s 126ms/step - loss: 0.0954 - val_loss: 0.0918\n",
      "\n",
      "Epoch 00475: LearningRateScheduler reducing learning rate to 0.0002983714260629993.\n",
      "Epoch 475/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0986\n",
      "Epoch 00475: val_loss did not improve from 0.08074\n",
      "3/3 [==============================] - 0s 132ms/step - loss: 0.0986 - val_loss: 0.0914\n",
      "\n",
      "Epoch 00476: LearningRateScheduler reducing learning rate to 0.0002983645160686587.\n",
      "Epoch 476/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1058\n",
      "Epoch 00476: val_loss did not improve from 0.08074\n",
      "3/3 [==============================] - 0s 139ms/step - loss: 0.1058 - val_loss: 0.0911\n",
      "\n",
      "Epoch 00477: LearningRateScheduler reducing learning rate to 0.0002983575915270485.\n",
      "Epoch 477/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1309\n",
      "Epoch 00477: val_loss did not improve from 0.08074\n",
      "3/3 [==============================] - 0s 133ms/step - loss: 0.1309 - val_loss: 0.0890\n",
      "\n",
      "Epoch 00478: LearningRateScheduler reducing learning rate to 0.00029835065243885453.\n",
      "Epoch 478/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0923\n",
      "Epoch 00478: val_loss did not improve from 0.08074\n",
      "3/3 [==============================] - 0s 131ms/step - loss: 0.0923 - val_loss: 0.0876\n",
      "\n",
      "Epoch 00479: LearningRateScheduler reducing learning rate to 0.0002983436988047641.\n",
      "Epoch 479/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1195\n",
      "Epoch 00479: val_loss did not improve from 0.08074\n",
      "3/3 [==============================] - 0s 134ms/step - loss: 0.1195 - val_loss: 0.0879\n",
      "\n",
      "Epoch 00480: LearningRateScheduler reducing learning rate to 0.00029833673062546606.\n",
      "Epoch 480/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1165\n",
      "Epoch 00480: val_loss did not improve from 0.08074\n",
      "3/3 [==============================] - 0s 131ms/step - loss: 0.1165 - val_loss: 0.0881\n",
      "\n",
      "Epoch 00481: LearningRateScheduler reducing learning rate to 0.0002983297479016505.\n",
      "Epoch 481/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1137\n",
      "Epoch 00481: val_loss did not improve from 0.08074\n",
      "3/3 [==============================] - 0s 137ms/step - loss: 0.1137 - val_loss: 0.0878\n",
      "\n",
      "Epoch 00482: LearningRateScheduler reducing learning rate to 0.0002983227506340092.\n",
      "Epoch 482/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1114\n",
      "Epoch 00482: val_loss did not improve from 0.08074\n",
      "3/3 [==============================] - 0s 131ms/step - loss: 0.1114 - val_loss: 0.0885\n",
      "\n",
      "Epoch 00483: LearningRateScheduler reducing learning rate to 0.0002983157388232352.\n",
      "Epoch 483/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1128\n",
      "Epoch 00483: val_loss did not improve from 0.08074\n",
      "3/3 [==============================] - 0s 159ms/step - loss: 0.1128 - val_loss: 0.0898\n",
      "\n",
      "Epoch 00484: LearningRateScheduler reducing learning rate to 0.00029830871247002303.\n",
      "Epoch 484/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1111\n",
      "Epoch 00484: val_loss did not improve from 0.08074\n",
      "3/3 [==============================] - 0s 134ms/step - loss: 0.1111 - val_loss: 0.0913\n",
      "\n",
      "Epoch 00485: LearningRateScheduler reducing learning rate to 0.00029830167157506867.\n",
      "Epoch 485/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0868\n",
      "Epoch 00485: val_loss did not improve from 0.08074\n",
      "3/3 [==============================] - 0s 128ms/step - loss: 0.0868 - val_loss: 0.0939\n",
      "\n",
      "Epoch 00486: LearningRateScheduler reducing learning rate to 0.00029829461613906956.\n",
      "Epoch 486/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0965\n",
      "Epoch 00486: val_loss did not improve from 0.08074\n",
      "3/3 [==============================] - 0s 131ms/step - loss: 0.0965 - val_loss: 0.0956\n",
      "\n",
      "Epoch 00487: LearningRateScheduler reducing learning rate to 0.00029828754616272453.\n",
      "Epoch 487/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0946\n",
      "Epoch 00487: val_loss did not improve from 0.08074\n",
      "3/3 [==============================] - 0s 131ms/step - loss: 0.0946 - val_loss: 0.0965\n",
      "\n",
      "Epoch 00488: LearningRateScheduler reducing learning rate to 0.00029828046164673386.\n",
      "Epoch 488/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0932\n",
      "Epoch 00488: val_loss did not improve from 0.08074\n",
      "3/3 [==============================] - 0s 131ms/step - loss: 0.0932 - val_loss: 0.0970\n",
      "\n",
      "Epoch 00489: LearningRateScheduler reducing learning rate to 0.00029827336259179935.\n",
      "Epoch 489/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1271\n",
      "Epoch 00489: val_loss did not improve from 0.08074\n",
      "3/3 [==============================] - 0s 132ms/step - loss: 0.1271 - val_loss: 0.0960\n",
      "\n",
      "Epoch 00490: LearningRateScheduler reducing learning rate to 0.0002982662489986241.\n",
      "Epoch 490/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0908\n",
      "Epoch 00490: val_loss did not improve from 0.08074\n",
      "3/3 [==============================] - 0s 130ms/step - loss: 0.0908 - val_loss: 0.0931\n",
      "\n",
      "Epoch 00491: LearningRateScheduler reducing learning rate to 0.00029825912086791284.\n",
      "Epoch 491/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0971\n",
      "Epoch 00491: val_loss did not improve from 0.08074\n",
      "3/3 [==============================] - 0s 134ms/step - loss: 0.0971 - val_loss: 0.0904\n",
      "\n",
      "Epoch 00492: LearningRateScheduler reducing learning rate to 0.0002982519782003715.\n",
      "Epoch 492/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1055\n",
      "Epoch 00492: val_loss did not improve from 0.08074\n",
      "3/3 [==============================] - 0s 142ms/step - loss: 0.1055 - val_loss: 0.0893\n",
      "\n",
      "Epoch 00493: LearningRateScheduler reducing learning rate to 0.00029824482099670766.\n",
      "Epoch 493/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1014\n",
      "Epoch 00493: val_loss did not improve from 0.08074\n",
      "3/3 [==============================] - 0s 131ms/step - loss: 0.1014 - val_loss: 0.0886\n",
      "\n",
      "Epoch 00494: LearningRateScheduler reducing learning rate to 0.00029823764925763027.\n",
      "Epoch 494/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1178\n",
      "Epoch 00494: val_loss did not improve from 0.08074\n",
      "3/3 [==============================] - 0s 139ms/step - loss: 0.1178 - val_loss: 0.0878\n",
      "\n",
      "Epoch 00495: LearningRateScheduler reducing learning rate to 0.0002982304629838496.\n",
      "Epoch 495/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1032\n",
      "Epoch 00495: val_loss did not improve from 0.08074\n",
      "3/3 [==============================] - 0s 150ms/step - loss: 0.1032 - val_loss: 0.0871\n",
      "\n",
      "Epoch 00496: LearningRateScheduler reducing learning rate to 0.0002982232621760776.\n",
      "Epoch 496/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0938\n",
      "Epoch 00496: val_loss did not improve from 0.08074\n",
      "3/3 [==============================] - 0s 132ms/step - loss: 0.0938 - val_loss: 0.0867\n",
      "\n",
      "Epoch 00497: LearningRateScheduler reducing learning rate to 0.0002982160468350274.\n",
      "Epoch 497/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1041\n",
      "Epoch 00497: val_loss did not improve from 0.08074\n",
      "3/3 [==============================] - 0s 132ms/step - loss: 0.1041 - val_loss: 0.0870\n",
      "\n",
      "Epoch 00498: LearningRateScheduler reducing learning rate to 0.00029820881696141383.\n",
      "Epoch 498/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1070\n",
      "Epoch 00498: val_loss did not improve from 0.08074\n",
      "3/3 [==============================] - 0s 135ms/step - loss: 0.1070 - val_loss: 0.0875\n",
      "\n",
      "Epoch 00499: LearningRateScheduler reducing learning rate to 0.0002982015725559529.\n",
      "Epoch 499/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1120\n",
      "Epoch 00499: val_loss did not improve from 0.08074\n",
      "3/3 [==============================] - 0s 133ms/step - loss: 0.1120 - val_loss: 0.0880\n",
      "\n",
      "Epoch 00500: LearningRateScheduler reducing learning rate to 0.00029819431361936225.\n",
      "Epoch 500/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0968\n",
      "Epoch 00500: val_loss did not improve from 0.08074\n",
      "3/3 [==============================] - 0s 132ms/step - loss: 0.0968 - val_loss: 0.0891\n",
      "\n",
      "Epoch 00501: LearningRateScheduler reducing learning rate to 0.0002981870401523609.\n",
      "Epoch 501/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0976\n",
      "Epoch 00501: val_loss did not improve from 0.08074\n",
      "3/3 [==============================] - 0s 133ms/step - loss: 0.0976 - val_loss: 0.0894\n",
      "\n",
      "Epoch 00502: LearningRateScheduler reducing learning rate to 0.0002981797521556693.\n",
      "Epoch 502/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1069\n",
      "Epoch 00502: val_loss did not improve from 0.08074\n",
      "3/3 [==============================] - 0s 151ms/step - loss: 0.1069 - val_loss: 0.0901\n",
      "\n",
      "Epoch 00503: LearningRateScheduler reducing learning rate to 0.00029817244963000936.\n",
      "Epoch 503/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0852\n",
      "Epoch 00503: val_loss did not improve from 0.08074\n",
      "3/3 [==============================] - 0s 132ms/step - loss: 0.0852 - val_loss: 0.0923\n",
      "\n",
      "Epoch 00504: LearningRateScheduler reducing learning rate to 0.0002981651325761043.\n",
      "Epoch 504/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0933\n",
      "Epoch 00504: val_loss did not improve from 0.08074\n",
      "3/3 [==============================] - 0s 130ms/step - loss: 0.0933 - val_loss: 0.0931\n",
      "\n",
      "Epoch 00505: LearningRateScheduler reducing learning rate to 0.00029815780099467907.\n",
      "Epoch 505/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1097\n",
      "Epoch 00505: val_loss did not improve from 0.08074\n",
      "3/3 [==============================] - 0s 131ms/step - loss: 0.1097 - val_loss: 0.0936\n",
      "\n",
      "Epoch 00506: LearningRateScheduler reducing learning rate to 0.00029815045488645973.\n",
      "Epoch 506/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1136\n",
      "Epoch 00506: val_loss did not improve from 0.08074\n",
      "3/3 [==============================] - 0s 130ms/step - loss: 0.1136 - val_loss: 0.0939\n",
      "\n",
      "Epoch 00507: LearningRateScheduler reducing learning rate to 0.000298143094252174.\n",
      "Epoch 507/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0987\n",
      "Epoch 00507: val_loss did not improve from 0.08074\n",
      "3/3 [==============================] - 0s 139ms/step - loss: 0.0987 - val_loss: 0.0931\n",
      "\n",
      "Epoch 00508: LearningRateScheduler reducing learning rate to 0.00029813571909255096.\n",
      "Epoch 508/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0985\n",
      "Epoch 00508: val_loss did not improve from 0.08074\n",
      "3/3 [==============================] - 0s 133ms/step - loss: 0.0985 - val_loss: 0.0918\n",
      "\n",
      "Epoch 00509: LearningRateScheduler reducing learning rate to 0.00029812832940832114.\n",
      "Epoch 509/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0902\n",
      "Epoch 00509: val_loss did not improve from 0.08074\n",
      "3/3 [==============================] - 0s 133ms/step - loss: 0.0902 - val_loss: 0.0912\n",
      "\n",
      "Epoch 00510: LearningRateScheduler reducing learning rate to 0.00029812092520021643.\n",
      "Epoch 510/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1047\n",
      "Epoch 00510: val_loss did not improve from 0.08074\n",
      "3/3 [==============================] - 0s 134ms/step - loss: 0.1047 - val_loss: 0.0913\n",
      "\n",
      "Epoch 00511: LearningRateScheduler reducing learning rate to 0.00029811350646897036.\n",
      "Epoch 511/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1110\n",
      "Epoch 00511: val_loss did not improve from 0.08074\n",
      "3/3 [==============================] - 0s 132ms/step - loss: 0.1110 - val_loss: 0.0897\n",
      "\n",
      "Epoch 00512: LearningRateScheduler reducing learning rate to 0.00029810607321531773.\n",
      "Epoch 512/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1036\n",
      "Epoch 00512: val_loss did not improve from 0.08074\n",
      "3/3 [==============================] - 0s 133ms/step - loss: 0.1036 - val_loss: 0.0881\n",
      "\n",
      "Epoch 00513: LearningRateScheduler reducing learning rate to 0.0002980986254399947.\n",
      "Epoch 513/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1010\n",
      "Epoch 00513: val_loss did not improve from 0.08074\n",
      "3/3 [==============================] - 0s 134ms/step - loss: 0.1010 - val_loss: 0.0866\n",
      "\n",
      "Epoch 00514: LearningRateScheduler reducing learning rate to 0.0002980911631437392.\n",
      "Epoch 514/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0772\n",
      "Epoch 00514: val_loss did not improve from 0.08074\n",
      "3/3 [==============================] - 0s 127ms/step - loss: 0.0772 - val_loss: 0.0845\n",
      "\n",
      "Epoch 00515: LearningRateScheduler reducing learning rate to 0.00029808368632729023.\n",
      "Epoch 515/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1071\n",
      "Epoch 00515: val_loss did not improve from 0.08074\n",
      "3/3 [==============================] - 0s 137ms/step - loss: 0.1071 - val_loss: 0.0836\n",
      "\n",
      "Epoch 00516: LearningRateScheduler reducing learning rate to 0.00029807619499138847.\n",
      "Epoch 516/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1215\n",
      "Epoch 00516: val_loss did not improve from 0.08074\n",
      "3/3 [==============================] - 0s 134ms/step - loss: 0.1215 - val_loss: 0.0835\n",
      "\n",
      "Epoch 00517: LearningRateScheduler reducing learning rate to 0.0002980686891367759.\n",
      "Epoch 517/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1104\n",
      "Epoch 00517: val_loss did not improve from 0.08074\n",
      "3/3 [==============================] - 0s 134ms/step - loss: 0.1104 - val_loss: 0.0839\n",
      "\n",
      "Epoch 00518: LearningRateScheduler reducing learning rate to 0.000298061168764196.\n",
      "Epoch 518/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0933\n",
      "Epoch 00518: val_loss did not improve from 0.08074\n",
      "3/3 [==============================] - 0s 135ms/step - loss: 0.0933 - val_loss: 0.0842\n",
      "\n",
      "Epoch 00519: LearningRateScheduler reducing learning rate to 0.0002980536338743937.\n",
      "Epoch 519/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1013\n",
      "Epoch 00519: val_loss did not improve from 0.08074\n",
      "3/3 [==============================] - 0s 136ms/step - loss: 0.1013 - val_loss: 0.0843\n",
      "\n",
      "Epoch 00520: LearningRateScheduler reducing learning rate to 0.00029804608446811534.\n",
      "Epoch 520/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1047\n",
      "Epoch 00520: val_loss did not improve from 0.08074\n",
      "3/3 [==============================] - 0s 131ms/step - loss: 0.1047 - val_loss: 0.0851\n",
      "\n",
      "Epoch 00521: LearningRateScheduler reducing learning rate to 0.0002980385205461087.\n",
      "Epoch 521/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1098\n",
      "Epoch 00521: val_loss did not improve from 0.08074\n",
      "3/3 [==============================] - 0s 140ms/step - loss: 0.1098 - val_loss: 0.0848\n",
      "\n",
      "Epoch 00522: LearningRateScheduler reducing learning rate to 0.000298030942109123.\n",
      "Epoch 522/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0984\n",
      "Epoch 00522: val_loss did not improve from 0.08074\n",
      "3/3 [==============================] - 0s 134ms/step - loss: 0.0984 - val_loss: 0.0848\n",
      "\n",
      "Epoch 00523: LearningRateScheduler reducing learning rate to 0.0002980233491579089.\n",
      "Epoch 523/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0991\n",
      "Epoch 00523: val_loss did not improve from 0.08074\n",
      "3/3 [==============================] - 0s 131ms/step - loss: 0.0991 - val_loss: 0.0860\n",
      "\n",
      "Epoch 00524: LearningRateScheduler reducing learning rate to 0.00029801574169321856.\n",
      "Epoch 524/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0946\n",
      "Epoch 00524: val_loss did not improve from 0.08074\n",
      "3/3 [==============================] - 0s 139ms/step - loss: 0.0946 - val_loss: 0.0876\n",
      "\n",
      "Epoch 00525: LearningRateScheduler reducing learning rate to 0.00029800811971580546.\n",
      "Epoch 525/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0985\n",
      "Epoch 00525: val_loss did not improve from 0.08074\n",
      "3/3 [==============================] - 0s 135ms/step - loss: 0.0985 - val_loss: 0.0892\n",
      "\n",
      "Epoch 00526: LearningRateScheduler reducing learning rate to 0.00029800048322642457.\n",
      "Epoch 526/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.116 - ETA: 0s - loss: 0.1131\n",
      "Epoch 00526: val_loss did not improve from 0.08074\n",
      "3/3 [==============================] - 0s 149ms/step - loss: 0.1131 - val_loss: 0.0908\n",
      "\n",
      "Epoch 00527: LearningRateScheduler reducing learning rate to 0.0002979928322258323.\n",
      "Epoch 527/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0984\n",
      "Epoch 00527: val_loss did not improve from 0.08074\n",
      "3/3 [==============================] - 0s 130ms/step - loss: 0.0984 - val_loss: 0.0915\n",
      "\n",
      "Epoch 00528: LearningRateScheduler reducing learning rate to 0.0002979851667147865.\n",
      "Epoch 528/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0994\n",
      "Epoch 00528: val_loss did not improve from 0.08074\n",
      "3/3 [==============================] - 0s 125ms/step - loss: 0.0994 - val_loss: 0.0921\n",
      "\n",
      "Epoch 00529: LearningRateScheduler reducing learning rate to 0.0002979774866940465.\n",
      "Epoch 529/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0868\n",
      "Epoch 00529: val_loss did not improve from 0.08074\n",
      "3/3 [==============================] - 0s 132ms/step - loss: 0.0868 - val_loss: 0.0914\n",
      "\n",
      "Epoch 00530: LearningRateScheduler reducing learning rate to 0.000297969792164373.\n",
      "Epoch 530/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0952\n",
      "Epoch 00530: val_loss did not improve from 0.08074\n",
      "3/3 [==============================] - 0s 137ms/step - loss: 0.0952 - val_loss: 0.0916\n",
      "\n",
      "Epoch 00531: LearningRateScheduler reducing learning rate to 0.00029796208312652815.\n",
      "Epoch 531/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0897\n",
      "Epoch 00531: val_loss did not improve from 0.08074\n",
      "3/3 [==============================] - 0s 131ms/step - loss: 0.0897 - val_loss: 0.0908\n",
      "\n",
      "Epoch 00532: LearningRateScheduler reducing learning rate to 0.0002979543595812755.\n",
      "Epoch 532/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1111\n",
      "Epoch 00532: val_loss did not improve from 0.08074\n",
      "3/3 [==============================] - 0s 135ms/step - loss: 0.1111 - val_loss: 0.0900\n",
      "\n",
      "Epoch 00533: LearningRateScheduler reducing learning rate to 0.0002979466215293802.\n",
      "Epoch 533/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1111\n",
      "Epoch 00533: val_loss did not improve from 0.08074\n",
      "3/3 [==============================] - 0s 135ms/step - loss: 0.1111 - val_loss: 0.0900\n",
      "\n",
      "Epoch 00534: LearningRateScheduler reducing learning rate to 0.0002979388689716087.\n",
      "Epoch 534/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0937\n",
      "Epoch 00534: val_loss did not improve from 0.08074\n",
      "3/3 [==============================] - 0s 143ms/step - loss: 0.0937 - val_loss: 0.0883\n",
      "\n",
      "Epoch 00535: LearningRateScheduler reducing learning rate to 0.00029793110190872883.\n",
      "Epoch 535/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1080\n",
      "Epoch 00535: val_loss did not improve from 0.08074\n",
      "3/3 [==============================] - 0s 131ms/step - loss: 0.1080 - val_loss: 0.0859\n",
      "\n",
      "Epoch 00536: LearningRateScheduler reducing learning rate to 0.00029792332034150995.\n",
      "Epoch 536/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1068\n",
      "Epoch 00536: val_loss did not improve from 0.08074\n",
      "3/3 [==============================] - 0s 139ms/step - loss: 0.1068 - val_loss: 0.0826\n",
      "\n",
      "Epoch 00537: LearningRateScheduler reducing learning rate to 0.00029791552427072286.\n",
      "Epoch 537/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0923\n",
      "Epoch 00537: val_loss improved from 0.08074 to 0.08028, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.0923 - val_loss: 0.0803\n",
      "\n",
      "Epoch 00538: LearningRateScheduler reducing learning rate to 0.00029790771369713983.\n",
      "Epoch 538/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0868\n",
      "Epoch 00538: val_loss improved from 0.08028 to 0.07862, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.0868 - val_loss: 0.0786\n",
      "\n",
      "Epoch 00539: LearningRateScheduler reducing learning rate to 0.0002978998886215344.\n",
      "Epoch 539/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1006\n",
      "Epoch 00539: val_loss improved from 0.07862 to 0.07749, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1006 - val_loss: 0.0775\n",
      "\n",
      "Epoch 00540: LearningRateScheduler reducing learning rate to 0.0002978920490446818.\n",
      "Epoch 540/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1006\n",
      "Epoch 00540: val_loss improved from 0.07749 to 0.07728, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1006 - val_loss: 0.0773\n",
      "\n",
      "Epoch 00541: LearningRateScheduler reducing learning rate to 0.00029788419496735844.\n",
      "Epoch 541/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1127\n",
      "Epoch 00541: val_loss did not improve from 0.07728\n",
      "3/3 [==============================] - 0s 136ms/step - loss: 0.1127 - val_loss: 0.0782\n",
      "\n",
      "Epoch 00542: LearningRateScheduler reducing learning rate to 0.0002978763263903424.\n",
      "Epoch 542/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0928\n",
      "Epoch 00542: val_loss did not improve from 0.07728\n",
      "3/3 [==============================] - 0s 144ms/step - loss: 0.0928 - val_loss: 0.0795\n",
      "\n",
      "Epoch 00543: LearningRateScheduler reducing learning rate to 0.00029786844331441297.\n",
      "Epoch 543/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1188\n",
      "Epoch 00543: val_loss did not improve from 0.07728\n",
      "3/3 [==============================] - 0s 143ms/step - loss: 0.1188 - val_loss: 0.0803\n",
      "\n",
      "Epoch 00544: LearningRateScheduler reducing learning rate to 0.0002978605457403511.\n",
      "Epoch 544/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1032\n",
      "Epoch 00544: val_loss did not improve from 0.07728\n",
      "3/3 [==============================] - 0s 137ms/step - loss: 0.1032 - val_loss: 0.0808\n",
      "\n",
      "Epoch 00545: LearningRateScheduler reducing learning rate to 0.0002978526336689389.\n",
      "Epoch 545/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0982\n",
      "Epoch 00545: val_loss did not improve from 0.07728\n",
      "3/3 [==============================] - 0s 143ms/step - loss: 0.0982 - val_loss: 0.0809\n",
      "\n",
      "Epoch 00546: LearningRateScheduler reducing learning rate to 0.00029784470710096016.\n",
      "Epoch 546/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1045\n",
      "Epoch 00546: val_loss did not improve from 0.07728\n",
      "3/3 [==============================] - 0s 139ms/step - loss: 0.1045 - val_loss: 0.0811\n",
      "\n",
      "Epoch 00547: LearningRateScheduler reducing learning rate to 0.0002978367660372001.\n",
      "Epoch 547/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1002\n",
      "Epoch 00547: val_loss did not improve from 0.07728\n",
      "3/3 [==============================] - 0s 150ms/step - loss: 0.1002 - val_loss: 0.0821\n",
      "\n",
      "Epoch 00548: LearningRateScheduler reducing learning rate to 0.0002978288104784453.\n",
      "Epoch 548/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1035\n",
      "Epoch 00548: val_loss did not improve from 0.07728\n",
      "3/3 [==============================] - 0s 140ms/step - loss: 0.1035 - val_loss: 0.0822\n",
      "\n",
      "Epoch 00549: LearningRateScheduler reducing learning rate to 0.0002978208404254836.\n",
      "Epoch 549/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1122\n",
      "Epoch 00549: val_loss did not improve from 0.07728\n",
      "3/3 [==============================] - 0s 134ms/step - loss: 0.1122 - val_loss: 0.0813\n",
      "\n",
      "Epoch 00550: LearningRateScheduler reducing learning rate to 0.0002978128558791046.\n",
      "Epoch 550/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0816\n",
      "Epoch 00550: val_loss did not improve from 0.07728\n",
      "3/3 [==============================] - 0s 132ms/step - loss: 0.0816 - val_loss: 0.0810\n",
      "\n",
      "Epoch 00551: LearningRateScheduler reducing learning rate to 0.0002978048568400992.\n",
      "Epoch 551/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1167\n",
      "Epoch 00551: val_loss did not improve from 0.07728\n",
      "3/3 [==============================] - 0s 136ms/step - loss: 0.1167 - val_loss: 0.0801\n",
      "\n",
      "Epoch 00552: LearningRateScheduler reducing learning rate to 0.0002977968433092597.\n",
      "Epoch 552/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0789\n",
      "Epoch 00552: val_loss did not improve from 0.07728\n",
      "3/3 [==============================] - 0s 130ms/step - loss: 0.0789 - val_loss: 0.0790\n",
      "\n",
      "Epoch 00553: LearningRateScheduler reducing learning rate to 0.00029778881528737976.\n",
      "Epoch 553/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1290\n",
      "Epoch 00553: val_loss did not improve from 0.07728\n",
      "3/3 [==============================] - 0s 134ms/step - loss: 0.1290 - val_loss: 0.0776\n",
      "\n",
      "Epoch 00554: LearningRateScheduler reducing learning rate to 0.0002977807727752547.\n",
      "Epoch 554/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0963\n",
      "Epoch 00554: val_loss did not improve from 0.07728\n",
      "3/3 [==============================] - 0s 137ms/step - loss: 0.0963 - val_loss: 0.0778\n",
      "\n",
      "Epoch 00555: LearningRateScheduler reducing learning rate to 0.0002977727157736811.\n",
      "Epoch 555/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1115\n",
      "Epoch 00555: val_loss did not improve from 0.07728\n",
      "3/3 [==============================] - 0s 133ms/step - loss: 0.1115 - val_loss: 0.0786\n",
      "\n",
      "Epoch 00556: LearningRateScheduler reducing learning rate to 0.000297764644283457.\n",
      "Epoch 556/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0976\n",
      "Epoch 00556: val_loss did not improve from 0.07728\n",
      "3/3 [==============================] - 0s 145ms/step - loss: 0.0976 - val_loss: 0.0803\n",
      "\n",
      "Epoch 00557: LearningRateScheduler reducing learning rate to 0.00029775655830538195.\n",
      "Epoch 557/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1028\n",
      "Epoch 00557: val_loss did not improve from 0.07728\n",
      "3/3 [==============================] - 0s 145ms/step - loss: 0.1028 - val_loss: 0.0812\n",
      "\n",
      "Epoch 00558: LearningRateScheduler reducing learning rate to 0.00029774845784025683.\n",
      "Epoch 558/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1146\n",
      "Epoch 00558: val_loss did not improve from 0.07728\n",
      "3/3 [==============================] - 0s 143ms/step - loss: 0.1146 - val_loss: 0.0819\n",
      "\n",
      "Epoch 00559: LearningRateScheduler reducing learning rate to 0.0002977403428888841.\n",
      "Epoch 559/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0842\n",
      "Epoch 00559: val_loss did not improve from 0.07728\n",
      "3/3 [==============================] - 0s 138ms/step - loss: 0.0842 - val_loss: 0.0818\n",
      "\n",
      "Epoch 00560: LearningRateScheduler reducing learning rate to 0.0002977322134520675.\n",
      "Epoch 560/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1036\n",
      "Epoch 00560: val_loss did not improve from 0.07728\n",
      "3/3 [==============================] - 0s 150ms/step - loss: 0.1036 - val_loss: 0.0831\n",
      "\n",
      "Epoch 00561: LearningRateScheduler reducing learning rate to 0.0002977240695306122.\n",
      "Epoch 561/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0896\n",
      "Epoch 00561: val_loss did not improve from 0.07728\n",
      "3/3 [==============================] - 0s 160ms/step - loss: 0.0896 - val_loss: 0.0862\n",
      "\n",
      "Epoch 00562: LearningRateScheduler reducing learning rate to 0.000297715911125325.\n",
      "Epoch 562/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0940\n",
      "Epoch 00562: val_loss did not improve from 0.07728\n",
      "3/3 [==============================] - 0s 153ms/step - loss: 0.0940 - val_loss: 0.0883\n",
      "\n",
      "Epoch 00563: LearningRateScheduler reducing learning rate to 0.00029770773823701403.\n",
      "Epoch 563/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0903\n",
      "Epoch 00563: val_loss did not improve from 0.07728\n",
      "3/3 [==============================] - 0s 136ms/step - loss: 0.0903 - val_loss: 0.0896\n",
      "\n",
      "Epoch 00564: LearningRateScheduler reducing learning rate to 0.0002976995508664886.\n",
      "Epoch 564/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0802\n",
      "Epoch 00564: val_loss did not improve from 0.07728\n",
      "3/3 [==============================] - 0s 135ms/step - loss: 0.0802 - val_loss: 0.0896\n",
      "\n",
      "Epoch 00565: LearningRateScheduler reducing learning rate to 0.00029769134901456.\n",
      "Epoch 565/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0970\n",
      "Epoch 00565: val_loss did not improve from 0.07728\n",
      "3/3 [==============================] - 0s 157ms/step - loss: 0.0970 - val_loss: 0.0885\n",
      "\n",
      "Epoch 00566: LearningRateScheduler reducing learning rate to 0.0002976831326820404.\n",
      "Epoch 566/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1100\n",
      "Epoch 00566: val_loss did not improve from 0.07728\n",
      "3/3 [==============================] - 0s 139ms/step - loss: 0.1100 - val_loss: 0.0863\n",
      "\n",
      "Epoch 00567: LearningRateScheduler reducing learning rate to 0.0002976749018697438.\n",
      "Epoch 567/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1154\n",
      "Epoch 00567: val_loss did not improve from 0.07728\n",
      "3/3 [==============================] - 0s 134ms/step - loss: 0.1154 - val_loss: 0.0837\n",
      "\n",
      "Epoch 00568: LearningRateScheduler reducing learning rate to 0.0002976666565784854.\n",
      "Epoch 568/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0943\n",
      "Epoch 00568: val_loss did not improve from 0.07728\n",
      "3/3 [==============================] - 0s 138ms/step - loss: 0.0943 - val_loss: 0.0820\n",
      "\n",
      "Epoch 00569: LearningRateScheduler reducing learning rate to 0.0002976583968090819.\n",
      "Epoch 569/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0844\n",
      "Epoch 00569: val_loss did not improve from 0.07728\n",
      "3/3 [==============================] - 0s 137ms/step - loss: 0.0844 - val_loss: 0.0795\n",
      "\n",
      "Epoch 00570: LearningRateScheduler reducing learning rate to 0.00029765012256235154.\n",
      "Epoch 570/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1115\n",
      "Epoch 00570: val_loss did not improve from 0.07728\n",
      "3/3 [==============================] - 0s 141ms/step - loss: 0.1115 - val_loss: 0.0782\n",
      "\n",
      "Epoch 00571: LearningRateScheduler reducing learning rate to 0.0002976418338391138.\n",
      "Epoch 571/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0915\n",
      "Epoch 00571: val_loss did not improve from 0.07728\n",
      "3/3 [==============================] - 0s 140ms/step - loss: 0.0915 - val_loss: 0.0786\n",
      "\n",
      "Epoch 00572: LearningRateScheduler reducing learning rate to 0.0002976335306401898.\n",
      "Epoch 572/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0852\n",
      "Epoch 00572: val_loss did not improve from 0.07728\n",
      "3/3 [==============================] - 0s 145ms/step - loss: 0.0852 - val_loss: 0.0781\n",
      "\n",
      "Epoch 00573: LearningRateScheduler reducing learning rate to 0.0002976252129664019.\n",
      "Epoch 573/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1057\n",
      "Epoch 00573: val_loss improved from 0.07728 to 0.07606, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.1057 - val_loss: 0.0761\n",
      "\n",
      "Epoch 00574: LearningRateScheduler reducing learning rate to 0.000297616880818574.\n",
      "Epoch 574/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0917\n",
      "Epoch 00574: val_loss improved from 0.07606 to 0.07353, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.0917 - val_loss: 0.0735\n",
      "\n",
      "Epoch 00575: LearningRateScheduler reducing learning rate to 0.00029760853419753147.\n",
      "Epoch 575/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0804\n",
      "Epoch 00575: val_loss improved from 0.07353 to 0.07149, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.0804 - val_loss: 0.0715\n",
      "\n",
      "Epoch 00576: LearningRateScheduler reducing learning rate to 0.000297600173104101.\n",
      "Epoch 576/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1035\n",
      "Epoch 00576: val_loss improved from 0.07149 to 0.07079, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.1035 - val_loss: 0.0708\n",
      "\n",
      "Epoch 00577: LearningRateScheduler reducing learning rate to 0.00029759179753911086.\n",
      "Epoch 577/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0989\n",
      "Epoch 00577: val_loss improved from 0.07079 to 0.07031, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.0989 - val_loss: 0.0703\n",
      "\n",
      "Epoch 00578: LearningRateScheduler reducing learning rate to 0.00029758340750339054.\n",
      "Epoch 578/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0885\n",
      "Epoch 00578: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 0s 159ms/step - loss: 0.0885 - val_loss: 0.0704\n",
      "\n",
      "Epoch 00579: LearningRateScheduler reducing learning rate to 0.00029757500299777126.\n",
      "Epoch 579/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1168\n",
      "Epoch 00579: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 0s 138ms/step - loss: 0.1168 - val_loss: 0.0719\n",
      "\n",
      "Epoch 00580: LearningRateScheduler reducing learning rate to 0.0002975665840230854.\n",
      "Epoch 580/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1094\n",
      "Epoch 00580: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 0s 161ms/step - loss: 0.1094 - val_loss: 0.0758\n",
      "\n",
      "Epoch 00581: LearningRateScheduler reducing learning rate to 0.00029755815058016684.\n",
      "Epoch 581/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0821\n",
      "Epoch 00581: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 0s 149ms/step - loss: 0.0821 - val_loss: 0.0802\n",
      "\n",
      "Epoch 00582: LearningRateScheduler reducing learning rate to 0.000297549702669851.\n",
      "Epoch 582/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0994\n",
      "Epoch 00582: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 0s 135ms/step - loss: 0.0994 - val_loss: 0.0850\n",
      "\n",
      "Epoch 00583: LearningRateScheduler reducing learning rate to 0.00029754124029297466.\n",
      "Epoch 583/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0947\n",
      "Epoch 00583: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 0s 136ms/step - loss: 0.0947 - val_loss: 0.0877\n",
      "\n",
      "Epoch 00584: LearningRateScheduler reducing learning rate to 0.00029753276345037603.\n",
      "Epoch 584/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0837\n",
      "Epoch 00584: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 0s 143ms/step - loss: 0.0837 - val_loss: 0.0897\n",
      "\n",
      "Epoch 00585: LearningRateScheduler reducing learning rate to 0.0002975242721428948.\n",
      "Epoch 585/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1027\n",
      "Epoch 00585: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 0s 144ms/step - loss: 0.1027 - val_loss: 0.0911\n",
      "\n",
      "Epoch 00586: LearningRateScheduler reducing learning rate to 0.0002975157663713719.\n",
      "Epoch 586/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0897\n",
      "Epoch 00586: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 0s 152ms/step - loss: 0.0897 - val_loss: 0.0918\n",
      "\n",
      "Epoch 00587: LearningRateScheduler reducing learning rate to 0.0002975072461366501.\n",
      "Epoch 587/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1207\n",
      "Epoch 00587: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 0s 133ms/step - loss: 0.1207 - val_loss: 0.0909\n",
      "\n",
      "Epoch 00588: LearningRateScheduler reducing learning rate to 0.0002974987114395731.\n",
      "Epoch 588/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1079\n",
      "Epoch 00588: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 0s 144ms/step - loss: 0.1079 - val_loss: 0.0887\n",
      "\n",
      "Epoch 00589: LearningRateScheduler reducing learning rate to 0.0002974901622809864.\n",
      "Epoch 589/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0877\n",
      "Epoch 00589: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 0s 145ms/step - loss: 0.0877 - val_loss: 0.0874\n",
      "\n",
      "Epoch 00590: LearningRateScheduler reducing learning rate to 0.0002974815986617369.\n",
      "Epoch 590/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0916\n",
      "Epoch 00590: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 0s 128ms/step - loss: 0.0916 - val_loss: 0.0856\n",
      "\n",
      "Epoch 00591: LearningRateScheduler reducing learning rate to 0.00029747302058267264.\n",
      "Epoch 591/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0965\n",
      "Epoch 00591: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 0s 155ms/step - loss: 0.0965 - val_loss: 0.0823\n",
      "\n",
      "Epoch 00592: LearningRateScheduler reducing learning rate to 0.0002974644280446434.\n",
      "Epoch 592/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0855\n",
      "Epoch 00592: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 0s 135ms/step - loss: 0.0855 - val_loss: 0.0814\n",
      "\n",
      "Epoch 00593: LearningRateScheduler reducing learning rate to 0.00029745582104850036.\n",
      "Epoch 593/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1135\n",
      "Epoch 00593: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 173ms/step - loss: 0.1135 - val_loss: 0.0821\n",
      "\n",
      "Epoch 00594: LearningRateScheduler reducing learning rate to 0.00029744719959509604.\n",
      "Epoch 594/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1022\n",
      "Epoch 00594: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 0s 138ms/step - loss: 0.1022 - val_loss: 0.0828\n",
      "\n",
      "Epoch 00595: LearningRateScheduler reducing learning rate to 0.0002974385636852843.\n",
      "Epoch 595/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0932\n",
      "Epoch 00595: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 0s 146ms/step - loss: 0.0932 - val_loss: 0.0814\n",
      "\n",
      "Epoch 00596: LearningRateScheduler reducing learning rate to 0.00029742991331992066.\n",
      "Epoch 596/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0927\n",
      "Epoch 00596: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 0s 139ms/step - loss: 0.0927 - val_loss: 0.0792\n",
      "\n",
      "Epoch 00597: LearningRateScheduler reducing learning rate to 0.00029742124849986193.\n",
      "Epoch 597/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0954\n",
      "Epoch 00597: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 0s 143ms/step - loss: 0.0954 - val_loss: 0.0774\n",
      "\n",
      "Epoch 00598: LearningRateScheduler reducing learning rate to 0.0002974125692259663.\n",
      "Epoch 598/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1022\n",
      "Epoch 00598: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 0s 133ms/step - loss: 0.1022 - val_loss: 0.0757\n",
      "\n",
      "Epoch 00599: LearningRateScheduler reducing learning rate to 0.0002974038754990936.\n",
      "Epoch 599/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1032\n",
      "Epoch 00599: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 0s 157ms/step - loss: 0.1032 - val_loss: 0.0755\n",
      "\n",
      "Epoch 00600: LearningRateScheduler reducing learning rate to 0.0002973951673201049.\n",
      "Epoch 600/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0983\n",
      "Epoch 00600: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 0s 137ms/step - loss: 0.0983 - val_loss: 0.0746\n",
      "\n",
      "Epoch 00601: LearningRateScheduler reducing learning rate to 0.0002973864446898628.\n",
      "Epoch 601/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1005\n",
      "Epoch 00601: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 0s 141ms/step - loss: 0.1005 - val_loss: 0.0738\n",
      "\n",
      "Epoch 00602: LearningRateScheduler reducing learning rate to 0.00029737770760923123.\n",
      "Epoch 602/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1157\n",
      "Epoch 00602: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 0s 135ms/step - loss: 0.1157 - val_loss: 0.0750\n",
      "\n",
      "Epoch 00603: LearningRateScheduler reducing learning rate to 0.0002973689560790757.\n",
      "Epoch 603/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0932\n",
      "Epoch 00603: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 0s 135ms/step - loss: 0.0932 - val_loss: 0.0764\n",
      "\n",
      "Epoch 00604: LearningRateScheduler reducing learning rate to 0.00029736019010026304.\n",
      "Epoch 604/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0834\n",
      "Epoch 00604: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 0s 130ms/step - loss: 0.0834 - val_loss: 0.0779\n",
      "\n",
      "Epoch 00605: LearningRateScheduler reducing learning rate to 0.0002973514096736615.\n",
      "Epoch 605/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1126\n",
      "Epoch 00605: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 0s 139ms/step - loss: 0.1126 - val_loss: 0.0785\n",
      "\n",
      "Epoch 00606: LearningRateScheduler reducing learning rate to 0.0002973426148001408.\n",
      "Epoch 606/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0861\n",
      "Epoch 00606: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 0s 131ms/step - loss: 0.0861 - val_loss: 0.0794\n",
      "\n",
      "Epoch 00607: LearningRateScheduler reducing learning rate to 0.00029733380548057216.\n",
      "Epoch 607/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1094\n",
      "Epoch 00607: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 167ms/step - loss: 0.1094 - val_loss: 0.0804\n",
      "\n",
      "Epoch 00608: LearningRateScheduler reducing learning rate to 0.0002973249817158281.\n",
      "Epoch 608/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0892\n",
      "Epoch 00608: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 0s 140ms/step - loss: 0.0892 - val_loss: 0.0823\n",
      "\n",
      "Epoch 00609: LearningRateScheduler reducing learning rate to 0.0002973161435067827.\n",
      "Epoch 609/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0961\n",
      "Epoch 00609: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 0s 135ms/step - loss: 0.0961 - val_loss: 0.0826\n",
      "\n",
      "Epoch 00610: LearningRateScheduler reducing learning rate to 0.00029730729085431136.\n",
      "Epoch 610/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0844\n",
      "Epoch 00610: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 0s 136ms/step - loss: 0.0844 - val_loss: 0.0829\n",
      "\n",
      "Epoch 00611: LearningRateScheduler reducing learning rate to 0.0002972984237592909.\n",
      "Epoch 611/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0928\n",
      "Epoch 00611: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 0s 136ms/step - loss: 0.0928 - val_loss: 0.0822\n",
      "\n",
      "Epoch 00612: LearningRateScheduler reducing learning rate to 0.0002972895422225997.\n",
      "Epoch 612/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0887\n",
      "Epoch 00612: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 0s 137ms/step - loss: 0.0887 - val_loss: 0.0822\n",
      "\n",
      "Epoch 00613: LearningRateScheduler reducing learning rate to 0.00029728064624511756.\n",
      "Epoch 613/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0801\n",
      "Epoch 00613: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 0s 138ms/step - loss: 0.0801 - val_loss: 0.0838\n",
      "\n",
      "Epoch 00614: LearningRateScheduler reducing learning rate to 0.0002972717358277255.\n",
      "Epoch 614/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0704\n",
      "Epoch 00614: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 0s 135ms/step - loss: 0.0704 - val_loss: 0.0860\n",
      "\n",
      "Epoch 00615: LearningRateScheduler reducing learning rate to 0.00029726281097130624.\n",
      "Epoch 615/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0802\n",
      "Epoch 00615: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 0s 155ms/step - loss: 0.0802 - val_loss: 0.0861\n",
      "\n",
      "Epoch 00616: LearningRateScheduler reducing learning rate to 0.0002972538716767437.\n",
      "Epoch 616/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0996\n",
      "Epoch 00616: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 0s 143ms/step - loss: 0.0996 - val_loss: 0.0857\n",
      "\n",
      "Epoch 00617: LearningRateScheduler reducing learning rate to 0.00029724491794492347.\n",
      "Epoch 617/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0961\n",
      "Epoch 00617: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 0s 132ms/step - loss: 0.0961 - val_loss: 0.0850\n",
      "\n",
      "Epoch 00618: LearningRateScheduler reducing learning rate to 0.0002972359497767323.\n",
      "Epoch 618/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1038\n",
      "Epoch 00618: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 0s 132ms/step - loss: 0.1038 - val_loss: 0.0837\n",
      "\n",
      "Epoch 00619: LearningRateScheduler reducing learning rate to 0.00029722696717305854.\n",
      "Epoch 619/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0948\n",
      "Epoch 00619: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 0s 133ms/step - loss: 0.0948 - val_loss: 0.0811\n",
      "\n",
      "Epoch 00620: LearningRateScheduler reducing learning rate to 0.000297217970134792.\n",
      "Epoch 620/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0944\n",
      "Epoch 00620: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 0s 132ms/step - loss: 0.0944 - val_loss: 0.0773\n",
      "\n",
      "Epoch 00621: LearningRateScheduler reducing learning rate to 0.00029720895866282385.\n",
      "Epoch 621/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1033\n",
      "Epoch 00621: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 0s 138ms/step - loss: 0.1033 - val_loss: 0.0755\n",
      "\n",
      "Epoch 00622: LearningRateScheduler reducing learning rate to 0.00029719993275804664.\n",
      "Epoch 622/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1042\n",
      "Epoch 00622: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 0s 135ms/step - loss: 0.1042 - val_loss: 0.0765\n",
      "\n",
      "Epoch 00623: LearningRateScheduler reducing learning rate to 0.0002971908924213545.\n",
      "Epoch 623/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1147\n",
      "Epoch 00623: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 0s 147ms/step - loss: 0.1147 - val_loss: 0.0793\n",
      "\n",
      "Epoch 00624: LearningRateScheduler reducing learning rate to 0.0002971818376536427.\n",
      "Epoch 624/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1144\n",
      "Epoch 00624: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 0s 134ms/step - loss: 0.1144 - val_loss: 0.0820\n",
      "\n",
      "Epoch 00625: LearningRateScheduler reducing learning rate to 0.0002971727684558084.\n",
      "Epoch 625/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1053\n",
      "Epoch 00625: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 0s 134ms/step - loss: 0.1053 - val_loss: 0.0845\n",
      "\n",
      "Epoch 00626: LearningRateScheduler reducing learning rate to 0.0002971636848287498.\n",
      "Epoch 626/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0822\n",
      "Epoch 00626: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 0s 133ms/step - loss: 0.0822 - val_loss: 0.0859\n",
      "\n",
      "Epoch 00627: LearningRateScheduler reducing learning rate to 0.00029715458677336654.\n",
      "Epoch 627/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0976\n",
      "Epoch 00627: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 0s 133ms/step - loss: 0.0976 - val_loss: 0.0875\n",
      "\n",
      "Epoch 00628: LearningRateScheduler reducing learning rate to 0.00029714547429056.\n",
      "Epoch 628/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0872\n",
      "Epoch 00628: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 0s 135ms/step - loss: 0.0872 - val_loss: 0.0884\n",
      "\n",
      "Epoch 00629: LearningRateScheduler reducing learning rate to 0.00029713634738123265.\n",
      "Epoch 629/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1187\n",
      "Epoch 00629: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 0s 158ms/step - loss: 0.1187 - val_loss: 0.0889\n",
      "\n",
      "Epoch 00630: LearningRateScheduler reducing learning rate to 0.00029712720604628865.\n",
      "Epoch 630/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1112\n",
      "Epoch 00630: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 0s 132ms/step - loss: 0.1112 - val_loss: 0.0886\n",
      "\n",
      "Epoch 00631: LearningRateScheduler reducing learning rate to 0.0002971180502866334.\n",
      "Epoch 631/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0926\n",
      "Epoch 00631: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 0s 135ms/step - loss: 0.0926 - val_loss: 0.0874\n",
      "\n",
      "Epoch 00632: LearningRateScheduler reducing learning rate to 0.00029710888010317375.\n",
      "Epoch 632/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0996\n",
      "Epoch 00632: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 0s 142ms/step - loss: 0.0996 - val_loss: 0.0849\n",
      "\n",
      "Epoch 00633: LearningRateScheduler reducing learning rate to 0.0002970996954968181.\n",
      "Epoch 633/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0798\n",
      "Epoch 00633: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 0s 133ms/step - loss: 0.0798 - val_loss: 0.0827\n",
      "\n",
      "Epoch 00634: LearningRateScheduler reducing learning rate to 0.0002970904964684762.\n",
      "Epoch 634/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0948\n",
      "Epoch 00634: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 0s 145ms/step - loss: 0.0948 - val_loss: 0.0804\n",
      "\n",
      "Epoch 00635: LearningRateScheduler reducing learning rate to 0.00029708128301905924.\n",
      "Epoch 635/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0959\n",
      "Epoch 00635: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 0s 133ms/step - loss: 0.0959 - val_loss: 0.0783\n",
      "\n",
      "Epoch 00636: LearningRateScheduler reducing learning rate to 0.0002970720551494798.\n",
      "Epoch 636/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0826\n",
      "Epoch 00636: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 0s 152ms/step - loss: 0.0826 - val_loss: 0.0749\n",
      "\n",
      "Epoch 00637: LearningRateScheduler reducing learning rate to 0.00029706281286065195.\n",
      "Epoch 637/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1024\n",
      "Epoch 00637: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 0s 134ms/step - loss: 0.1024 - val_loss: 0.0734\n",
      "\n",
      "Epoch 00638: LearningRateScheduler reducing learning rate to 0.0002970535561534912.\n",
      "Epoch 638/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1027\n",
      "Epoch 00638: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 0s 146ms/step - loss: 0.1027 - val_loss: 0.0726\n",
      "\n",
      "Epoch 00639: LearningRateScheduler reducing learning rate to 0.0002970442850289143.\n",
      "Epoch 639/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0907\n",
      "Epoch 00639: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 0s 137ms/step - loss: 0.0907 - val_loss: 0.0725\n",
      "\n",
      "Epoch 00640: LearningRateScheduler reducing learning rate to 0.0002970349994878397.\n",
      "Epoch 640/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0973\n",
      "Epoch 00640: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 0s 143ms/step - loss: 0.0973 - val_loss: 0.0738\n",
      "\n",
      "Epoch 00641: LearningRateScheduler reducing learning rate to 0.0002970256995311872.\n",
      "Epoch 641/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1048\n",
      "Epoch 00641: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 0s 146ms/step - loss: 0.1048 - val_loss: 0.0756\n",
      "\n",
      "Epoch 00642: LearningRateScheduler reducing learning rate to 0.00029701638515987783.\n",
      "Epoch 642/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1092\n",
      "Epoch 00642: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 0s 156ms/step - loss: 0.1092 - val_loss: 0.0763\n",
      "\n",
      "Epoch 00643: LearningRateScheduler reducing learning rate to 0.00029700705637483434.\n",
      "Epoch 643/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0911\n",
      "Epoch 00643: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 0s 133ms/step - loss: 0.0911 - val_loss: 0.0776\n",
      "\n",
      "Epoch 00644: LearningRateScheduler reducing learning rate to 0.0002969977131769807.\n",
      "Epoch 644/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0808\n",
      "Epoch 00644: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 0s 139ms/step - loss: 0.0808 - val_loss: 0.0785\n",
      "\n",
      "Epoch 00645: LearningRateScheduler reducing learning rate to 0.0002969883555672424.\n",
      "Epoch 645/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1057\n",
      "Epoch 00645: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 0s 136ms/step - loss: 0.1057 - val_loss: 0.0818\n",
      "\n",
      "Epoch 00646: LearningRateScheduler reducing learning rate to 0.0002969789835465463.\n",
      "Epoch 646/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0881\n",
      "Epoch 00646: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 0s 150ms/step - loss: 0.0881 - val_loss: 0.0864\n",
      "\n",
      "Epoch 00647: LearningRateScheduler reducing learning rate to 0.0002969695971158207.\n",
      "Epoch 647/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0962\n",
      "Epoch 00647: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 0s 151ms/step - loss: 0.0962 - val_loss: 0.0899\n",
      "\n",
      "Epoch 00648: LearningRateScheduler reducing learning rate to 0.0002969601962759954.\n",
      "Epoch 648/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1049\n",
      "Epoch 00648: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 0s 148ms/step - loss: 0.1049 - val_loss: 0.0915\n",
      "\n",
      "Epoch 00649: LearningRateScheduler reducing learning rate to 0.0002969507810280016.\n",
      "Epoch 649/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0911\n",
      "Epoch 00649: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 0s 133ms/step - loss: 0.0911 - val_loss: 0.0924\n",
      "\n",
      "Epoch 00650: LearningRateScheduler reducing learning rate to 0.0002969413513727718.\n",
      "Epoch 650/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0897\n",
      "Epoch 00650: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 0s 134ms/step - loss: 0.0897 - val_loss: 0.0923\n",
      "\n",
      "Epoch 00651: LearningRateScheduler reducing learning rate to 0.00029693190731124014.\n",
      "Epoch 651/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0716\n",
      "Epoch 00651: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 0s 134ms/step - loss: 0.0716 - val_loss: 0.0913\n",
      "\n",
      "Epoch 00652: LearningRateScheduler reducing learning rate to 0.00029692244884434204.\n",
      "Epoch 652/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1037- ETA: 0s - loss: 0.117\n",
      "Epoch 00652: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 0s 149ms/step - loss: 0.1037 - val_loss: 0.0894\n",
      "\n",
      "Epoch 00653: LearningRateScheduler reducing learning rate to 0.00029691297597301435.\n",
      "Epoch 653/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0874\n",
      "Epoch 00653: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 0s 146ms/step - loss: 0.0874 - val_loss: 0.0879\n",
      "\n",
      "Epoch 00654: LearningRateScheduler reducing learning rate to 0.0002969034886981954.\n",
      "Epoch 654/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0848\n",
      "Epoch 00654: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 0s 135ms/step - loss: 0.0848 - val_loss: 0.0858\n",
      "\n",
      "Epoch 00655: LearningRateScheduler reducing learning rate to 0.00029689398702082494.\n",
      "Epoch 655/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0842\n",
      "Epoch 00655: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 0s 137ms/step - loss: 0.0842 - val_loss: 0.0837\n",
      "\n",
      "Epoch 00656: LearningRateScheduler reducing learning rate to 0.0002968844709418441.\n",
      "Epoch 656/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0775\n",
      "Epoch 00656: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 0s 129ms/step - loss: 0.0775 - val_loss: 0.0821\n",
      "\n",
      "Epoch 00657: LearningRateScheduler reducing learning rate to 0.0002968749404621955.\n",
      "Epoch 657/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1034\n",
      "Epoch 00657: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 0s 162ms/step - loss: 0.1034 - val_loss: 0.0815\n",
      "\n",
      "Epoch 00658: LearningRateScheduler reducing learning rate to 0.0002968653955828231.\n",
      "Epoch 658/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0859\n",
      "Epoch 00658: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 0s 130ms/step - loss: 0.0859 - val_loss: 0.0819\n",
      "\n",
      "Epoch 00659: LearningRateScheduler reducing learning rate to 0.00029685583630467243.\n",
      "Epoch 659/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0941\n",
      "Epoch 00659: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 0s 132ms/step - loss: 0.0941 - val_loss: 0.0834\n",
      "\n",
      "Epoch 00660: LearningRateScheduler reducing learning rate to 0.00029684626262869036.\n",
      "Epoch 660/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0993\n",
      "Epoch 00660: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 0s 156ms/step - loss: 0.0993 - val_loss: 0.0850\n",
      "\n",
      "Epoch 00661: LearningRateScheduler reducing learning rate to 0.00029683667455582507.\n",
      "Epoch 661/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0943\n",
      "Epoch 00661: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 0s 152ms/step - loss: 0.0943 - val_loss: 0.0878\n",
      "\n",
      "Epoch 00662: LearningRateScheduler reducing learning rate to 0.00029682707208702645.\n",
      "Epoch 662/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0802\n",
      "Epoch 00662: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 0s 135ms/step - loss: 0.0802 - val_loss: 0.0928\n",
      "\n",
      "Epoch 00663: LearningRateScheduler reducing learning rate to 0.00029681745522324544.\n",
      "Epoch 663/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1078\n",
      "Epoch 00663: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 0s 155ms/step - loss: 0.1078 - val_loss: 0.0937\n",
      "\n",
      "Epoch 00664: LearningRateScheduler reducing learning rate to 0.0002968078239654348.\n",
      "Epoch 664/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0987\n",
      "Epoch 00664: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 0s 136ms/step - loss: 0.0987 - val_loss: 0.0927\n",
      "\n",
      "Epoch 00665: LearningRateScheduler reducing learning rate to 0.00029679817831454843.\n",
      "Epoch 665/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1012\n",
      "Epoch 00665: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 173ms/step - loss: 0.1012 - val_loss: 0.0892\n",
      "\n",
      "Epoch 00666: LearningRateScheduler reducing learning rate to 0.0002967885182715417.\n",
      "Epoch 666/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1076\n",
      "Epoch 00666: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 0s 155ms/step - loss: 0.1076 - val_loss: 0.0864\n",
      "\n",
      "Epoch 00667: LearningRateScheduler reducing learning rate to 0.0002967788438373716.\n",
      "Epoch 667/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0787\n",
      "Epoch 00667: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 0s 139ms/step - loss: 0.0787 - val_loss: 0.0855\n",
      "\n",
      "Epoch 00668: LearningRateScheduler reducing learning rate to 0.0002967691550129963.\n",
      "Epoch 668/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1052\n",
      "Epoch 00668: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 0s 137ms/step - loss: 0.1052 - val_loss: 0.0848\n",
      "\n",
      "Epoch 00669: LearningRateScheduler reducing learning rate to 0.0002967594517993756.\n",
      "Epoch 669/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1158\n",
      "Epoch 00669: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 0s 151ms/step - loss: 0.1158 - val_loss: 0.0841\n",
      "\n",
      "Epoch 00670: LearningRateScheduler reducing learning rate to 0.0002967497341974705.\n",
      "Epoch 670/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0919\n",
      "Epoch 00670: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 0s 135ms/step - loss: 0.0919 - val_loss: 0.0831\n",
      "\n",
      "Epoch 00671: LearningRateScheduler reducing learning rate to 0.0002967400022082437.\n",
      "Epoch 671/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0950\n",
      "Epoch 00671: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 0s 140ms/step - loss: 0.0950 - val_loss: 0.0819\n",
      "\n",
      "Epoch 00672: LearningRateScheduler reducing learning rate to 0.0002967302558326591.\n",
      "Epoch 672/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1026\n",
      "Epoch 00672: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 0s 134ms/step - loss: 0.1026 - val_loss: 0.0809\n",
      "\n",
      "Epoch 00673: LearningRateScheduler reducing learning rate to 0.000296720495071682.\n",
      "Epoch 673/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1012\n",
      "Epoch 00673: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 0s 140ms/step - loss: 0.1012 - val_loss: 0.0804\n",
      "\n",
      "Epoch 00674: LearningRateScheduler reducing learning rate to 0.00029671071992627934.\n",
      "Epoch 674/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0891\n",
      "Epoch 00674: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 0s 137ms/step - loss: 0.0891 - val_loss: 0.0806\n",
      "\n",
      "Epoch 00675: LearningRateScheduler reducing learning rate to 0.0002967009303974194.\n",
      "Epoch 675/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1043\n",
      "Epoch 00675: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 0s 133ms/step - loss: 0.1043 - val_loss: 0.0806\n",
      "\n",
      "Epoch 00676: LearningRateScheduler reducing learning rate to 0.00029669112648607175.\n",
      "Epoch 676/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0975\n",
      "Epoch 00676: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 0s 140ms/step - loss: 0.0975 - val_loss: 0.0804\n",
      "\n",
      "Epoch 00677: LearningRateScheduler reducing learning rate to 0.0002966813081932076.\n",
      "Epoch 677/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0860\n",
      "Epoch 00677: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 0s 135ms/step - loss: 0.0860 - val_loss: 0.0813\n",
      "Epoch 00677: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2a5f6a456a0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_dataloader, epochs=Max_epoch, initial_epoch=0, validation_data=val_dataloader, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_epoch = early_stopping.stopped_epoch + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 解凍"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(model_body.layers)): \n",
    "    model_body.layers[i].trainable = True\n",
    "\n",
    "model.compile(optimizer = opt, loss={'yolo_loss': lambda y_true, y_pred: y_pred})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping  = EarlyStopping(monitor='val_loss', min_delta = 0, patience = 300, verbose = 1)\n",
    "\n",
    "callbacks = [checkpoint_callback, lr_scheduler, early_stopping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00678: LearningRateScheduler reducing learning rate to 0.00029667147551979935.\n",
      "Epoch 678/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1336\n",
      "Epoch 00678: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 3s 922ms/step - loss: 0.1336 - val_loss: 0.1296\n",
      "\n",
      "Epoch 00679: LearningRateScheduler reducing learning rate to 0.00029666162846682105.\n",
      "Epoch 679/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1184\n",
      "Epoch 00679: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.1184 - val_loss: 0.1062\n",
      "\n",
      "Epoch 00680: LearningRateScheduler reducing learning rate to 0.0002966517670352481.\n",
      "Epoch 680/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1036\n",
      "Epoch 00680: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.1036 - val_loss: 0.0897\n",
      "\n",
      "Epoch 00681: LearningRateScheduler reducing learning rate to 0.00029664189122605714.\n",
      "Epoch 681/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1006\n",
      "Epoch 00681: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.1006 - val_loss: 0.0845\n",
      "\n",
      "Epoch 00682: LearningRateScheduler reducing learning rate to 0.0002966320010402265.\n",
      "Epoch 682/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1076\n",
      "Epoch 00682: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.1076 - val_loss: 0.0855\n",
      "\n",
      "Epoch 00683: LearningRateScheduler reducing learning rate to 0.00029662209647873594.\n",
      "Epoch 683/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0929\n",
      "Epoch 00683: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0929 - val_loss: 0.0870\n",
      "\n",
      "Epoch 00684: LearningRateScheduler reducing learning rate to 0.00029661217754256623.\n",
      "Epoch 684/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0989\n",
      "Epoch 00684: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0989 - val_loss: 0.0889\n",
      "\n",
      "Epoch 00685: LearningRateScheduler reducing learning rate to 0.0002966022442327002.\n",
      "Epoch 685/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0915\n",
      "Epoch 00685: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0915 - val_loss: 0.0895\n",
      "\n",
      "Epoch 00686: LearningRateScheduler reducing learning rate to 0.0002965922965501215.\n",
      "Epoch 686/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0982\n",
      "Epoch 00686: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0982 - val_loss: 0.0869\n",
      "\n",
      "Epoch 00687: LearningRateScheduler reducing learning rate to 0.00029658233449581564.\n",
      "Epoch 687/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0906\n",
      "Epoch 00687: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0906 - val_loss: 0.0857\n",
      "\n",
      "Epoch 00688: LearningRateScheduler reducing learning rate to 0.0002965723580707693.\n",
      "Epoch 688/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0990\n",
      "Epoch 00688: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0990 - val_loss: 0.0852\n",
      "\n",
      "Epoch 00689: LearningRateScheduler reducing learning rate to 0.0002965623672759707.\n",
      "Epoch 689/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0946\n",
      "Epoch 00689: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0946 - val_loss: 0.0829\n",
      "\n",
      "Epoch 00690: LearningRateScheduler reducing learning rate to 0.00029655236211240945.\n",
      "Epoch 690/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0899\n",
      "Epoch 00690: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 225ms/step - loss: 0.0899 - val_loss: 0.0901\n",
      "\n",
      "Epoch 00691: LearningRateScheduler reducing learning rate to 0.0002965423425810765.\n",
      "Epoch 691/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1014\n",
      "Epoch 00691: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.1014 - val_loss: 0.1186\n",
      "\n",
      "Epoch 00692: LearningRateScheduler reducing learning rate to 0.0002965323086829645.\n",
      "Epoch 692/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0976\n",
      "Epoch 00692: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0976 - val_loss: 0.1046\n",
      "\n",
      "Epoch 00693: LearningRateScheduler reducing learning rate to 0.00029652226041906713.\n",
      "Epoch 693/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0830\n",
      "Epoch 00693: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0830 - val_loss: 0.1005\n",
      "\n",
      "Epoch 00694: LearningRateScheduler reducing learning rate to 0.0002965121977903798.\n",
      "Epoch 694/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0976\n",
      "Epoch 00694: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0976 - val_loss: 0.0963\n",
      "\n",
      "Epoch 00695: LearningRateScheduler reducing learning rate to 0.00029650212079789927.\n",
      "Epoch 695/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1164\n",
      "Epoch 00695: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.1164 - val_loss: 0.0959\n",
      "\n",
      "Epoch 00696: LearningRateScheduler reducing learning rate to 0.00029649202944262356.\n",
      "Epoch 696/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0892\n",
      "Epoch 00696: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 242ms/step - loss: 0.0892 - val_loss: 0.0966\n",
      "\n",
      "Epoch 00697: LearningRateScheduler reducing learning rate to 0.00029648192372555234.\n",
      "Epoch 697/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0857\n",
      "Epoch 00697: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 226ms/step - loss: 0.0857 - val_loss: 0.0967\n",
      "\n",
      "Epoch 00698: LearningRateScheduler reducing learning rate to 0.0002964718036476866.\n",
      "Epoch 698/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0754\n",
      "Epoch 00698: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0754 - val_loss: 0.0942\n",
      "\n",
      "Epoch 00699: LearningRateScheduler reducing learning rate to 0.00029646166921002873.\n",
      "Epoch 699/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0967\n",
      "Epoch 00699: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0967 - val_loss: 0.0921\n",
      "\n",
      "Epoch 00700: LearningRateScheduler reducing learning rate to 0.0002964515204135826.\n",
      "Epoch 700/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0925\n",
      "Epoch 00700: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0925 - val_loss: 0.0912\n",
      "\n",
      "Epoch 00701: LearningRateScheduler reducing learning rate to 0.00029644135725935344.\n",
      "Epoch 701/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1000\n",
      "Epoch 00701: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.1000 - val_loss: 0.0915\n",
      "\n",
      "Epoch 00702: LearningRateScheduler reducing learning rate to 0.00029643117974834793.\n",
      "Epoch 702/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0958\n",
      "Epoch 00702: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 228ms/step - loss: 0.0958 - val_loss: 0.0929\n",
      "\n",
      "Epoch 00703: LearningRateScheduler reducing learning rate to 0.0002964209878815742.\n",
      "Epoch 703/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0834\n",
      "Epoch 00703: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0834 - val_loss: 0.0941\n",
      "\n",
      "Epoch 00704: LearningRateScheduler reducing learning rate to 0.00029641078166004175.\n",
      "Epoch 704/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1240\n",
      "Epoch 00704: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 228ms/step - loss: 0.1240 - val_loss: 0.0944\n",
      "\n",
      "Epoch 00705: LearningRateScheduler reducing learning rate to 0.00029640056108476155.\n",
      "Epoch 705/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1082\n",
      "Epoch 00705: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.1082 - val_loss: 0.0947\n",
      "\n",
      "Epoch 00706: LearningRateScheduler reducing learning rate to 0.000296390326156746.\n",
      "Epoch 706/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0958\n",
      "Epoch 00706: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0958 - val_loss: 0.0947\n",
      "\n",
      "Epoch 00707: LearningRateScheduler reducing learning rate to 0.00029638007687700883.\n",
      "Epoch 707/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0905\n",
      "Epoch 00707: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 228ms/step - loss: 0.0905 - val_loss: 0.0960\n",
      "\n",
      "Epoch 00708: LearningRateScheduler reducing learning rate to 0.0002963698132465653.\n",
      "Epoch 708/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0967\n",
      "Epoch 00708: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0967 - val_loss: 0.0971\n",
      "\n",
      "Epoch 00709: LearningRateScheduler reducing learning rate to 0.0002963595352664321.\n",
      "Epoch 709/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0950\n",
      "Epoch 00709: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0950 - val_loss: 0.0969\n",
      "\n",
      "Epoch 00710: LearningRateScheduler reducing learning rate to 0.0002963492429376271.\n",
      "Epoch 710/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0900\n",
      "Epoch 00710: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0900 - val_loss: 0.0956\n",
      "\n",
      "Epoch 00711: LearningRateScheduler reducing learning rate to 0.00029633893626116993.\n",
      "Epoch 711/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1039\n",
      "Epoch 00711: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.1039 - val_loss: 0.0945\n",
      "\n",
      "Epoch 00712: LearningRateScheduler reducing learning rate to 0.0002963286152380815.\n",
      "Epoch 712/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0923\n",
      "Epoch 00712: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0923 - val_loss: 0.0938\n",
      "\n",
      "Epoch 00713: LearningRateScheduler reducing learning rate to 0.0002963182798693841.\n",
      "Epoch 713/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1008\n",
      "Epoch 00713: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.1008 - val_loss: 0.0928\n",
      "\n",
      "Epoch 00714: LearningRateScheduler reducing learning rate to 0.00029630793015610144.\n",
      "Epoch 714/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1074\n",
      "Epoch 00714: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 228ms/step - loss: 0.1074 - val_loss: 0.0909\n",
      "\n",
      "Epoch 00715: LearningRateScheduler reducing learning rate to 0.0002962975660992587.\n",
      "Epoch 715/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0975\n",
      "Epoch 00715: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0975 - val_loss: 0.0894\n",
      "\n",
      "Epoch 00716: LearningRateScheduler reducing learning rate to 0.0002962871876998825.\n",
      "Epoch 716/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0914\n",
      "Epoch 00716: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 237ms/step - loss: 0.0914 - val_loss: 0.0886\n",
      "\n",
      "Epoch 00717: LearningRateScheduler reducing learning rate to 0.00029627679495900075.\n",
      "Epoch 717/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0983\n",
      "Epoch 00717: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0983 - val_loss: 0.0898\n",
      "\n",
      "Epoch 00718: LearningRateScheduler reducing learning rate to 0.00029626638787764304.\n",
      "Epoch 718/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1291\n",
      "Epoch 00718: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 226ms/step - loss: 0.1291 - val_loss: 0.0914\n",
      "\n",
      "Epoch 00719: LearningRateScheduler reducing learning rate to 0.00029625596645684006.\n",
      "Epoch 719/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0907\n",
      "Epoch 00719: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0907 - val_loss: 0.0917\n",
      "\n",
      "Epoch 00720: LearningRateScheduler reducing learning rate to 0.0002962455306976241.\n",
      "Epoch 720/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1014\n",
      "Epoch 00720: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.1014 - val_loss: 0.0912\n",
      "\n",
      "Epoch 00721: LearningRateScheduler reducing learning rate to 0.0002962350806010289.\n",
      "Epoch 721/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1090\n",
      "Epoch 00721: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.1090 - val_loss: 0.0898\n",
      "\n",
      "Epoch 00722: LearningRateScheduler reducing learning rate to 0.0002962246161680896.\n",
      "Epoch 722/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0978\n",
      "Epoch 00722: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0978 - val_loss: 0.0897\n",
      "\n",
      "Epoch 00723: LearningRateScheduler reducing learning rate to 0.0002962141373998426.\n",
      "Epoch 723/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0913\n",
      "Epoch 00723: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0913 - val_loss: 0.0898\n",
      "\n",
      "Epoch 00724: LearningRateScheduler reducing learning rate to 0.000296203644297326.\n",
      "Epoch 724/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0906\n",
      "Epoch 00724: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0906 - val_loss: 0.0897\n",
      "\n",
      "Epoch 00725: LearningRateScheduler reducing learning rate to 0.00029619313686157897.\n",
      "Epoch 725/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0777\n",
      "Epoch 00725: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0777 - val_loss: 0.0892\n",
      "\n",
      "Epoch 00726: LearningRateScheduler reducing learning rate to 0.0002961826150936425.\n",
      "Epoch 726/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0823\n",
      "Epoch 00726: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0823 - val_loss: 0.0884\n",
      "\n",
      "Epoch 00727: LearningRateScheduler reducing learning rate to 0.0002961720789945586.\n",
      "Epoch 727/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0992\n",
      "Epoch 00727: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0992 - val_loss: 0.0905\n",
      "\n",
      "Epoch 00728: LearningRateScheduler reducing learning rate to 0.0002961615285653711.\n",
      "Epoch 728/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0821\n",
      "Epoch 00728: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0821 - val_loss: 0.0938\n",
      "\n",
      "Epoch 00729: LearningRateScheduler reducing learning rate to 0.0002961509638071249.\n",
      "Epoch 729/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0878\n",
      "Epoch 00729: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0878 - val_loss: 0.0953\n",
      "\n",
      "Epoch 00730: LearningRateScheduler reducing learning rate to 0.0002961403847208665.\n",
      "Epoch 730/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0846\n",
      "Epoch 00730: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0846 - val_loss: 0.0966\n",
      "\n",
      "Epoch 00731: LearningRateScheduler reducing learning rate to 0.0002961297913076438.\n",
      "Epoch 731/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0914\n",
      "Epoch 00731: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 242ms/step - loss: 0.0914 - val_loss: 0.0966\n",
      "\n",
      "Epoch 00732: LearningRateScheduler reducing learning rate to 0.0002961191835685061.\n",
      "Epoch 732/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0823\n",
      "Epoch 00732: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 238ms/step - loss: 0.0823 - val_loss: 0.0960\n",
      "\n",
      "Epoch 00733: LearningRateScheduler reducing learning rate to 0.0002961085615045041.\n",
      "Epoch 733/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1067\n",
      "Epoch 00733: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.1067 - val_loss: 0.0938\n",
      "\n",
      "Epoch 00734: LearningRateScheduler reducing learning rate to 0.00029609792511668995.\n",
      "Epoch 734/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0827\n",
      "Epoch 00734: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0827 - val_loss: 0.0892\n",
      "\n",
      "Epoch 00735: LearningRateScheduler reducing learning rate to 0.00029608727440611724.\n",
      "Epoch 735/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1031\n",
      "Epoch 00735: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.1031 - val_loss: 0.0871\n",
      "\n",
      "Epoch 00736: LearningRateScheduler reducing learning rate to 0.00029607660937384093.\n",
      "Epoch 736/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1000\n",
      "Epoch 00736: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 238ms/step - loss: 0.1000 - val_loss: 0.0860\n",
      "\n",
      "Epoch 00737: LearningRateScheduler reducing learning rate to 0.00029606593002091735.\n",
      "Epoch 737/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0938\n",
      "Epoch 00737: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0938 - val_loss: 0.0860\n",
      "\n",
      "Epoch 00738: LearningRateScheduler reducing learning rate to 0.00029605523634840437.\n",
      "Epoch 738/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1043\n",
      "Epoch 00738: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.1043 - val_loss: 0.0866\n",
      "\n",
      "Epoch 00739: LearningRateScheduler reducing learning rate to 0.00029604452835736125.\n",
      "Epoch 739/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1010\n",
      "Epoch 00739: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.1010 - val_loss: 0.0873\n",
      "\n",
      "Epoch 00740: LearningRateScheduler reducing learning rate to 0.0002960338060488486.\n",
      "Epoch 740/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0798\n",
      "Epoch 00740: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 228ms/step - loss: 0.0798 - val_loss: 0.0871\n",
      "\n",
      "Epoch 00741: LearningRateScheduler reducing learning rate to 0.00029602306942392854.\n",
      "Epoch 741/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0982\n",
      "Epoch 00741: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 239ms/step - loss: 0.0982 - val_loss: 0.0856\n",
      "\n",
      "Epoch 00742: LearningRateScheduler reducing learning rate to 0.0002960123184836645.\n",
      "Epoch 742/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0892\n",
      "Epoch 00742: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0892 - val_loss: 0.0840\n",
      "\n",
      "Epoch 00743: LearningRateScheduler reducing learning rate to 0.0002960015532291214.\n",
      "Epoch 743/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1015\n",
      "Epoch 00743: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.1015 - val_loss: 0.0828\n",
      "\n",
      "Epoch 00744: LearningRateScheduler reducing learning rate to 0.00029599077366136554.\n",
      "Epoch 744/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0866\n",
      "Epoch 00744: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0866 - val_loss: 0.0824\n",
      "\n",
      "Epoch 00745: LearningRateScheduler reducing learning rate to 0.0002959799797814647.\n",
      "Epoch 745/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0973\n",
      "Epoch 00745: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0973 - val_loss: 0.0829\n",
      "\n",
      "Epoch 00746: LearningRateScheduler reducing learning rate to 0.0002959691715904881.\n",
      "Epoch 746/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0879\n",
      "Epoch 00746: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0879 - val_loss: 0.0833\n",
      "\n",
      "Epoch 00747: LearningRateScheduler reducing learning rate to 0.0002959583490895062.\n",
      "Epoch 747/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0864\n",
      "Epoch 00747: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0864 - val_loss: 0.0835\n",
      "\n",
      "Epoch 00748: LearningRateScheduler reducing learning rate to 0.00029594751227959105.\n",
      "Epoch 748/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1035\n",
      "Epoch 00748: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.1035 - val_loss: 0.0834\n",
      "\n",
      "Epoch 00749: LearningRateScheduler reducing learning rate to 0.00029593666116181606.\n",
      "Epoch 749/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0782\n",
      "Epoch 00749: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0782 - val_loss: 0.0849\n",
      "\n",
      "Epoch 00750: LearningRateScheduler reducing learning rate to 0.000295925795737256.\n",
      "Epoch 750/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0963\n",
      "Epoch 00750: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0963 - val_loss: 0.0851\n",
      "\n",
      "Epoch 00751: LearningRateScheduler reducing learning rate to 0.00029591491600698715.\n",
      "Epoch 751/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0918\n",
      "Epoch 00751: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0918 - val_loss: 0.0845\n",
      "\n",
      "Epoch 00752: LearningRateScheduler reducing learning rate to 0.0002959040219720872.\n",
      "Epoch 752/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0837\n",
      "Epoch 00752: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0837 - val_loss: 0.0831\n",
      "\n",
      "Epoch 00753: LearningRateScheduler reducing learning rate to 0.0002958931136336353.\n",
      "Epoch 753/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0918\n",
      "Epoch 00753: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0918 - val_loss: 0.0821\n",
      "\n",
      "Epoch 00754: LearningRateScheduler reducing learning rate to 0.0002958821909927118.\n",
      "Epoch 754/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0994\n",
      "Epoch 00754: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0994 - val_loss: 0.0824\n",
      "\n",
      "Epoch 00755: LearningRateScheduler reducing learning rate to 0.0002958712540503987.\n",
      "Epoch 755/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0984\n",
      "Epoch 00755: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0984 - val_loss: 0.0829\n",
      "\n",
      "Epoch 00756: LearningRateScheduler reducing learning rate to 0.00029586030280777923.\n",
      "Epoch 756/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0956\n",
      "Epoch 00756: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0956 - val_loss: 0.0816\n",
      "\n",
      "Epoch 00757: LearningRateScheduler reducing learning rate to 0.00029584933726593826.\n",
      "Epoch 757/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0924\n",
      "Epoch 00757: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 237ms/step - loss: 0.0924 - val_loss: 0.0803\n",
      "\n",
      "Epoch 00758: LearningRateScheduler reducing learning rate to 0.00029583835742596197.\n",
      "Epoch 758/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1013\n",
      "Epoch 00758: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.1013 - val_loss: 0.0791\n",
      "\n",
      "Epoch 00759: LearningRateScheduler reducing learning rate to 0.00029582736328893775.\n",
      "Epoch 759/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0940\n",
      "Epoch 00759: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0940 - val_loss: 0.0791\n",
      "\n",
      "Epoch 00760: LearningRateScheduler reducing learning rate to 0.00029581635485595475.\n",
      "Epoch 760/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0929\n",
      "Epoch 00760: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0929 - val_loss: 0.0792\n",
      "\n",
      "Epoch 00761: LearningRateScheduler reducing learning rate to 0.0002958053321281034.\n",
      "Epoch 761/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0850\n",
      "Epoch 00761: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0850 - val_loss: 0.0790\n",
      "\n",
      "Epoch 00762: LearningRateScheduler reducing learning rate to 0.00029579429510647544.\n",
      "Epoch 762/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0969\n",
      "Epoch 00762: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0969 - val_loss: 0.0795\n",
      "\n",
      "Epoch 00763: LearningRateScheduler reducing learning rate to 0.0002957832437921642.\n",
      "Epoch 763/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0923\n",
      "Epoch 00763: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0923 - val_loss: 0.0799\n",
      "\n",
      "Epoch 00764: LearningRateScheduler reducing learning rate to 0.00029577217818626416.\n",
      "Epoch 764/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0921\n",
      "Epoch 00764: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0921 - val_loss: 0.0795\n",
      "\n",
      "Epoch 00765: LearningRateScheduler reducing learning rate to 0.0002957610982898716.\n",
      "Epoch 765/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0867\n",
      "Epoch 00765: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 226ms/step - loss: 0.0867 - val_loss: 0.0788\n",
      "\n",
      "Epoch 00766: LearningRateScheduler reducing learning rate to 0.00029575000410408394.\n",
      "Epoch 766/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0872\n",
      "Epoch 00766: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0872 - val_loss: 0.0768\n",
      "\n",
      "Epoch 00767: LearningRateScheduler reducing learning rate to 0.00029573889563000005.\n",
      "Epoch 767/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1026\n",
      "Epoch 00767: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.1026 - val_loss: 0.0758\n",
      "\n",
      "Epoch 00768: LearningRateScheduler reducing learning rate to 0.00029572777286872027.\n",
      "Epoch 768/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0985\n",
      "Epoch 00768: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0985 - val_loss: 0.0745\n",
      "\n",
      "Epoch 00769: LearningRateScheduler reducing learning rate to 0.0002957166358213463.\n",
      "Epoch 769/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0907\n",
      "Epoch 00769: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0907 - val_loss: 0.0741\n",
      "\n",
      "Epoch 00770: LearningRateScheduler reducing learning rate to 0.0002957054844889814.\n",
      "Epoch 770/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0937\n",
      "Epoch 00770: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0937 - val_loss: 0.0739\n",
      "\n",
      "Epoch 00771: LearningRateScheduler reducing learning rate to 0.00029569431887273.\n",
      "Epoch 771/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0743\n",
      "Epoch 00771: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0743 - val_loss: 0.0734\n",
      "\n",
      "Epoch 00772: LearningRateScheduler reducing learning rate to 0.00029568313897369814.\n",
      "Epoch 772/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1039\n",
      "Epoch 00772: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 239ms/step - loss: 0.1039 - val_loss: 0.0739\n",
      "\n",
      "Epoch 00773: LearningRateScheduler reducing learning rate to 0.00029567194479299325.\n",
      "Epoch 773/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0880\n",
      "Epoch 00773: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0880 - val_loss: 0.0755\n",
      "\n",
      "Epoch 00774: LearningRateScheduler reducing learning rate to 0.00029566073633172416.\n",
      "Epoch 774/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0945\n",
      "Epoch 00774: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0945 - val_loss: 0.0788\n",
      "\n",
      "Epoch 00775: LearningRateScheduler reducing learning rate to 0.000295649513591001.\n",
      "Epoch 775/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0845\n",
      "Epoch 00775: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0845 - val_loss: 0.0822\n",
      "\n",
      "Epoch 00776: LearningRateScheduler reducing learning rate to 0.0002956382765719354.\n",
      "Epoch 776/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1021\n",
      "Epoch 00776: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.1021 - val_loss: 0.0831\n",
      "\n",
      "Epoch 00777: LearningRateScheduler reducing learning rate to 0.0002956270252756405.\n",
      "Epoch 777/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0891\n",
      "Epoch 00777: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0891 - val_loss: 0.0825\n",
      "\n",
      "Epoch 00778: LearningRateScheduler reducing learning rate to 0.0002956157597032307.\n",
      "Epoch 778/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0832\n",
      "Epoch 00778: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0832 - val_loss: 0.0822\n",
      "\n",
      "Epoch 00779: LearningRateScheduler reducing learning rate to 0.00029560447985582195.\n",
      "Epoch 779/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0878\n",
      "Epoch 00779: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0878 - val_loss: 0.0822\n",
      "\n",
      "Epoch 00780: LearningRateScheduler reducing learning rate to 0.00029559318573453146.\n",
      "Epoch 780/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0849\n",
      "Epoch 00780: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0849 - val_loss: 0.0819\n",
      "\n",
      "Epoch 00781: LearningRateScheduler reducing learning rate to 0.000295581877340478.\n",
      "Epoch 781/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0855\n",
      "Epoch 00781: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0855 - val_loss: 0.0810\n",
      "\n",
      "Epoch 00782: LearningRateScheduler reducing learning rate to 0.0002955705546747817.\n",
      "Epoch 782/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0841\n",
      "Epoch 00782: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0841 - val_loss: 0.0806\n",
      "\n",
      "Epoch 00783: LearningRateScheduler reducing learning rate to 0.00029555921773856403.\n",
      "Epoch 783/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0852\n",
      "Epoch 00783: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0852 - val_loss: 0.0805\n",
      "\n",
      "Epoch 00784: LearningRateScheduler reducing learning rate to 0.00029554786653294805.\n",
      "Epoch 784/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1037\n",
      "Epoch 00784: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.1037 - val_loss: 0.0802\n",
      "\n",
      "Epoch 00785: LearningRateScheduler reducing learning rate to 0.000295536501059058.\n",
      "Epoch 785/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0823\n",
      "Epoch 00785: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 237ms/step - loss: 0.0823 - val_loss: 0.0793\n",
      "\n",
      "Epoch 00786: LearningRateScheduler reducing learning rate to 0.0002955251213180197.\n",
      "Epoch 786/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0829\n",
      "Epoch 00786: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0829 - val_loss: 0.0784\n",
      "\n",
      "Epoch 00787: LearningRateScheduler reducing learning rate to 0.0002955137273109604.\n",
      "Epoch 787/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1032\n",
      "Epoch 00787: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.1032 - val_loss: 0.0768\n",
      "\n",
      "Epoch 00788: LearningRateScheduler reducing learning rate to 0.00029550231903900865.\n",
      "Epoch 788/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0782\n",
      "Epoch 00788: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 237ms/step - loss: 0.0782 - val_loss: 0.0753\n",
      "\n",
      "Epoch 00789: LearningRateScheduler reducing learning rate to 0.00029549089650329443.\n",
      "Epoch 789/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0900\n",
      "Epoch 00789: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0900 - val_loss: 0.0746\n",
      "\n",
      "Epoch 00790: LearningRateScheduler reducing learning rate to 0.00029547945970494926.\n",
      "Epoch 790/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0924\n",
      "Epoch 00790: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 237ms/step - loss: 0.0924 - val_loss: 0.0747\n",
      "\n",
      "Epoch 00791: LearningRateScheduler reducing learning rate to 0.00029546800864510586.\n",
      "Epoch 791/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0967\n",
      "Epoch 00791: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0967 - val_loss: 0.0760\n",
      "\n",
      "Epoch 00792: LearningRateScheduler reducing learning rate to 0.0002954565433248986.\n",
      "Epoch 792/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0900\n",
      "Epoch 00792: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0900 - val_loss: 0.0781\n",
      "\n",
      "Epoch 00793: LearningRateScheduler reducing learning rate to 0.0002954450637454631.\n",
      "Epoch 793/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0787\n",
      "Epoch 00793: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0787 - val_loss: 0.0832\n",
      "\n",
      "Epoch 00794: LearningRateScheduler reducing learning rate to 0.00029543356990793644.\n",
      "Epoch 794/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0809\n",
      "Epoch 00794: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0809 - val_loss: 0.0890\n",
      "\n",
      "Epoch 00795: LearningRateScheduler reducing learning rate to 0.00029542206181345707.\n",
      "Epoch 795/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1019\n",
      "Epoch 00795: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.1019 - val_loss: 0.0923\n",
      "\n",
      "Epoch 00796: LearningRateScheduler reducing learning rate to 0.00029541053946316497.\n",
      "Epoch 796/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0900\n",
      "Epoch 00796: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0900 - val_loss: 0.0933\n",
      "\n",
      "Epoch 00797: LearningRateScheduler reducing learning rate to 0.0002953990028582014.\n",
      "Epoch 797/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0947\n",
      "Epoch 00797: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0947 - val_loss: 0.0939\n",
      "\n",
      "Epoch 00798: LearningRateScheduler reducing learning rate to 0.00029538745199970913.\n",
      "Epoch 798/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0870\n",
      "Epoch 00798: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0870 - val_loss: 0.0938\n",
      "\n",
      "Epoch 00799: LearningRateScheduler reducing learning rate to 0.00029537588688883224.\n",
      "Epoch 799/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0895\n",
      "Epoch 00799: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0895 - val_loss: 0.0927\n",
      "\n",
      "Epoch 00800: LearningRateScheduler reducing learning rate to 0.0002953643075267164.\n",
      "Epoch 800/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1077\n",
      "Epoch 00800: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 228ms/step - loss: 0.1077 - val_loss: 0.0914\n",
      "\n",
      "Epoch 00801: LearningRateScheduler reducing learning rate to 0.0002953527139145084.\n",
      "Epoch 801/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0670\n",
      "Epoch 00801: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 237ms/step - loss: 0.0670 - val_loss: 0.0895\n",
      "\n",
      "Epoch 00802: LearningRateScheduler reducing learning rate to 0.0002953411060533567.\n",
      "Epoch 802/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0975\n",
      "Epoch 00802: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 240ms/step - loss: 0.0975 - val_loss: 0.0890\n",
      "\n",
      "Epoch 00803: LearningRateScheduler reducing learning rate to 0.0002953294839444112.\n",
      "Epoch 803/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0891\n",
      "Epoch 00803: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0891 - val_loss: 0.0883\n",
      "\n",
      "Epoch 00804: LearningRateScheduler reducing learning rate to 0.0002953178475888228.\n",
      "Epoch 804/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0731\n",
      "Epoch 00804: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 228ms/step - loss: 0.0731 - val_loss: 0.0865\n",
      "\n",
      "Epoch 00805: LearningRateScheduler reducing learning rate to 0.0002953061969877445.\n",
      "Epoch 805/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0841\n",
      "Epoch 00805: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0841 - val_loss: 0.0866\n",
      "\n",
      "Epoch 00806: LearningRateScheduler reducing learning rate to 0.00029529453214232997.\n",
      "Epoch 806/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0913\n",
      "Epoch 00806: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0913 - val_loss: 0.0882\n",
      "\n",
      "Epoch 00807: LearningRateScheduler reducing learning rate to 0.0002952828530537348.\n",
      "Epoch 807/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0760\n",
      "Epoch 00807: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 240ms/step - loss: 0.0760 - val_loss: 0.0889\n",
      "\n",
      "Epoch 00808: LearningRateScheduler reducing learning rate to 0.00029527115972311586.\n",
      "Epoch 808/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0884\n",
      "Epoch 00808: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0884 - val_loss: 0.0872\n",
      "\n",
      "Epoch 00809: LearningRateScheduler reducing learning rate to 0.0002952594521516313.\n",
      "Epoch 809/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0973\n",
      "Epoch 00809: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0973 - val_loss: 0.0853\n",
      "\n",
      "Epoch 00810: LearningRateScheduler reducing learning rate to 0.00029524773034044085.\n",
      "Epoch 810/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0779\n",
      "Epoch 00810: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 228ms/step - loss: 0.0779 - val_loss: 0.0824\n",
      "\n",
      "Epoch 00811: LearningRateScheduler reducing learning rate to 0.0002952359942907056.\n",
      "Epoch 811/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0714\n",
      "Epoch 00811: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0714 - val_loss: 0.0795\n",
      "\n",
      "Epoch 00812: LearningRateScheduler reducing learning rate to 0.00029522424400358797.\n",
      "Epoch 812/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0927\n",
      "Epoch 00812: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0927 - val_loss: 0.0760\n",
      "\n",
      "Epoch 00813: LearningRateScheduler reducing learning rate to 0.00029521247948025194.\n",
      "Epoch 813/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0994\n",
      "Epoch 00813: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0994 - val_loss: 0.0743\n",
      "\n",
      "Epoch 00814: LearningRateScheduler reducing learning rate to 0.0002952007007218627.\n",
      "Epoch 814/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0921\n",
      "Epoch 00814: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0921 - val_loss: 0.0751\n",
      "\n",
      "Epoch 00815: LearningRateScheduler reducing learning rate to 0.000295188907729587.\n",
      "Epoch 815/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0748\n",
      "Epoch 00815: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0748 - val_loss: 0.0775\n",
      "\n",
      "Epoch 00816: LearningRateScheduler reducing learning rate to 0.0002951771005045931.\n",
      "Epoch 816/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0680\n",
      "Epoch 00816: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0680 - val_loss: 0.0812\n",
      "\n",
      "Epoch 00817: LearningRateScheduler reducing learning rate to 0.0002951652790480503.\n",
      "Epoch 817/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0759\n",
      "Epoch 00817: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0759 - val_loss: 0.0833\n",
      "\n",
      "Epoch 00818: LearningRateScheduler reducing learning rate to 0.00029515344336112964.\n",
      "Epoch 818/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0904\n",
      "Epoch 00818: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0904 - val_loss: 0.0835\n",
      "\n",
      "Epoch 00819: LearningRateScheduler reducing learning rate to 0.0002951415934450036.\n",
      "Epoch 819/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0824\n",
      "Epoch 00819: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0824 - val_loss: 0.0826\n",
      "\n",
      "Epoch 00820: LearningRateScheduler reducing learning rate to 0.0002951297293008458.\n",
      "Epoch 820/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0789\n",
      "Epoch 00820: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0789 - val_loss: 0.0811\n",
      "\n",
      "Epoch 00821: LearningRateScheduler reducing learning rate to 0.0002951178509298314.\n",
      "Epoch 821/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0927\n",
      "Epoch 00821: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0927 - val_loss: 0.0785\n",
      "\n",
      "Epoch 00822: LearningRateScheduler reducing learning rate to 0.00029510595833313706.\n",
      "Epoch 822/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0934\n",
      "Epoch 00822: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 237ms/step - loss: 0.0934 - val_loss: 0.0749\n",
      "\n",
      "Epoch 00823: LearningRateScheduler reducing learning rate to 0.00029509405151194074.\n",
      "Epoch 823/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0817\n",
      "Epoch 00823: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0817 - val_loss: 0.0718\n",
      "\n",
      "Epoch 00824: LearningRateScheduler reducing learning rate to 0.0002950821304674218.\n",
      "Epoch 824/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0850\n",
      "Epoch 00824: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0850 - val_loss: 0.0712\n",
      "\n",
      "Epoch 00825: LearningRateScheduler reducing learning rate to 0.0002950701952007611.\n",
      "Epoch 825/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0778\n",
      "Epoch 00825: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0778 - val_loss: 0.0712\n",
      "\n",
      "Epoch 00826: LearningRateScheduler reducing learning rate to 0.0002950582457131408.\n",
      "Epoch 826/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0803\n",
      "Epoch 00826: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0803 - val_loss: 0.0711\n",
      "\n",
      "Epoch 00827: LearningRateScheduler reducing learning rate to 0.0002950462820057446.\n",
      "Epoch 827/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0807\n",
      "Epoch 00827: val_loss did not improve from 0.07031\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0807 - val_loss: 0.0708\n",
      "\n",
      "Epoch 00828: LearningRateScheduler reducing learning rate to 0.0002950343040797575.\n",
      "Epoch 828/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0762\n",
      "Epoch 00828: val_loss improved from 0.07031 to 0.06998, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 10s 3s/step - loss: 0.0762 - val_loss: 0.0700\n",
      "\n",
      "Epoch 00829: LearningRateScheduler reducing learning rate to 0.0002950223119363659.\n",
      "Epoch 829/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0844\n",
      "Epoch 00829: val_loss improved from 0.06998 to 0.06801, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 10s 3s/step - loss: 0.0844 - val_loss: 0.0680\n",
      "\n",
      "Epoch 00830: LearningRateScheduler reducing learning rate to 0.0002950103055767577.\n",
      "Epoch 830/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0839\n",
      "Epoch 00830: val_loss improved from 0.06801 to 0.06716, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.0839 - val_loss: 0.0672\n",
      "\n",
      "Epoch 00831: LearningRateScheduler reducing learning rate to 0.0002949982850021221.\n",
      "Epoch 831/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0960\n",
      "Epoch 00831: val_loss did not improve from 0.06716\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0960 - val_loss: 0.0687\n",
      "\n",
      "Epoch 00832: LearningRateScheduler reducing learning rate to 0.00029498625021364983.\n",
      "Epoch 832/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0817\n",
      "Epoch 00832: val_loss did not improve from 0.06716\n",
      "3/3 [==============================] - 1s 237ms/step - loss: 0.0817 - val_loss: 0.0708\n",
      "\n",
      "Epoch 00833: LearningRateScheduler reducing learning rate to 0.00029497420121253295.\n",
      "Epoch 833/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0745\n",
      "Epoch 00833: val_loss did not improve from 0.06716\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0745 - val_loss: 0.0734\n",
      "\n",
      "Epoch 00834: LearningRateScheduler reducing learning rate to 0.00029496213799996493.\n",
      "Epoch 834/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0819\n",
      "Epoch 00834: val_loss did not improve from 0.06716\n",
      "3/3 [==============================] - 1s 242ms/step - loss: 0.0819 - val_loss: 0.0758\n",
      "\n",
      "Epoch 00835: LearningRateScheduler reducing learning rate to 0.0002949500605771407.\n",
      "Epoch 835/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0671\n",
      "Epoch 00835: val_loss did not improve from 0.06716\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0671 - val_loss: 0.0775\n",
      "\n",
      "Epoch 00836: LearningRateScheduler reducing learning rate to 0.00029493796894525644.\n",
      "Epoch 836/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0921\n",
      "Epoch 00836: val_loss did not improve from 0.06716\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0921 - val_loss: 0.0777\n",
      "\n",
      "Epoch 00837: LearningRateScheduler reducing learning rate to 0.00029492586310551.\n",
      "Epoch 837/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0826\n",
      "Epoch 00837: val_loss did not improve from 0.06716\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0826 - val_loss: 0.0776\n",
      "\n",
      "Epoch 00838: LearningRateScheduler reducing learning rate to 0.00029491374305910036.\n",
      "Epoch 838/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0753\n",
      "Epoch 00838: val_loss did not improve from 0.06716\n",
      "3/3 [==============================] - 1s 225ms/step - loss: 0.0753 - val_loss: 0.0772\n",
      "\n",
      "Epoch 00839: LearningRateScheduler reducing learning rate to 0.0002949016088072281.\n",
      "Epoch 839/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0805\n",
      "Epoch 00839: val_loss did not improve from 0.06716\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0805 - val_loss: 0.0762\n",
      "\n",
      "Epoch 00840: LearningRateScheduler reducing learning rate to 0.0002948894603510952.\n",
      "Epoch 840/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0817\n",
      "Epoch 00840: val_loss did not improve from 0.06716\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0817 - val_loss: 0.0751\n",
      "\n",
      "Epoch 00841: LearningRateScheduler reducing learning rate to 0.0002948772976919049.\n",
      "Epoch 841/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0772\n",
      "Epoch 00841: val_loss did not improve from 0.06716\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0772 - val_loss: 0.0738\n",
      "\n",
      "Epoch 00842: LearningRateScheduler reducing learning rate to 0.00029486512083086203.\n",
      "Epoch 842/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0651\n",
      "Epoch 00842: val_loss did not improve from 0.06716\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0651 - val_loss: 0.0725\n",
      "\n",
      "Epoch 00843: LearningRateScheduler reducing learning rate to 0.00029485292976917264.\n",
      "Epoch 843/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0854\n",
      "Epoch 00843: val_loss did not improve from 0.06716\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0854 - val_loss: 0.0723\n",
      "\n",
      "Epoch 00844: LearningRateScheduler reducing learning rate to 0.0002948407245080443.\n",
      "Epoch 844/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0646\n",
      "Epoch 00844: val_loss did not improve from 0.06716\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0646 - val_loss: 0.0725\n",
      "\n",
      "Epoch 00845: LearningRateScheduler reducing learning rate to 0.000294828505048686.\n",
      "Epoch 845/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0963\n",
      "Epoch 00845: val_loss did not improve from 0.06716\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0963 - val_loss: 0.0735\n",
      "\n",
      "Epoch 00846: LearningRateScheduler reducing learning rate to 0.00029481627139230814.\n",
      "Epoch 846/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0648\n",
      "Epoch 00846: val_loss did not improve from 0.06716\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0648 - val_loss: 0.0751\n",
      "\n",
      "Epoch 00847: LearningRateScheduler reducing learning rate to 0.0002948040235401225.\n",
      "Epoch 847/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0887\n",
      "Epoch 00847: val_loss did not improve from 0.06716\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0887 - val_loss: 0.0776\n",
      "\n",
      "Epoch 00848: LearningRateScheduler reducing learning rate to 0.0002947917614933421.\n",
      "Epoch 848/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0917\n",
      "Epoch 00848: val_loss did not improve from 0.06716\n",
      "3/3 [==============================] - 1s 225ms/step - loss: 0.0917 - val_loss: 0.0782\n",
      "\n",
      "Epoch 00849: LearningRateScheduler reducing learning rate to 0.00029477948525318166.\n",
      "Epoch 849/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0806\n",
      "Epoch 00849: val_loss did not improve from 0.06716\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0806 - val_loss: 0.0789\n",
      "\n",
      "Epoch 00850: LearningRateScheduler reducing learning rate to 0.00029476719482085723.\n",
      "Epoch 850/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0832\n",
      "Epoch 00850: val_loss did not improve from 0.06716\n",
      "3/3 [==============================] - 1s 237ms/step - loss: 0.0832 - val_loss: 0.0800\n",
      "\n",
      "Epoch 00851: LearningRateScheduler reducing learning rate to 0.0002947548901975861.\n",
      "Epoch 851/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0650\n",
      "Epoch 00851: val_loss did not improve from 0.06716\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0650 - val_loss: 0.0807\n",
      "\n",
      "Epoch 00852: LearningRateScheduler reducing learning rate to 0.000294742571384587.\n",
      "Epoch 852/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0735\n",
      "Epoch 00852: val_loss did not improve from 0.06716\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0735 - val_loss: 0.0805\n",
      "\n",
      "Epoch 00853: LearningRateScheduler reducing learning rate to 0.0002947302383830803.\n",
      "Epoch 853/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0738\n",
      "Epoch 00853: val_loss did not improve from 0.06716\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0738 - val_loss: 0.0787\n",
      "\n",
      "Epoch 00854: LearningRateScheduler reducing learning rate to 0.0002947178911942876.\n",
      "Epoch 854/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0755\n",
      "Epoch 00854: val_loss did not improve from 0.06716\n",
      "3/3 [==============================] - 1s 228ms/step - loss: 0.0755 - val_loss: 0.0773\n",
      "\n",
      "Epoch 00855: LearningRateScheduler reducing learning rate to 0.00029470552981943174.\n",
      "Epoch 855/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0737\n",
      "Epoch 00855: val_loss did not improve from 0.06716\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0737 - val_loss: 0.0769\n",
      "\n",
      "Epoch 00856: LearningRateScheduler reducing learning rate to 0.0002946931542597373.\n",
      "Epoch 856/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0607\n",
      "Epoch 00856: val_loss did not improve from 0.06716\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0607 - val_loss: 0.0772\n",
      "\n",
      "Epoch 00857: LearningRateScheduler reducing learning rate to 0.0002946807645164301.\n",
      "Epoch 857/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0712\n",
      "Epoch 00857: val_loss did not improve from 0.06716\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0712 - val_loss: 0.0766\n",
      "\n",
      "Epoch 00858: LearningRateScheduler reducing learning rate to 0.0002946683605907374.\n",
      "Epoch 858/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0721\n",
      "Epoch 00858: val_loss did not improve from 0.06716\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0721 - val_loss: 0.0752\n",
      "\n",
      "Epoch 00859: LearningRateScheduler reducing learning rate to 0.00029465594248388765.\n",
      "Epoch 859/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0791\n",
      "Epoch 00859: val_loss did not improve from 0.06716\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0791 - val_loss: 0.0736\n",
      "\n",
      "Epoch 00860: LearningRateScheduler reducing learning rate to 0.0002946435101971111.\n",
      "Epoch 860/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0713\n",
      "Epoch 00860: val_loss did not improve from 0.06716\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0713 - val_loss: 0.0725\n",
      "\n",
      "Epoch 00861: LearningRateScheduler reducing learning rate to 0.0002946310637316391.\n",
      "Epoch 861/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0614\n",
      "Epoch 00861: val_loss did not improve from 0.06716\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0614 - val_loss: 0.0730\n",
      "\n",
      "Epoch 00862: LearningRateScheduler reducing learning rate to 0.00029461860308870454.\n",
      "Epoch 862/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0841\n",
      "Epoch 00862: val_loss did not improve from 0.06716\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0841 - val_loss: 0.0744\n",
      "\n",
      "Epoch 00863: LearningRateScheduler reducing learning rate to 0.0002946061282695416.\n",
      "Epoch 863/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0694\n",
      "Epoch 00863: val_loss did not improve from 0.06716\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0694 - val_loss: 0.0768\n",
      "\n",
      "Epoch 00864: LearningRateScheduler reducing learning rate to 0.0002945936392753861.\n",
      "Epoch 864/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0689\n",
      "Epoch 00864: val_loss did not improve from 0.06716\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0689 - val_loss: 0.0807\n",
      "\n",
      "Epoch 00865: LearningRateScheduler reducing learning rate to 0.000294581136107475.\n",
      "Epoch 865/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0666\n",
      "Epoch 00865: val_loss did not improve from 0.06716\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0666 - val_loss: 0.0839\n",
      "\n",
      "Epoch 00866: LearningRateScheduler reducing learning rate to 0.00029456861876704665.\n",
      "Epoch 866/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0746\n",
      "Epoch 00866: val_loss did not improve from 0.06716\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0746 - val_loss: 0.0842\n",
      "\n",
      "Epoch 00867: LearningRateScheduler reducing learning rate to 0.0002945560872553411.\n",
      "Epoch 867/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0800\n",
      "Epoch 00867: val_loss did not improve from 0.06716\n",
      "3/3 [==============================] - 1s 227ms/step - loss: 0.0800 - val_loss: 0.0842\n",
      "\n",
      "Epoch 00868: LearningRateScheduler reducing learning rate to 0.0002945435415735996.\n",
      "Epoch 868/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0675\n",
      "Epoch 00868: val_loss did not improve from 0.06716\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0675 - val_loss: 0.0839\n",
      "\n",
      "Epoch 00869: LearningRateScheduler reducing learning rate to 0.0002945309817230647.\n",
      "Epoch 869/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0752\n",
      "Epoch 00869: val_loss did not improve from 0.06716\n",
      "3/3 [==============================] - 1s 226ms/step - loss: 0.0752 - val_loss: 0.0818\n",
      "\n",
      "Epoch 00870: LearningRateScheduler reducing learning rate to 0.00029451840770498063.\n",
      "Epoch 870/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0783\n",
      "Epoch 00870: val_loss did not improve from 0.06716\n",
      "3/3 [==============================] - 1s 228ms/step - loss: 0.0783 - val_loss: 0.0796\n",
      "\n",
      "Epoch 00871: LearningRateScheduler reducing learning rate to 0.00029450581952059285.\n",
      "Epoch 871/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0753\n",
      "Epoch 00871: val_loss did not improve from 0.06716\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0753 - val_loss: 0.0780\n",
      "\n",
      "Epoch 00872: LearningRateScheduler reducing learning rate to 0.0002944932171711482.\n",
      "Epoch 872/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0570\n",
      "Epoch 00872: val_loss did not improve from 0.06716\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0570 - val_loss: 0.0762\n",
      "\n",
      "Epoch 00873: LearningRateScheduler reducing learning rate to 0.0002944806006578949.\n",
      "Epoch 873/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0899\n",
      "Epoch 00873: val_loss did not improve from 0.06716\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0899 - val_loss: 0.0751\n",
      "\n",
      "Epoch 00874: LearningRateScheduler reducing learning rate to 0.00029446796998208285.\n",
      "Epoch 874/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0629\n",
      "Epoch 00874: val_loss did not improve from 0.06716\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0629 - val_loss: 0.0741\n",
      "\n",
      "Epoch 00875: LearningRateScheduler reducing learning rate to 0.00029445532514496304.\n",
      "Epoch 875/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0781\n",
      "Epoch 00875: val_loss did not improve from 0.06716\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0781 - val_loss: 0.0734\n",
      "\n",
      "Epoch 00876: LearningRateScheduler reducing learning rate to 0.00029444266614778793.\n",
      "Epoch 876/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0761\n",
      "Epoch 00876: val_loss did not improve from 0.06716\n",
      "3/3 [==============================] - 1s 237ms/step - loss: 0.0761 - val_loss: 0.0730\n",
      "\n",
      "Epoch 00877: LearningRateScheduler reducing learning rate to 0.0002944299929918114.\n",
      "Epoch 877/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0702\n",
      "Epoch 00877: val_loss did not improve from 0.06716\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0702 - val_loss: 0.0727\n",
      "\n",
      "Epoch 00878: LearningRateScheduler reducing learning rate to 0.0002944173056782889.\n",
      "Epoch 878/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0570\n",
      "Epoch 00878: val_loss did not improve from 0.06716\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0570 - val_loss: 0.0733\n",
      "\n",
      "Epoch 00879: LearningRateScheduler reducing learning rate to 0.000294404604208477.\n",
      "Epoch 879/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0715\n",
      "Epoch 00879: val_loss did not improve from 0.06716\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0715 - val_loss: 0.0732\n",
      "\n",
      "Epoch 00880: LearningRateScheduler reducing learning rate to 0.0002943918885836339.\n",
      "Epoch 880/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0687\n",
      "Epoch 00880: val_loss did not improve from 0.06716\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0687 - val_loss: 0.0726\n",
      "\n",
      "Epoch 00881: LearningRateScheduler reducing learning rate to 0.00029437915880501904.\n",
      "Epoch 881/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0832\n",
      "Epoch 00881: val_loss did not improve from 0.06716\n",
      "3/3 [==============================] - 1s 239ms/step - loss: 0.0832 - val_loss: 0.0736\n",
      "\n",
      "Epoch 00882: LearningRateScheduler reducing learning rate to 0.00029436641487389333.\n",
      "Epoch 882/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0633\n",
      "Epoch 00882: val_loss did not improve from 0.06716\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0633 - val_loss: 0.0742\n",
      "\n",
      "Epoch 00883: LearningRateScheduler reducing learning rate to 0.0002943536567915192.\n",
      "Epoch 883/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0743\n",
      "Epoch 00883: val_loss did not improve from 0.06716\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0743 - val_loss: 0.0746\n",
      "\n",
      "Epoch 00884: LearningRateScheduler reducing learning rate to 0.0002943408845591602.\n",
      "Epoch 884/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0650\n",
      "Epoch 00884: val_loss did not improve from 0.06716\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0650 - val_loss: 0.0764\n",
      "\n",
      "Epoch 00885: LearningRateScheduler reducing learning rate to 0.00029432809817808166.\n",
      "Epoch 885/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0829\n",
      "Epoch 00885: val_loss did not improve from 0.06716\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0829 - val_loss: 0.0780\n",
      "\n",
      "Epoch 00886: LearningRateScheduler reducing learning rate to 0.0002943152976495499.\n",
      "Epoch 886/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0547\n",
      "Epoch 00886: val_loss did not improve from 0.06716\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0547 - val_loss: 0.0784\n",
      "\n",
      "Epoch 00887: LearningRateScheduler reducing learning rate to 0.0002943024829748329.\n",
      "Epoch 887/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0778\n",
      "Epoch 00887: val_loss did not improve from 0.06716\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0778 - val_loss: 0.0780\n",
      "\n",
      "Epoch 00888: LearningRateScheduler reducing learning rate to 0.00029428965415520005.\n",
      "Epoch 888/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0700\n",
      "Epoch 00888: val_loss did not improve from 0.06716\n",
      "3/3 [==============================] - 1s 227ms/step - loss: 0.0700 - val_loss: 0.0781\n",
      "\n",
      "Epoch 00889: LearningRateScheduler reducing learning rate to 0.000294276811191922.\n",
      "Epoch 889/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0935\n",
      "Epoch 00889: val_loss did not improve from 0.06716\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0935 - val_loss: 0.0793\n",
      "\n",
      "Epoch 00890: LearningRateScheduler reducing learning rate to 0.00029426395408627094.\n",
      "Epoch 890/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0632\n",
      "Epoch 00890: val_loss did not improve from 0.06716\n",
      "3/3 [==============================] - 1s 239ms/step - loss: 0.0632 - val_loss: 0.0796\n",
      "\n",
      "Epoch 00891: LearningRateScheduler reducing learning rate to 0.00029425108283952033.\n",
      "Epoch 891/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0715\n",
      "Epoch 00891: val_loss did not improve from 0.06716\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0715 - val_loss: 0.0805\n",
      "\n",
      "Epoch 00892: LearningRateScheduler reducing learning rate to 0.0002942381974529451.\n",
      "Epoch 892/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0674\n",
      "Epoch 00892: val_loss did not improve from 0.06716\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0674 - val_loss: 0.0801\n",
      "\n",
      "Epoch 00893: LearningRateScheduler reducing learning rate to 0.00029422529792782164.\n",
      "Epoch 893/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0717\n",
      "Epoch 00893: val_loss did not improve from 0.06716\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0717 - val_loss: 0.0782\n",
      "\n",
      "Epoch 00894: LearningRateScheduler reducing learning rate to 0.00029421238426542766.\n",
      "Epoch 894/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0684\n",
      "Epoch 00894: val_loss did not improve from 0.06716\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0684 - val_loss: 0.0745\n",
      "\n",
      "Epoch 00895: LearningRateScheduler reducing learning rate to 0.00029419945646704226.\n",
      "Epoch 895/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0706\n",
      "Epoch 00895: val_loss did not improve from 0.06716\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0706 - val_loss: 0.0743\n",
      "\n",
      "Epoch 00896: LearningRateScheduler reducing learning rate to 0.00029418651453394593.\n",
      "Epoch 896/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0629\n",
      "Epoch 00896: val_loss did not improve from 0.06716\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0629 - val_loss: 0.0773\n",
      "\n",
      "Epoch 00897: LearningRateScheduler reducing learning rate to 0.0002941735584674207.\n",
      "Epoch 897/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0749\n",
      "Epoch 00897: val_loss did not improve from 0.06716\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0749 - val_loss: 0.0801\n",
      "\n",
      "Epoch 00898: LearningRateScheduler reducing learning rate to 0.0002941605882687498.\n",
      "Epoch 898/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0493\n",
      "Epoch 00898: val_loss did not improve from 0.06716\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0493 - val_loss: 0.0812\n",
      "\n",
      "Epoch 00899: LearningRateScheduler reducing learning rate to 0.00029414760393921805.\n",
      "Epoch 899/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0635\n",
      "Epoch 00899: val_loss did not improve from 0.06716\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0635 - val_loss: 0.0821\n",
      "\n",
      "Epoch 00900: LearningRateScheduler reducing learning rate to 0.0002941346054801115.\n",
      "Epoch 900/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0636\n",
      "Epoch 00900: val_loss did not improve from 0.06716\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0636 - val_loss: 0.0829\n",
      "\n",
      "Epoch 00901: LearningRateScheduler reducing learning rate to 0.0002941215928927177.\n",
      "Epoch 901/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0950\n",
      "Epoch 00901: val_loss did not improve from 0.06716\n",
      "3/3 [==============================] - 1s 228ms/step - loss: 0.0950 - val_loss: 0.0824\n",
      "\n",
      "Epoch 00902: LearningRateScheduler reducing learning rate to 0.0002941085661783256.\n",
      "Epoch 902/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0592\n",
      "Epoch 00902: val_loss did not improve from 0.06716\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0592 - val_loss: 0.0815\n",
      "\n",
      "Epoch 00903: LearningRateScheduler reducing learning rate to 0.0002940955253382255.\n",
      "Epoch 903/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0631\n",
      "Epoch 00903: val_loss did not improve from 0.06716\n",
      "3/3 [==============================] - 1s 226ms/step - loss: 0.0631 - val_loss: 0.0785\n",
      "\n",
      "Epoch 00904: LearningRateScheduler reducing learning rate to 0.00029408247037370915.\n",
      "Epoch 904/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0664\n",
      "Epoch 00904: val_loss did not improve from 0.06716\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0664 - val_loss: 0.0753\n",
      "\n",
      "Epoch 00905: LearningRateScheduler reducing learning rate to 0.0002940694012860697.\n",
      "Epoch 905/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0846\n",
      "Epoch 00905: val_loss did not improve from 0.06716\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0846 - val_loss: 0.0727\n",
      "\n",
      "Epoch 00906: LearningRateScheduler reducing learning rate to 0.0002940563180766016.\n",
      "Epoch 906/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0550\n",
      "Epoch 00906: val_loss did not improve from 0.06716\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0550 - val_loss: 0.0701\n",
      "\n",
      "Epoch 00907: LearningRateScheduler reducing learning rate to 0.00029404322074660076.\n",
      "Epoch 907/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0534\n",
      "Epoch 00907: val_loss did not improve from 0.06716\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0534 - val_loss: 0.0679\n",
      "\n",
      "Epoch 00908: LearningRateScheduler reducing learning rate to 0.0002940301092973646.\n",
      "Epoch 908/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0740\n",
      "Epoch 00908: val_loss did not improve from 0.06716\n",
      "3/3 [==============================] - 1s 237ms/step - loss: 0.0740 - val_loss: 0.0675\n",
      "\n",
      "Epoch 00909: LearningRateScheduler reducing learning rate to 0.0002940169837301918.\n",
      "Epoch 909/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0553\n",
      "Epoch 00909: val_loss improved from 0.06716 to 0.06568, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 10s 3s/step - loss: 0.0553 - val_loss: 0.0657\n",
      "\n",
      "Epoch 00910: LearningRateScheduler reducing learning rate to 0.0002940038440463824.\n",
      "Epoch 910/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0792\n",
      "Epoch 00910: val_loss improved from 0.06568 to 0.06497, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.0792 - val_loss: 0.0650\n",
      "\n",
      "Epoch 00911: LearningRateScheduler reducing learning rate to 0.00029399069024723805.\n",
      "Epoch 911/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0594\n",
      "Epoch 00911: val_loss improved from 0.06497 to 0.06346, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.0594 - val_loss: 0.0635\n",
      "\n",
      "Epoch 00912: LearningRateScheduler reducing learning rate to 0.00029397752233406154.\n",
      "Epoch 912/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0623\n",
      "Epoch 00912: val_loss improved from 0.06346 to 0.06043, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.0623 - val_loss: 0.0604\n",
      "\n",
      "Epoch 00913: LearningRateScheduler reducing learning rate to 0.0002939643403081572.\n",
      "Epoch 913/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0597\n",
      "Epoch 00913: val_loss improved from 0.06043 to 0.05931, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 10s 3s/step - loss: 0.0597 - val_loss: 0.0593\n",
      "\n",
      "Epoch 00914: LearningRateScheduler reducing learning rate to 0.00029395114417083085.\n",
      "Epoch 914/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0577\n",
      "Epoch 00914: val_loss did not improve from 0.05931\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0577 - val_loss: 0.0609\n",
      "\n",
      "Epoch 00915: LearningRateScheduler reducing learning rate to 0.0002939379339233895.\n",
      "Epoch 915/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0511\n",
      "Epoch 00915: val_loss did not improve from 0.05931\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0511 - val_loss: 0.0614\n",
      "\n",
      "Epoch 00916: LearningRateScheduler reducing learning rate to 0.00029392470956714174.\n",
      "Epoch 916/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0735\n",
      "Epoch 00916: val_loss did not improve from 0.05931\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0735 - val_loss: 0.0595\n",
      "\n",
      "Epoch 00917: LearningRateScheduler reducing learning rate to 0.00029391147110339734.\n",
      "Epoch 917/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0588\n",
      "Epoch 00917: val_loss improved from 0.05931 to 0.05434, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.0588 - val_loss: 0.0543\n",
      "\n",
      "Epoch 00918: LearningRateScheduler reducing learning rate to 0.0002938982185334677.\n",
      "Epoch 918/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0802\n",
      "Epoch 00918: val_loss improved from 0.05434 to 0.04914, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.0802 - val_loss: 0.0491\n",
      "\n",
      "Epoch 00919: LearningRateScheduler reducing learning rate to 0.00029388495185866546.\n",
      "Epoch 919/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0524\n",
      "Epoch 00919: val_loss improved from 0.04914 to 0.04874, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 10s 3s/step - loss: 0.0524 - val_loss: 0.0487\n",
      "\n",
      "Epoch 00920: LearningRateScheduler reducing learning rate to 0.0002938716710803048.\n",
      "Epoch 920/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0501\n",
      "Epoch 00920: val_loss did not improve from 0.04874\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0501 - val_loss: 0.0528\n",
      "\n",
      "Epoch 00921: LearningRateScheduler reducing learning rate to 0.00029385837619970114.\n",
      "Epoch 921/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0642\n",
      "Epoch 00921: val_loss did not improve from 0.04874\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0642 - val_loss: 0.0592\n",
      "\n",
      "Epoch 00922: LearningRateScheduler reducing learning rate to 0.00029384506721817134.\n",
      "Epoch 922/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0695\n",
      "Epoch 00922: val_loss did not improve from 0.04874\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0695 - val_loss: 0.0673\n",
      "\n",
      "Epoch 00923: LearningRateScheduler reducing learning rate to 0.0002938317441370338.\n",
      "Epoch 923/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0681\n",
      "Epoch 00923: val_loss did not improve from 0.04874\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0681 - val_loss: 0.0672\n",
      "\n",
      "Epoch 00924: LearningRateScheduler reducing learning rate to 0.00029381840695760807.\n",
      "Epoch 924/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0623\n",
      "Epoch 00924: val_loss did not improve from 0.04874\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0623 - val_loss: 0.0634\n",
      "\n",
      "Epoch 00925: LearningRateScheduler reducing learning rate to 0.0002938050556812153.\n",
      "Epoch 925/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0620\n",
      "Epoch 00925: val_loss did not improve from 0.04874\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0620 - val_loss: 0.0600\n",
      "\n",
      "Epoch 00926: LearningRateScheduler reducing learning rate to 0.000293791690309178.\n",
      "Epoch 926/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0614\n",
      "Epoch 00926: val_loss did not improve from 0.04874\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0614 - val_loss: 0.0543\n",
      "\n",
      "Epoch 00927: LearningRateScheduler reducing learning rate to 0.00029377831084282.\n",
      "Epoch 927/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0597\n",
      "Epoch 00927: val_loss did not improve from 0.04874\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0597 - val_loss: 0.0508\n",
      "\n",
      "Epoch 00928: LearningRateScheduler reducing learning rate to 0.00029376491728346654.\n",
      "Epoch 928/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0780\n",
      "Epoch 00928: val_loss improved from 0.04874 to 0.04689, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.0780 - val_loss: 0.0469\n",
      "\n",
      "Epoch 00929: LearningRateScheduler reducing learning rate to 0.0002937515096324443.\n",
      "Epoch 929/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0564\n",
      "Epoch 00929: val_loss improved from 0.04689 to 0.04498, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 10s 3s/step - loss: 0.0564 - val_loss: 0.0450\n",
      "\n",
      "Epoch 00930: LearningRateScheduler reducing learning rate to 0.00029373808789108133.\n",
      "Epoch 930/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0620\n",
      "Epoch 00930: val_loss improved from 0.04498 to 0.04289, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.0620 - val_loss: 0.0429\n",
      "\n",
      "Epoch 00931: LearningRateScheduler reducing learning rate to 0.0002937246520607071.\n",
      "Epoch 931/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0583\n",
      "Epoch 00931: val_loss did not improve from 0.04289\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0583 - val_loss: 0.0454\n",
      "\n",
      "Epoch 00932: LearningRateScheduler reducing learning rate to 0.0002937112021426525.\n",
      "Epoch 932/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0532\n",
      "Epoch 00932: val_loss did not improve from 0.04289\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0532 - val_loss: 0.0464\n",
      "\n",
      "Epoch 00933: LearningRateScheduler reducing learning rate to 0.0002936977381382498.\n",
      "Epoch 933/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0687\n",
      "Epoch 00933: val_loss did not improve from 0.04289\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0687 - val_loss: 0.0465\n",
      "\n",
      "Epoch 00934: LearningRateScheduler reducing learning rate to 0.00029368426004883246.\n",
      "Epoch 934/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0629\n",
      "Epoch 00934: val_loss did not improve from 0.04289\n",
      "3/3 [==============================] - 1s 227ms/step - loss: 0.0629 - val_loss: 0.0448\n",
      "\n",
      "Epoch 00935: LearningRateScheduler reducing learning rate to 0.00029367076787573565.\n",
      "Epoch 935/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0634\n",
      "Epoch 00935: val_loss did not improve from 0.04289\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0634 - val_loss: 0.0448\n",
      "\n",
      "Epoch 00936: LearningRateScheduler reducing learning rate to 0.00029365726162029586.\n",
      "Epoch 936/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0520\n",
      "Epoch 00936: val_loss did not improve from 0.04289\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0520 - val_loss: 0.0480\n",
      "\n",
      "Epoch 00937: LearningRateScheduler reducing learning rate to 0.00029364374128385077.\n",
      "Epoch 937/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0602\n",
      "Epoch 00937: val_loss did not improve from 0.04289\n",
      "3/3 [==============================] - 1s 225ms/step - loss: 0.0602 - val_loss: 0.0511\n",
      "\n",
      "Epoch 00938: LearningRateScheduler reducing learning rate to 0.0002936302068677397.\n",
      "Epoch 938/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0473\n",
      "Epoch 00938: val_loss did not improve from 0.04289\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0473 - val_loss: 0.0518\n",
      "\n",
      "Epoch 00939: LearningRateScheduler reducing learning rate to 0.0002936166583733032.\n",
      "Epoch 939/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0742\n",
      "Epoch 00939: val_loss did not improve from 0.04289\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0742 - val_loss: 0.0491\n",
      "\n",
      "Epoch 00940: LearningRateScheduler reducing learning rate to 0.00029360309580188333.\n",
      "Epoch 940/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0617\n",
      "Epoch 00940: val_loss improved from 0.04289 to 0.04195, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 10s 3s/step - loss: 0.0617 - val_loss: 0.0419\n",
      "\n",
      "Epoch 00941: LearningRateScheduler reducing learning rate to 0.0002935895191548235.\n",
      "Epoch 941/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0621\n",
      "Epoch 00941: val_loss improved from 0.04195 to 0.03975, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 10s 3s/step - loss: 0.0621 - val_loss: 0.0398\n",
      "\n",
      "Epoch 00942: LearningRateScheduler reducing learning rate to 0.0002935759284334685.\n",
      "Epoch 942/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0647\n",
      "Epoch 00942: val_loss did not improve from 0.03975\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0647 - val_loss: 0.0420\n",
      "\n",
      "Epoch 00943: LearningRateScheduler reducing learning rate to 0.0002935623236391645.\n",
      "Epoch 943/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0626\n",
      "Epoch 00943: val_loss did not improve from 0.03975\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0626 - val_loss: 0.0450\n",
      "\n",
      "Epoch 00944: LearningRateScheduler reducing learning rate to 0.0002935487047732591.\n",
      "Epoch 944/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0693\n",
      "Epoch 00944: val_loss did not improve from 0.03975\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0693 - val_loss: 0.0483\n",
      "\n",
      "Epoch 00945: LearningRateScheduler reducing learning rate to 0.0002935350718371012.\n",
      "Epoch 945/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0817\n",
      "Epoch 00945: val_loss did not improve from 0.03975\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0817 - val_loss: 0.0511\n",
      "\n",
      "Epoch 00946: LearningRateScheduler reducing learning rate to 0.00029352142483204133.\n",
      "Epoch 946/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0535\n",
      "Epoch 00946: val_loss did not improve from 0.03975\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0535 - val_loss: 0.0560\n",
      "\n",
      "Epoch 00947: LearningRateScheduler reducing learning rate to 0.00029350776375943117.\n",
      "Epoch 947/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0864\n",
      "Epoch 00947: val_loss did not improve from 0.03975\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0864 - val_loss: 0.0568\n",
      "\n",
      "Epoch 00948: LearningRateScheduler reducing learning rate to 0.0002934940886206239.\n",
      "Epoch 948/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0710\n",
      "Epoch 00948: val_loss did not improve from 0.03975\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0710 - val_loss: 0.0550\n",
      "\n",
      "Epoch 00949: LearningRateScheduler reducing learning rate to 0.00029348039941697404.\n",
      "Epoch 949/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0556\n",
      "Epoch 00949: val_loss did not improve from 0.03975\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0556 - val_loss: 0.0548\n",
      "\n",
      "Epoch 00950: LearningRateScheduler reducing learning rate to 0.0002934666961498375.\n",
      "Epoch 950/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0517\n",
      "Epoch 00950: val_loss did not improve from 0.03975\n",
      "3/3 [==============================] - 1s 228ms/step - loss: 0.0517 - val_loss: 0.0533\n",
      "\n",
      "Epoch 00951: LearningRateScheduler reducing learning rate to 0.0002934529788205718.\n",
      "Epoch 951/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0769\n",
      "Epoch 00951: val_loss did not improve from 0.03975\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0769 - val_loss: 0.0501\n",
      "\n",
      "Epoch 00952: LearningRateScheduler reducing learning rate to 0.0002934392474305355.\n",
      "Epoch 952/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0596\n",
      "Epoch 00952: val_loss did not improve from 0.03975\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0596 - val_loss: 0.0482\n",
      "\n",
      "Epoch 00953: LearningRateScheduler reducing learning rate to 0.0002934255019810888.\n",
      "Epoch 953/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0610\n",
      "Epoch 00953: val_loss did not improve from 0.03975\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0610 - val_loss: 0.0492\n",
      "\n",
      "Epoch 00954: LearningRateScheduler reducing learning rate to 0.0002934117424735931.\n",
      "Epoch 954/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0549\n",
      "Epoch 00954: val_loss did not improve from 0.03975\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0549 - val_loss: 0.0501\n",
      "\n",
      "Epoch 00955: LearningRateScheduler reducing learning rate to 0.0002933979689094115.\n",
      "Epoch 955/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0724\n",
      "Epoch 00955: val_loss did not improve from 0.03975\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0724 - val_loss: 0.0504\n",
      "\n",
      "Epoch 00956: LearningRateScheduler reducing learning rate to 0.0002933841812899082.\n",
      "Epoch 956/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0784\n",
      "Epoch 00956: val_loss did not improve from 0.03975\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0784 - val_loss: 0.0492\n",
      "\n",
      "Epoch 00957: LearningRateScheduler reducing learning rate to 0.0002933703796164489.\n",
      "Epoch 957/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0782\n",
      "Epoch 00957: val_loss did not improve from 0.03975\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0782 - val_loss: 0.0483\n",
      "\n",
      "Epoch 00958: LearningRateScheduler reducing learning rate to 0.00029335656389040067.\n",
      "Epoch 958/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0573\n",
      "Epoch 00958: val_loss did not improve from 0.03975\n",
      "3/3 [==============================] - 1s 227ms/step - loss: 0.0573 - val_loss: 0.0465\n",
      "\n",
      "Epoch 00959: LearningRateScheduler reducing learning rate to 0.0002933427341131321.\n",
      "Epoch 959/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0616\n",
      "Epoch 00959: val_loss did not improve from 0.03975\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0616 - val_loss: 0.0457\n",
      "\n",
      "Epoch 00960: LearningRateScheduler reducing learning rate to 0.0002933288902860129.\n",
      "Epoch 960/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0836\n",
      "Epoch 00960: val_loss did not improve from 0.03975\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0836 - val_loss: 0.0473\n",
      "\n",
      "Epoch 00961: LearningRateScheduler reducing learning rate to 0.00029331503241041446.\n",
      "Epoch 961/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0652\n",
      "Epoch 00961: val_loss did not improve from 0.03975\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0652 - val_loss: 0.0478\n",
      "\n",
      "Epoch 00962: LearningRateScheduler reducing learning rate to 0.0002933011604877093.\n",
      "Epoch 962/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0653\n",
      "Epoch 00962: val_loss did not improve from 0.03975\n",
      "3/3 [==============================] - 1s 239ms/step - loss: 0.0653 - val_loss: 0.0463\n",
      "\n",
      "Epoch 00963: LearningRateScheduler reducing learning rate to 0.0002932872745192716.\n",
      "Epoch 963/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0443\n",
      "Epoch 00963: val_loss did not improve from 0.03975\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0443 - val_loss: 0.0483\n",
      "\n",
      "Epoch 00964: LearningRateScheduler reducing learning rate to 0.0002932733745064768.\n",
      "Epoch 964/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0901\n",
      "Epoch 00964: val_loss did not improve from 0.03975\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0901 - val_loss: 0.0506\n",
      "\n",
      "Epoch 00965: LearningRateScheduler reducing learning rate to 0.0002932594604507016.\n",
      "Epoch 965/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0693\n",
      "Epoch 00965: val_loss did not improve from 0.03975\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0693 - val_loss: 0.0495\n",
      "\n",
      "Epoch 00966: LearningRateScheduler reducing learning rate to 0.0002932455323533243.\n",
      "Epoch 966/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0626\n",
      "Epoch 00966: val_loss did not improve from 0.03975\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0626 - val_loss: 0.0491\n",
      "\n",
      "Epoch 00967: LearningRateScheduler reducing learning rate to 0.0002932315902157245.\n",
      "Epoch 967/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0590\n",
      "Epoch 00967: val_loss did not improve from 0.03975\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0590 - val_loss: 0.0441\n",
      "\n",
      "Epoch 00968: LearningRateScheduler reducing learning rate to 0.0002932176340392833.\n",
      "Epoch 968/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0673\n",
      "Epoch 00968: val_loss did not improve from 0.03975\n",
      "3/3 [==============================] - 1s 227ms/step - loss: 0.0673 - val_loss: 0.0448\n",
      "\n",
      "Epoch 00969: LearningRateScheduler reducing learning rate to 0.0002932036638253829.\n",
      "Epoch 969/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0636\n",
      "Epoch 00969: val_loss did not improve from 0.03975\n",
      "3/3 [==============================] - 1s 228ms/step - loss: 0.0636 - val_loss: 0.0463\n",
      "\n",
      "Epoch 00970: LearningRateScheduler reducing learning rate to 0.00029318967957540725.\n",
      "Epoch 970/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0584\n",
      "Epoch 00970: val_loss did not improve from 0.03975\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0584 - val_loss: 0.0463\n",
      "\n",
      "Epoch 00971: LearningRateScheduler reducing learning rate to 0.0002931756812907414.\n",
      "Epoch 971/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0622\n",
      "Epoch 00971: val_loss did not improve from 0.03975\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0622 - val_loss: 0.0437\n",
      "\n",
      "Epoch 00972: LearningRateScheduler reducing learning rate to 0.000293161668972772.\n",
      "Epoch 972/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0665\n",
      "Epoch 00972: val_loss did not improve from 0.03975\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0665 - val_loss: 0.0418\n",
      "\n",
      "Epoch 00973: LearningRateScheduler reducing learning rate to 0.00029314764262288693.\n",
      "Epoch 973/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0424\n",
      "Epoch 00973: val_loss did not improve from 0.03975\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0424 - val_loss: 0.0403\n",
      "\n",
      "Epoch 00974: LearningRateScheduler reducing learning rate to 0.00029313360224247556.\n",
      "Epoch 974/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0530\n",
      "Epoch 00974: val_loss did not improve from 0.03975\n",
      "3/3 [==============================] - 1s 240ms/step - loss: 0.0530 - val_loss: 0.0411\n",
      "\n",
      "Epoch 00975: LearningRateScheduler reducing learning rate to 0.00029311954783292865.\n",
      "Epoch 975/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0592\n",
      "Epoch 00975: val_loss did not improve from 0.03975\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0592 - val_loss: 0.0439\n",
      "\n",
      "Epoch 00976: LearningRateScheduler reducing learning rate to 0.0002931054793956383.\n",
      "Epoch 976/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0588\n",
      "Epoch 00976: val_loss improved from 0.03975 to 0.03797, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.0588 - val_loss: 0.0380\n",
      "\n",
      "Epoch 00977: LearningRateScheduler reducing learning rate to 0.00029309139693199806.\n",
      "Epoch 977/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0431\n",
      "Epoch 00977: val_loss did not improve from 0.03797\n",
      "3/3 [==============================] - 1s 241ms/step - loss: 0.0431 - val_loss: 0.0410\n",
      "\n",
      "Epoch 00978: LearningRateScheduler reducing learning rate to 0.0002930773004434027.\n",
      "Epoch 978/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0580\n",
      "Epoch 00978: val_loss did not improve from 0.03797\n",
      "3/3 [==============================] - 1s 239ms/step - loss: 0.0580 - val_loss: 0.0475\n",
      "\n",
      "Epoch 00979: LearningRateScheduler reducing learning rate to 0.0002930631899312487.\n",
      "Epoch 979/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0488\n",
      "Epoch 00979: val_loss did not improve from 0.03797\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0488 - val_loss: 0.0468\n",
      "\n",
      "Epoch 00980: LearningRateScheduler reducing learning rate to 0.00029304906539693367.\n",
      "Epoch 980/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0649\n",
      "Epoch 00980: val_loss did not improve from 0.03797\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0649 - val_loss: 0.0431\n",
      "\n",
      "Epoch 00981: LearningRateScheduler reducing learning rate to 0.0002930349268418566.\n",
      "Epoch 981/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0571\n",
      "Epoch 00981: val_loss did not improve from 0.03797\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0571 - val_loss: 0.0448\n",
      "\n",
      "Epoch 00982: LearningRateScheduler reducing learning rate to 0.000293020774267418.\n",
      "Epoch 982/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0495\n",
      "Epoch 00982: val_loss did not improve from 0.03797\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0495 - val_loss: 0.0458\n",
      "\n",
      "Epoch 00983: LearningRateScheduler reducing learning rate to 0.0002930066076750197.\n",
      "Epoch 983/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0504\n",
      "Epoch 00983: val_loss did not improve from 0.03797\n",
      "3/3 [==============================] - 1s 240ms/step - loss: 0.0504 - val_loss: 0.0438\n",
      "\n",
      "Epoch 00984: LearningRateScheduler reducing learning rate to 0.0002929924270660649.\n",
      "Epoch 984/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0510\n",
      "Epoch 00984: val_loss did not improve from 0.03797\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0510 - val_loss: 0.0459\n",
      "\n",
      "Epoch 00985: LearningRateScheduler reducing learning rate to 0.00029297823244195837.\n",
      "Epoch 985/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0607\n",
      "Epoch 00985: val_loss did not improve from 0.03797\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0607 - val_loss: 0.0418\n",
      "\n",
      "Epoch 00986: LearningRateScheduler reducing learning rate to 0.000292964023804106.\n",
      "Epoch 986/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0442\n",
      "Epoch 00986: val_loss did not improve from 0.03797\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0442 - val_loss: 0.0391\n",
      "\n",
      "Epoch 00987: LearningRateScheduler reducing learning rate to 0.0002929498011539151.\n",
      "Epoch 987/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0538\n",
      "Epoch 00987: val_loss did not improve from 0.03797\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0538 - val_loss: 0.0426\n",
      "\n",
      "Epoch 00988: LearningRateScheduler reducing learning rate to 0.00029293556449279466.\n",
      "Epoch 988/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0576\n",
      "Epoch 00988: val_loss did not improve from 0.03797\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0576 - val_loss: 0.0398\n",
      "\n",
      "Epoch 00989: LearningRateScheduler reducing learning rate to 0.00029292131382215476.\n",
      "Epoch 989/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0557\n",
      "Epoch 00989: val_loss improved from 0.03797 to 0.03750, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.0557 - val_loss: 0.0375\n",
      "\n",
      "Epoch 00990: LearningRateScheduler reducing learning rate to 0.0002929070491434069.\n",
      "Epoch 990/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0658\n",
      "Epoch 00990: val_loss improved from 0.03750 to 0.03689, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 10s 3s/step - loss: 0.0658 - val_loss: 0.0369\n",
      "\n",
      "Epoch 00991: LearningRateScheduler reducing learning rate to 0.0002928927704579642.\n",
      "Epoch 991/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0515\n",
      "Epoch 00991: val_loss did not improve from 0.03689\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0515 - val_loss: 0.0404\n",
      "\n",
      "Epoch 00992: LearningRateScheduler reducing learning rate to 0.0002928784777672408.\n",
      "Epoch 992/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0466\n",
      "Epoch 00992: val_loss did not improve from 0.03689\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0466 - val_loss: 0.0402\n",
      "\n",
      "Epoch 00993: LearningRateScheduler reducing learning rate to 0.0002928641710726525.\n",
      "Epoch 993/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0502\n",
      "Epoch 00993: val_loss did not improve from 0.03689\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0502 - val_loss: 0.0369\n",
      "\n",
      "Epoch 00994: LearningRateScheduler reducing learning rate to 0.00029284985037561647.\n",
      "Epoch 994/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0601\n",
      "Epoch 00994: val_loss did not improve from 0.03689\n",
      "3/3 [==============================] - 1s 238ms/step - loss: 0.0601 - val_loss: 0.0372\n",
      "\n",
      "Epoch 00995: LearningRateScheduler reducing learning rate to 0.00029283551567755114.\n",
      "Epoch 995/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0549\n",
      "Epoch 00995: val_loss did not improve from 0.03689\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0549 - val_loss: 0.0388\n",
      "\n",
      "Epoch 00996: LearningRateScheduler reducing learning rate to 0.0002928211669798764.\n",
      "Epoch 996/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0497\n",
      "Epoch 00996: val_loss did not improve from 0.03689\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0497 - val_loss: 0.0386\n",
      "\n",
      "Epoch 00997: LearningRateScheduler reducing learning rate to 0.0002928068042840136.\n",
      "Epoch 997/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0558\n",
      "Epoch 00997: val_loss improved from 0.03689 to 0.03559, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.0558 - val_loss: 0.0356\n",
      "\n",
      "Epoch 00998: LearningRateScheduler reducing learning rate to 0.0002927924275913852.\n",
      "Epoch 998/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0508\n",
      "Epoch 00998: val_loss improved from 0.03559 to 0.03252, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 10s 3s/step - loss: 0.0508 - val_loss: 0.0325\n",
      "\n",
      "Epoch 00999: LearningRateScheduler reducing learning rate to 0.0002927780369034155.\n",
      "Epoch 999/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0475\n",
      "Epoch 00999: val_loss did not improve from 0.03252\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0475 - val_loss: 0.0374\n",
      "\n",
      "Epoch 01000: LearningRateScheduler reducing learning rate to 0.00029276363222152977.\n",
      "Epoch 1000/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0535\n",
      "Epoch 01000: val_loss did not improve from 0.03252\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0535 - val_loss: 0.0407\n",
      "\n",
      "Epoch 01001: LearningRateScheduler reducing learning rate to 0.00029274921354715487.\n",
      "Epoch 1001/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0636\n",
      "Epoch 01001: val_loss did not improve from 0.03252\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0636 - val_loss: 0.0380\n",
      "\n",
      "Epoch 01002: LearningRateScheduler reducing learning rate to 0.000292734780881719.\n",
      "Epoch 1002/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0703\n",
      "Epoch 01002: val_loss did not improve from 0.03252\n",
      "3/3 [==============================] - 1s 239ms/step - loss: 0.0703 - val_loss: 0.0379\n",
      "\n",
      "Epoch 01003: LearningRateScheduler reducing learning rate to 0.00029272033422665173.\n",
      "Epoch 1003/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0421\n",
      "Epoch 01003: val_loss did not improve from 0.03252\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0421 - val_loss: 0.0409\n",
      "\n",
      "Epoch 01004: LearningRateScheduler reducing learning rate to 0.00029270587358338405.\n",
      "Epoch 1004/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0505\n",
      "Epoch 01004: val_loss did not improve from 0.03252\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0505 - val_loss: 0.0443\n",
      "\n",
      "Epoch 01005: LearningRateScheduler reducing learning rate to 0.00029269139895334836.\n",
      "Epoch 1005/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0539\n",
      "Epoch 01005: val_loss did not improve from 0.03252\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0539 - val_loss: 0.0460\n",
      "\n",
      "Epoch 01006: LearningRateScheduler reducing learning rate to 0.00029267691033797835.\n",
      "Epoch 1006/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0628\n",
      "Epoch 01006: val_loss did not improve from 0.03252\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0628 - val_loss: 0.0413\n",
      "\n",
      "Epoch 01007: LearningRateScheduler reducing learning rate to 0.0002926624077387092.\n",
      "Epoch 1007/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0557\n",
      "Epoch 01007: val_loss did not improve from 0.03252\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0557 - val_loss: 0.0384\n",
      "\n",
      "Epoch 01008: LearningRateScheduler reducing learning rate to 0.00029264789115697736.\n",
      "Epoch 1008/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0516\n",
      "Epoch 01008: val_loss did not improve from 0.03252\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0516 - val_loss: 0.0438\n",
      "\n",
      "Epoch 01009: LearningRateScheduler reducing learning rate to 0.0002926333605942208.\n",
      "Epoch 1009/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0570\n",
      "Epoch 01009: val_loss did not improve from 0.03252\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0570 - val_loss: 0.0446\n",
      "\n",
      "Epoch 01010: LearningRateScheduler reducing learning rate to 0.0002926188160518787.\n",
      "Epoch 1010/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0376\n",
      "Epoch 01010: val_loss did not improve from 0.03252\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0376 - val_loss: 0.0476\n",
      "\n",
      "Epoch 01011: LearningRateScheduler reducing learning rate to 0.00029260425753139185.\n",
      "Epoch 1011/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0453\n",
      "Epoch 01011: val_loss did not improve from 0.03252\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0453 - val_loss: 0.0491\n",
      "\n",
      "Epoch 01012: LearningRateScheduler reducing learning rate to 0.00029258968503420227.\n",
      "Epoch 1012/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0653\n",
      "Epoch 01012: val_loss did not improve from 0.03252\n",
      "3/3 [==============================] - 1s 242ms/step - loss: 0.0653 - val_loss: 0.0448\n",
      "\n",
      "Epoch 01013: LearningRateScheduler reducing learning rate to 0.0002925750985617534.\n",
      "Epoch 1013/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0464\n",
      "Epoch 01013: val_loss did not improve from 0.03252\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0464 - val_loss: 0.0431\n",
      "\n",
      "Epoch 01014: LearningRateScheduler reducing learning rate to 0.00029256049811549.\n",
      "Epoch 1014/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0602\n",
      "Epoch 01014: val_loss did not improve from 0.03252\n",
      "3/3 [==============================] - 1s 238ms/step - loss: 0.0602 - val_loss: 0.0629\n",
      "\n",
      "Epoch 01015: LearningRateScheduler reducing learning rate to 0.0002925458836968584.\n",
      "Epoch 1015/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0639\n",
      "Epoch 01015: val_loss did not improve from 0.03252\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0639 - val_loss: 0.0908\n",
      "\n",
      "Epoch 01016: LearningRateScheduler reducing learning rate to 0.00029253125530730607.\n",
      "Epoch 1016/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0707\n",
      "Epoch 01016: val_loss did not improve from 0.03252\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0707 - val_loss: 0.1315\n",
      "\n",
      "Epoch 01017: LearningRateScheduler reducing learning rate to 0.000292516612948282.\n",
      "Epoch 1017/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0576\n",
      "Epoch 01017: val_loss did not improve from 0.03252\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0576 - val_loss: 0.1715\n",
      "\n",
      "Epoch 01018: LearningRateScheduler reducing learning rate to 0.0002925019566212367.\n",
      "Epoch 1018/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0634\n",
      "Epoch 01018: val_loss did not improve from 0.03252\n",
      "3/3 [==============================] - 1s 228ms/step - loss: 0.0634 - val_loss: 0.2253\n",
      "\n",
      "Epoch 01019: LearningRateScheduler reducing learning rate to 0.00029248728632762173.\n",
      "Epoch 1019/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0514\n",
      "Epoch 01019: val_loss did not improve from 0.03252\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0514 - val_loss: 0.5557\n",
      "\n",
      "Epoch 01020: LearningRateScheduler reducing learning rate to 0.00029247260206889024.\n",
      "Epoch 1020/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0490\n",
      "Epoch 01020: val_loss did not improve from 0.03252\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0490 - val_loss: 2.1021\n",
      "\n",
      "Epoch 01021: LearningRateScheduler reducing learning rate to 0.00029245790384649687.\n",
      "Epoch 1021/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0449\n",
      "Epoch 01021: val_loss did not improve from 0.03252\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0449 - val_loss: 2.8908\n",
      "\n",
      "Epoch 01022: LearningRateScheduler reducing learning rate to 0.0002924431916618974.\n",
      "Epoch 1022/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0480\n",
      "Epoch 01022: val_loss did not improve from 0.03252\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0480 - val_loss: 1.3320\n",
      "\n",
      "Epoch 01023: LearningRateScheduler reducing learning rate to 0.0002924284655165491.\n",
      "Epoch 1023/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0498\n",
      "Epoch 01023: val_loss did not improve from 0.03252\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0498 - val_loss: 0.6951\n",
      "\n",
      "Epoch 01024: LearningRateScheduler reducing learning rate to 0.0002924137254119107.\n",
      "Epoch 1024/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0479\n",
      "Epoch 01024: val_loss did not improve from 0.03252\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0479 - val_loss: 0.4795\n",
      "\n",
      "Epoch 01025: LearningRateScheduler reducing learning rate to 0.00029239897134944215.\n",
      "Epoch 1025/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0386\n",
      "Epoch 01025: val_loss did not improve from 0.03252\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0386 - val_loss: 0.3140\n",
      "\n",
      "Epoch 01026: LearningRateScheduler reducing learning rate to 0.00029238420333060504.\n",
      "Epoch 1026/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0626\n",
      "Epoch 01026: val_loss did not improve from 0.03252\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0626 - val_loss: 0.1695\n",
      "\n",
      "Epoch 01027: LearningRateScheduler reducing learning rate to 0.00029236942135686195.\n",
      "Epoch 1027/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0561\n",
      "Epoch 01027: val_loss did not improve from 0.03252\n",
      "3/3 [==============================] - 1s 237ms/step - loss: 0.0561 - val_loss: 0.1068\n",
      "\n",
      "Epoch 01028: LearningRateScheduler reducing learning rate to 0.00029235462542967727.\n",
      "Epoch 1028/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0499\n",
      "Epoch 01028: val_loss did not improve from 0.03252\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0499 - val_loss: 0.0881\n",
      "\n",
      "Epoch 01029: LearningRateScheduler reducing learning rate to 0.0002923398155505164.\n",
      "Epoch 1029/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0502\n",
      "Epoch 01029: val_loss did not improve from 0.03252\n",
      "3/3 [==============================] - 1s 226ms/step - loss: 0.0502 - val_loss: 0.0830\n",
      "\n",
      "Epoch 01030: LearningRateScheduler reducing learning rate to 0.00029232499172084645.\n",
      "Epoch 1030/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0565\n",
      "Epoch 01030: val_loss did not improve from 0.03252\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0565 - val_loss: 0.0779\n",
      "\n",
      "Epoch 01031: LearningRateScheduler reducing learning rate to 0.0002923101539421356.\n",
      "Epoch 1031/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0485\n",
      "Epoch 01031: val_loss did not improve from 0.03252\n",
      "3/3 [==============================] - 1s 227ms/step - loss: 0.0485 - val_loss: 0.0699\n",
      "\n",
      "Epoch 01032: LearningRateScheduler reducing learning rate to 0.0002922953022158538.\n",
      "Epoch 1032/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0448\n",
      "Epoch 01032: val_loss did not improve from 0.03252\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0448 - val_loss: 0.0571\n",
      "\n",
      "Epoch 01033: LearningRateScheduler reducing learning rate to 0.00029228043654347185.\n",
      "Epoch 1033/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0816\n",
      "Epoch 01033: val_loss did not improve from 0.03252\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0816 - val_loss: 0.0480\n",
      "\n",
      "Epoch 01034: LearningRateScheduler reducing learning rate to 0.00029226555692646246.\n",
      "Epoch 1034/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0468\n",
      "Epoch 01034: val_loss did not improve from 0.03252\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0468 - val_loss: 0.0424\n",
      "\n",
      "Epoch 01035: LearningRateScheduler reducing learning rate to 0.00029225066336629937.\n",
      "Epoch 1035/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0551\n",
      "Epoch 01035: val_loss did not improve from 0.03252\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0551 - val_loss: 0.0378\n",
      "\n",
      "Epoch 01036: LearningRateScheduler reducing learning rate to 0.00029223575586445784.\n",
      "Epoch 1036/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0469\n",
      "Epoch 01036: val_loss did not improve from 0.03252\n",
      "3/3 [==============================] - 1s 227ms/step - loss: 0.0469 - val_loss: 0.0385\n",
      "\n",
      "Epoch 01037: LearningRateScheduler reducing learning rate to 0.00029222083442241456.\n",
      "Epoch 1037/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0403\n",
      "Epoch 01037: val_loss did not improve from 0.03252\n",
      "3/3 [==============================] - 1s 225ms/step - loss: 0.0403 - val_loss: 0.0397\n",
      "\n",
      "Epoch 01038: LearningRateScheduler reducing learning rate to 0.0002922058990416474.\n",
      "Epoch 1038/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0397\n",
      "Epoch 01038: val_loss did not improve from 0.03252\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0397 - val_loss: 0.0445\n",
      "\n",
      "Epoch 01039: LearningRateScheduler reducing learning rate to 0.00029219094972363595.\n",
      "Epoch 1039/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0529\n",
      "Epoch 01039: val_loss did not improve from 0.03252\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0529 - val_loss: 0.0452\n",
      "\n",
      "Epoch 01040: LearningRateScheduler reducing learning rate to 0.00029217598646986075.\n",
      "Epoch 1040/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0718\n",
      "Epoch 01040: val_loss did not improve from 0.03252\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0718 - val_loss: 0.0471\n",
      "\n",
      "Epoch 01041: LearningRateScheduler reducing learning rate to 0.00029216100928180404.\n",
      "Epoch 1041/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0410\n",
      "Epoch 01041: val_loss did not improve from 0.03252\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0410 - val_loss: 0.0515\n",
      "\n",
      "Epoch 01042: LearningRateScheduler reducing learning rate to 0.00029214601816094943.\n",
      "Epoch 1042/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0585\n",
      "Epoch 01042: val_loss did not improve from 0.03252\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0585 - val_loss: 0.0512\n",
      "\n",
      "Epoch 01043: LearningRateScheduler reducing learning rate to 0.0002921310131087817.\n",
      "Epoch 1043/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0402\n",
      "Epoch 01043: val_loss did not improve from 0.03252\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0402 - val_loss: 0.0465\n",
      "\n",
      "Epoch 01044: LearningRateScheduler reducing learning rate to 0.0002921159941267872.\n",
      "Epoch 1044/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0714\n",
      "Epoch 01044: val_loss did not improve from 0.03252\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0714 - val_loss: 0.0405\n",
      "\n",
      "Epoch 01045: LearningRateScheduler reducing learning rate to 0.00029210096121645354.\n",
      "Epoch 1045/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0519\n",
      "Epoch 01045: val_loss did not improve from 0.03252\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0519 - val_loss: 0.0511\n",
      "\n",
      "Epoch 01046: LearningRateScheduler reducing learning rate to 0.0002920859143792698.\n",
      "Epoch 1046/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0609\n",
      "Epoch 01046: val_loss did not improve from 0.03252\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0609 - val_loss: 0.0663\n",
      "\n",
      "Epoch 01047: LearningRateScheduler reducing learning rate to 0.00029207085361672647.\n",
      "Epoch 1047/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0577\n",
      "Epoch 01047: val_loss did not improve from 0.03252\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0577 - val_loss: 0.0755\n",
      "\n",
      "Epoch 01048: LearningRateScheduler reducing learning rate to 0.00029205577893031526.\n",
      "Epoch 1048/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0500\n",
      "Epoch 01048: val_loss did not improve from 0.03252\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0500 - val_loss: 0.0764\n",
      "\n",
      "Epoch 01049: LearningRateScheduler reducing learning rate to 0.00029204069032152935.\n",
      "Epoch 1049/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0499\n",
      "Epoch 01049: val_loss did not improve from 0.03252\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0499 - val_loss: 0.0868\n",
      "\n",
      "Epoch 01050: LearningRateScheduler reducing learning rate to 0.00029202558779186337.\n",
      "Epoch 1050/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0529\n",
      "Epoch 01050: val_loss did not improve from 0.03252\n",
      "3/3 [==============================] - 1s 228ms/step - loss: 0.0529 - val_loss: 0.0809\n",
      "\n",
      "Epoch 01051: LearningRateScheduler reducing learning rate to 0.0002920104713428132.\n",
      "Epoch 1051/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0369\n",
      "Epoch 01051: val_loss did not improve from 0.03252\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0369 - val_loss: 0.0590\n",
      "\n",
      "Epoch 01052: LearningRateScheduler reducing learning rate to 0.00029199534097587614.\n",
      "Epoch 1052/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0560\n",
      "Epoch 01052: val_loss did not improve from 0.03252\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0560 - val_loss: 0.0545\n",
      "\n",
      "Epoch 01053: LearningRateScheduler reducing learning rate to 0.000291980196692551.\n",
      "Epoch 1053/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0520\n",
      "Epoch 01053: val_loss did not improve from 0.03252\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0520 - val_loss: 0.0527\n",
      "\n",
      "Epoch 01054: LearningRateScheduler reducing learning rate to 0.0002919650384943378.\n",
      "Epoch 1054/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0459\n",
      "Epoch 01054: val_loss did not improve from 0.03252\n",
      "3/3 [==============================] - 1s 238ms/step - loss: 0.0459 - val_loss: 0.0478\n",
      "\n",
      "Epoch 01055: LearningRateScheduler reducing learning rate to 0.0002919498663827379.\n",
      "Epoch 1055/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0532\n",
      "Epoch 01055: val_loss did not improve from 0.03252\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0532 - val_loss: 0.0436\n",
      "\n",
      "Epoch 01056: LearningRateScheduler reducing learning rate to 0.0002919346803592543.\n",
      "Epoch 1056/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0562\n",
      "Epoch 01056: val_loss did not improve from 0.03252\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0562 - val_loss: 0.0438\n",
      "\n",
      "Epoch 01057: LearningRateScheduler reducing learning rate to 0.00029191948042539106.\n",
      "Epoch 1057/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0412\n",
      "Epoch 01057: val_loss did not improve from 0.03252\n",
      "3/3 [==============================] - 1s 239ms/step - loss: 0.0412 - val_loss: 0.0450\n",
      "\n",
      "Epoch 01058: LearningRateScheduler reducing learning rate to 0.00029190426658265385.\n",
      "Epoch 1058/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0763\n",
      "Epoch 01058: val_loss did not improve from 0.03252\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0763 - val_loss: 0.0497\n",
      "\n",
      "Epoch 01059: LearningRateScheduler reducing learning rate to 0.0002918890388325496.\n",
      "Epoch 1059/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0430\n",
      "Epoch 01059: val_loss did not improve from 0.03252\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0430 - val_loss: 0.0559\n",
      "\n",
      "Epoch 01060: LearningRateScheduler reducing learning rate to 0.00029187379717658673.\n",
      "Epoch 1060/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0461\n",
      "Epoch 01060: val_loss did not improve from 0.03252\n",
      "3/3 [==============================] - 1s 237ms/step - loss: 0.0461 - val_loss: 0.0573\n",
      "\n",
      "Epoch 01061: LearningRateScheduler reducing learning rate to 0.00029185854161627487.\n",
      "Epoch 1061/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0527\n",
      "Epoch 01061: val_loss did not improve from 0.03252\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0527 - val_loss: 0.0526\n",
      "\n",
      "Epoch 01062: LearningRateScheduler reducing learning rate to 0.0002918432721531252.\n",
      "Epoch 1062/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0407\n",
      "Epoch 01062: val_loss did not improve from 0.03252\n",
      "3/3 [==============================] - 1s 226ms/step - loss: 0.0407 - val_loss: 0.0477\n",
      "\n",
      "Epoch 01063: LearningRateScheduler reducing learning rate to 0.00029182798878865006.\n",
      "Epoch 1063/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0483\n",
      "Epoch 01063: val_loss did not improve from 0.03252\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0483 - val_loss: 0.0435\n",
      "\n",
      "Epoch 01064: LearningRateScheduler reducing learning rate to 0.0002918126915243634.\n",
      "Epoch 1064/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0530\n",
      "Epoch 01064: val_loss did not improve from 0.03252\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0530 - val_loss: 0.0430\n",
      "\n",
      "Epoch 01065: LearningRateScheduler reducing learning rate to 0.0002917973803617805.\n",
      "Epoch 1065/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0726\n",
      "Epoch 01065: val_loss did not improve from 0.03252\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0726 - val_loss: 0.0430\n",
      "\n",
      "Epoch 01066: LearningRateScheduler reducing learning rate to 0.0002917820553024179.\n",
      "Epoch 1066/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0469\n",
      "Epoch 01066: val_loss did not improve from 0.03252\n",
      "3/3 [==============================] - 1s 227ms/step - loss: 0.0469 - val_loss: 0.0434\n",
      "\n",
      "Epoch 01067: LearningRateScheduler reducing learning rate to 0.00029176671634779353.\n",
      "Epoch 1067/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0527\n",
      "Epoch 01067: val_loss did not improve from 0.03252\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0527 - val_loss: 0.0422\n",
      "\n",
      "Epoch 01068: LearningRateScheduler reducing learning rate to 0.0002917513634994268.\n",
      "Epoch 1068/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0329\n",
      "Epoch 01068: val_loss did not improve from 0.03252\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0329 - val_loss: 0.0407\n",
      "\n",
      "Epoch 01069: LearningRateScheduler reducing learning rate to 0.00029173599675883855.\n",
      "Epoch 1069/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0678\n",
      "Epoch 01069: val_loss did not improve from 0.03252\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0678 - val_loss: 0.0421\n",
      "\n",
      "Epoch 01070: LearningRateScheduler reducing learning rate to 0.0002917206161275507.\n",
      "Epoch 1070/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0395\n",
      "Epoch 01070: val_loss did not improve from 0.03252\n",
      "3/3 [==============================] - 1s 238ms/step - loss: 0.0395 - val_loss: 0.0445\n",
      "\n",
      "Epoch 01071: LearningRateScheduler reducing learning rate to 0.0002917052216070869.\n",
      "Epoch 1071/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0525\n",
      "Epoch 01071: val_loss did not improve from 0.03252\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0525 - val_loss: 0.0472\n",
      "\n",
      "Epoch 01072: LearningRateScheduler reducing learning rate to 0.00029168981319897195.\n",
      "Epoch 1072/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0335\n",
      "Epoch 01072: val_loss did not improve from 0.03252\n",
      "3/3 [==============================] - 1s 228ms/step - loss: 0.0335 - val_loss: 0.0481\n",
      "\n",
      "Epoch 01073: LearningRateScheduler reducing learning rate to 0.00029167439090473205.\n",
      "Epoch 1073/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0361\n",
      "Epoch 01073: val_loss did not improve from 0.03252\n",
      "3/3 [==============================] - 1s 227ms/step - loss: 0.0361 - val_loss: 0.0467\n",
      "\n",
      "Epoch 01074: LearningRateScheduler reducing learning rate to 0.00029165895472589484.\n",
      "Epoch 1074/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0615\n",
      "Epoch 01074: val_loss did not improve from 0.03252\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0615 - val_loss: 0.0476\n",
      "\n",
      "Epoch 01075: LearningRateScheduler reducing learning rate to 0.00029164350466398927.\n",
      "Epoch 1075/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0486\n",
      "Epoch 01075: val_loss did not improve from 0.03252\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0486 - val_loss: 0.0458\n",
      "\n",
      "Epoch 01076: LearningRateScheduler reducing learning rate to 0.00029162804072054585.\n",
      "Epoch 1076/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0403\n",
      "Epoch 01076: val_loss did not improve from 0.03252\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0403 - val_loss: 0.0471\n",
      "\n",
      "Epoch 01077: LearningRateScheduler reducing learning rate to 0.00029161256289709613.\n",
      "Epoch 1077/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0441\n",
      "Epoch 01077: val_loss did not improve from 0.03252\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0441 - val_loss: 0.0473\n",
      "\n",
      "Epoch 01078: LearningRateScheduler reducing learning rate to 0.0002915970711951734.\n",
      "Epoch 1078/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0479\n",
      "Epoch 01078: val_loss did not improve from 0.03252\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0479 - val_loss: 0.0479\n",
      "\n",
      "Epoch 01079: LearningRateScheduler reducing learning rate to 0.000291581565616312.\n",
      "Epoch 1079/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0455\n",
      "Epoch 01079: val_loss did not improve from 0.03252\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0455 - val_loss: 0.0459\n",
      "\n",
      "Epoch 01080: LearningRateScheduler reducing learning rate to 0.00029156604616204786.\n",
      "Epoch 1080/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0685\n",
      "Epoch 01080: val_loss did not improve from 0.03252\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0685 - val_loss: 0.0435\n",
      "\n",
      "Epoch 01081: LearningRateScheduler reducing learning rate to 0.0002915505128339182.\n",
      "Epoch 1081/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0531\n",
      "Epoch 01081: val_loss did not improve from 0.03252\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0531 - val_loss: 0.0391\n",
      "\n",
      "Epoch 01082: LearningRateScheduler reducing learning rate to 0.0002915349656334617.\n",
      "Epoch 1082/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0534\n",
      "Epoch 01082: val_loss did not improve from 0.03252\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0534 - val_loss: 0.0387\n",
      "\n",
      "Epoch 01083: LearningRateScheduler reducing learning rate to 0.0002915194045622182.\n",
      "Epoch 1083/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0384\n",
      "Epoch 01083: val_loss did not improve from 0.03252\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0384 - val_loss: 0.0378\n",
      "\n",
      "Epoch 01084: LearningRateScheduler reducing learning rate to 0.00029150382962172926.\n",
      "Epoch 1084/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0351\n",
      "Epoch 01084: val_loss did not improve from 0.03252\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0351 - val_loss: 0.0418\n",
      "\n",
      "Epoch 01085: LearningRateScheduler reducing learning rate to 0.0002914882408135374.\n",
      "Epoch 1085/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0461\n",
      "Epoch 01085: val_loss did not improve from 0.03252\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0461 - val_loss: 0.0436\n",
      "\n",
      "Epoch 01086: LearningRateScheduler reducing learning rate to 0.0002914726381391869.\n",
      "Epoch 1086/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0475\n",
      "Epoch 01086: val_loss did not improve from 0.03252\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0475 - val_loss: 0.0412\n",
      "\n",
      "Epoch 01087: LearningRateScheduler reducing learning rate to 0.0002914570216002232.\n",
      "Epoch 1087/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0396\n",
      "Epoch 01087: val_loss did not improve from 0.03252\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0396 - val_loss: 0.0367\n",
      "\n",
      "Epoch 01088: LearningRateScheduler reducing learning rate to 0.0002914413911981931.\n",
      "Epoch 1088/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0620\n",
      "Epoch 01088: val_loss improved from 0.03252 to 0.03102, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.0620 - val_loss: 0.0310\n",
      "\n",
      "Epoch 01089: LearningRateScheduler reducing learning rate to 0.00029142574693464483.\n",
      "Epoch 1089/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0479\n",
      "Epoch 01089: val_loss improved from 0.03102 to 0.02809, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.0479 - val_loss: 0.0281\n",
      "\n",
      "Epoch 01090: LearningRateScheduler reducing learning rate to 0.000291410088811128.\n",
      "Epoch 1090/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0491\n",
      "Epoch 01090: val_loss did not improve from 0.02809\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0491 - val_loss: 0.0311\n",
      "\n",
      "Epoch 01091: LearningRateScheduler reducing learning rate to 0.00029139441682919366.\n",
      "Epoch 1091/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0487\n",
      "Epoch 01091: val_loss did not improve from 0.02809\n",
      "3/3 [==============================] - 1s 237ms/step - loss: 0.0487 - val_loss: 0.0325\n",
      "\n",
      "Epoch 01092: LearningRateScheduler reducing learning rate to 0.0002913787309903941.\n",
      "Epoch 1092/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0527\n",
      "Epoch 01092: val_loss did not improve from 0.02809\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0527 - val_loss: 0.0325\n",
      "\n",
      "Epoch 01093: LearningRateScheduler reducing learning rate to 0.00029136303129628294.\n",
      "Epoch 1093/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0393\n",
      "Epoch 01093: val_loss did not improve from 0.02809\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0393 - val_loss: 0.0330\n",
      "\n",
      "Epoch 01094: LearningRateScheduler reducing learning rate to 0.00029134731774841543.\n",
      "Epoch 1094/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0509\n",
      "Epoch 01094: val_loss did not improve from 0.02809\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0509 - val_loss: 0.0327\n",
      "\n",
      "Epoch 01095: LearningRateScheduler reducing learning rate to 0.00029133159034834793.\n",
      "Epoch 1095/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0363\n",
      "Epoch 01095: val_loss did not improve from 0.02809\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0363 - val_loss: 0.0305\n",
      "\n",
      "Epoch 01096: LearningRateScheduler reducing learning rate to 0.0002913158490976384.\n",
      "Epoch 1096/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0342\n",
      "Epoch 01096: val_loss improved from 0.02809 to 0.02774, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.0342 - val_loss: 0.0277\n",
      "\n",
      "Epoch 01097: LearningRateScheduler reducing learning rate to 0.0002913000939978459.\n",
      "Epoch 1097/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0541\n",
      "Epoch 01097: val_loss did not improve from 0.02774\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0541 - val_loss: 0.0319\n",
      "\n",
      "Epoch 01098: LearningRateScheduler reducing learning rate to 0.0002912843250505311.\n",
      "Epoch 1098/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0380\n",
      "Epoch 01098: val_loss did not improve from 0.02774\n",
      "3/3 [==============================] - 1s 228ms/step - loss: 0.0380 - val_loss: 0.0330\n",
      "\n",
      "Epoch 01099: LearningRateScheduler reducing learning rate to 0.0002912685422572559.\n",
      "Epoch 1099/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0360\n",
      "Epoch 01099: val_loss did not improve from 0.02774\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0360 - val_loss: 0.0354\n",
      "\n",
      "Epoch 01100: LearningRateScheduler reducing learning rate to 0.0002912527456195836.\n",
      "Epoch 1100/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0294\n",
      "Epoch 01100: val_loss did not improve from 0.02774\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0294 - val_loss: 0.0370\n",
      "\n",
      "Epoch 01101: LearningRateScheduler reducing learning rate to 0.00029123693513907897.\n",
      "Epoch 1101/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0460\n",
      "Epoch 01101: val_loss did not improve from 0.02774\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0460 - val_loss: 0.0364\n",
      "\n",
      "Epoch 01102: LearningRateScheduler reducing learning rate to 0.000291221110817308.\n",
      "Epoch 1102/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0405\n",
      "Epoch 01102: val_loss did not improve from 0.02774\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0405 - val_loss: 0.0352\n",
      "\n",
      "Epoch 01103: LearningRateScheduler reducing learning rate to 0.00029120527265583823.\n",
      "Epoch 1103/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0408\n",
      "Epoch 01103: val_loss did not improve from 0.02774\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0408 - val_loss: 0.0299\n",
      "\n",
      "Epoch 01104: LearningRateScheduler reducing learning rate to 0.0002911894206562384.\n",
      "Epoch 1104/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0393\n",
      "Epoch 01104: val_loss did not improve from 0.02774\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0393 - val_loss: 0.0313\n",
      "\n",
      "Epoch 01105: LearningRateScheduler reducing learning rate to 0.0002911735548200787.\n",
      "Epoch 1105/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0332\n",
      "Epoch 01105: val_loss did not improve from 0.02774\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0332 - val_loss: 0.0333\n",
      "\n",
      "Epoch 01106: LearningRateScheduler reducing learning rate to 0.00029115767514893064.\n",
      "Epoch 1106/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0397\n",
      "Epoch 01106: val_loss did not improve from 0.02774\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0397 - val_loss: 0.0328\n",
      "\n",
      "Epoch 01107: LearningRateScheduler reducing learning rate to 0.0002911417816443671.\n",
      "Epoch 1107/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0379\n",
      "Epoch 01107: val_loss did not improve from 0.02774\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0379 - val_loss: 0.0335\n",
      "\n",
      "Epoch 01108: LearningRateScheduler reducing learning rate to 0.00029112587430796255.\n",
      "Epoch 1108/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0436\n",
      "Epoch 01108: val_loss did not improve from 0.02774\n",
      "3/3 [==============================] - 1s 228ms/step - loss: 0.0436 - val_loss: 0.0340\n",
      "\n",
      "Epoch 01109: LearningRateScheduler reducing learning rate to 0.00029110995314129246.\n",
      "Epoch 1109/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0407\n",
      "Epoch 01109: val_loss did not improve from 0.02774\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0407 - val_loss: 0.0400\n",
      "\n",
      "Epoch 01110: LearningRateScheduler reducing learning rate to 0.00029109401814593396.\n",
      "Epoch 1110/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0570\n",
      "Epoch 01110: val_loss did not improve from 0.02774\n",
      "3/3 [==============================] - 1s 238ms/step - loss: 0.0570 - val_loss: 0.0431\n",
      "\n",
      "Epoch 01111: LearningRateScheduler reducing learning rate to 0.0002910780693234654.\n",
      "Epoch 1111/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0537\n",
      "Epoch 01111: val_loss did not improve from 0.02774\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0537 - val_loss: 0.0421\n",
      "\n",
      "Epoch 01112: LearningRateScheduler reducing learning rate to 0.00029106210667546656.\n",
      "Epoch 1112/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0494\n",
      "Epoch 01112: val_loss did not improve from 0.02774\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0494 - val_loss: 0.0362\n",
      "\n",
      "Epoch 01113: LearningRateScheduler reducing learning rate to 0.0002910461302035186.\n",
      "Epoch 1113/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0486\n",
      "Epoch 01113: val_loss did not improve from 0.02774\n",
      "3/3 [==============================] - 1s 238ms/step - loss: 0.0486 - val_loss: 0.0330\n",
      "\n",
      "Epoch 01114: LearningRateScheduler reducing learning rate to 0.000291030139909204.\n",
      "Epoch 1114/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0409\n",
      "Epoch 01114: val_loss did not improve from 0.02774\n",
      "3/3 [==============================] - 1s 239ms/step - loss: 0.0409 - val_loss: 0.0333\n",
      "\n",
      "Epoch 01115: LearningRateScheduler reducing learning rate to 0.0002910141357941067.\n",
      "Epoch 1115/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0349\n",
      "Epoch 01115: val_loss did not improve from 0.02774\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0349 - val_loss: 0.0318\n",
      "\n",
      "Epoch 01116: LearningRateScheduler reducing learning rate to 0.00029099811785981186.\n",
      "Epoch 1116/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0431\n",
      "Epoch 01116: val_loss did not improve from 0.02774\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0431 - val_loss: 0.0306\n",
      "\n",
      "Epoch 01117: LearningRateScheduler reducing learning rate to 0.0002909820861079061.\n",
      "Epoch 1117/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0442\n",
      "Epoch 01117: val_loss did not improve from 0.02774\n",
      "3/3 [==============================] - 1s 228ms/step - loss: 0.0442 - val_loss: 0.0298\n",
      "\n",
      "Epoch 01118: LearningRateScheduler reducing learning rate to 0.0002909660405399774.\n",
      "Epoch 1118/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0378\n",
      "Epoch 01118: val_loss did not improve from 0.02774\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0378 - val_loss: 0.0310\n",
      "\n",
      "Epoch 01119: LearningRateScheduler reducing learning rate to 0.0002909499811576152.\n",
      "Epoch 1119/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0449\n",
      "Epoch 01119: val_loss did not improve from 0.02774\n",
      "3/3 [==============================] - 1s 238ms/step - loss: 0.0449 - val_loss: 0.0330\n",
      "\n",
      "Epoch 01120: LearningRateScheduler reducing learning rate to 0.0002909339079624101.\n",
      "Epoch 1120/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0506\n",
      "Epoch 01120: val_loss did not improve from 0.02774\n",
      "3/3 [==============================] - 1s 237ms/step - loss: 0.0506 - val_loss: 0.0352\n",
      "\n",
      "Epoch 01121: LearningRateScheduler reducing learning rate to 0.0002909178209559543.\n",
      "Epoch 1121/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0368\n",
      "Epoch 01121: val_loss did not improve from 0.02774\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0368 - val_loss: 0.0351\n",
      "\n",
      "Epoch 01122: LearningRateScheduler reducing learning rate to 0.0002909017201398412.\n",
      "Epoch 1122/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0440\n",
      "Epoch 01122: val_loss did not improve from 0.02774\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0440 - val_loss: 0.0335\n",
      "\n",
      "Epoch 01123: LearningRateScheduler reducing learning rate to 0.0002908856055156655.\n",
      "Epoch 1123/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0353\n",
      "Epoch 01123: val_loss did not improve from 0.02774\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0353 - val_loss: 0.0333\n",
      "\n",
      "Epoch 01124: LearningRateScheduler reducing learning rate to 0.00029086947708502364.\n",
      "Epoch 1124/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0330\n",
      "Epoch 01124: val_loss did not improve from 0.02774\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0330 - val_loss: 0.0343\n",
      "\n",
      "Epoch 01125: LearningRateScheduler reducing learning rate to 0.000290853334849513.\n",
      "Epoch 1125/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0650\n",
      "Epoch 01125: val_loss did not improve from 0.02774\n",
      "3/3 [==============================] - 1s 238ms/step - loss: 0.0650 - val_loss: 0.0347\n",
      "\n",
      "Epoch 01126: LearningRateScheduler reducing learning rate to 0.00029083717881073256.\n",
      "Epoch 1126/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0299\n",
      "Epoch 01126: val_loss did not improve from 0.02774\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0299 - val_loss: 0.0323\n",
      "\n",
      "Epoch 01127: LearningRateScheduler reducing learning rate to 0.0002908210089702826.\n",
      "Epoch 1127/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0457\n",
      "Epoch 01127: val_loss did not improve from 0.02774\n",
      "3/3 [==============================] - 1s 227ms/step - loss: 0.0457 - val_loss: 0.0315\n",
      "\n",
      "Epoch 01128: LearningRateScheduler reducing learning rate to 0.0002908048253297648.\n",
      "Epoch 1128/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0425\n",
      "Epoch 01128: val_loss did not improve from 0.02774\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0425 - val_loss: 0.0335\n",
      "\n",
      "Epoch 01129: LearningRateScheduler reducing learning rate to 0.0002907886278907822.\n",
      "Epoch 1129/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0322\n",
      "Epoch 01129: val_loss did not improve from 0.02774\n",
      "3/3 [==============================] - 1s 227ms/step - loss: 0.0322 - val_loss: 0.0315\n",
      "\n",
      "Epoch 01130: LearningRateScheduler reducing learning rate to 0.0002907724166549392.\n",
      "Epoch 1130/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0557\n",
      "Epoch 01130: val_loss did not improve from 0.02774\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0557 - val_loss: 0.0297\n",
      "\n",
      "Epoch 01131: LearningRateScheduler reducing learning rate to 0.00029075619162384145.\n",
      "Epoch 1131/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0465\n",
      "Epoch 01131: val_loss improved from 0.02774 to 0.02764, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.0465 - val_loss: 0.0276\n",
      "\n",
      "Epoch 01132: LearningRateScheduler reducing learning rate to 0.00029073995279909617.\n",
      "Epoch 1132/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0377\n",
      "Epoch 01132: val_loss did not improve from 0.02764\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0377 - val_loss: 0.0280\n",
      "\n",
      "Epoch 01133: LearningRateScheduler reducing learning rate to 0.0002907237001823118.\n",
      "Epoch 1133/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0436\n",
      "Epoch 01133: val_loss did not improve from 0.02764\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0436 - val_loss: 0.0283\n",
      "\n",
      "Epoch 01134: LearningRateScheduler reducing learning rate to 0.0002907074337750983.\n",
      "Epoch 1134/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0331\n",
      "Epoch 01134: val_loss did not improve from 0.02764\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0331 - val_loss: 0.0279\n",
      "\n",
      "Epoch 01135: LearningRateScheduler reducing learning rate to 0.0002906911535790669.\n",
      "Epoch 1135/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0453\n",
      "Epoch 01135: val_loss did not improve from 0.02764\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0453 - val_loss: 0.0282\n",
      "\n",
      "Epoch 01136: LearningRateScheduler reducing learning rate to 0.00029067485959583.\n",
      "Epoch 1136/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0330\n",
      "Epoch 01136: val_loss did not improve from 0.02764\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0330 - val_loss: 0.0284\n",
      "\n",
      "Epoch 01137: LearningRateScheduler reducing learning rate to 0.00029065855182700174.\n",
      "Epoch 1137/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0381\n",
      "Epoch 01137: val_loss did not improve from 0.02764\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0381 - val_loss: 0.0304\n",
      "\n",
      "Epoch 01138: LearningRateScheduler reducing learning rate to 0.00029064223027419736.\n",
      "Epoch 1138/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0470\n",
      "Epoch 01138: val_loss did not improve from 0.02764\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0470 - val_loss: 0.0318\n",
      "\n",
      "Epoch 01139: LearningRateScheduler reducing learning rate to 0.0002906258949390336.\n",
      "Epoch 1139/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0351\n",
      "Epoch 01139: val_loss did not improve from 0.02764\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0351 - val_loss: 0.0346\n",
      "\n",
      "Epoch 01140: LearningRateScheduler reducing learning rate to 0.0002906095458231285.\n",
      "Epoch 1140/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0394\n",
      "Epoch 01140: val_loss did not improve from 0.02764\n",
      "3/3 [==============================] - 1s 242ms/step - loss: 0.0394 - val_loss: 0.0370\n",
      "\n",
      "Epoch 01141: LearningRateScheduler reducing learning rate to 0.0002905931829281014.\n",
      "Epoch 1141/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0390\n",
      "Epoch 01141: val_loss did not improve from 0.02764\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0390 - val_loss: 0.0342\n",
      "\n",
      "Epoch 01142: LearningRateScheduler reducing learning rate to 0.0002905768062555732.\n",
      "Epoch 1142/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0393\n",
      "Epoch 01142: val_loss did not improve from 0.02764\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0393 - val_loss: 0.0284\n",
      "\n",
      "Epoch 01143: LearningRateScheduler reducing learning rate to 0.0002905604158071659.\n",
      "Epoch 1143/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0369\n",
      "Epoch 01143: val_loss improved from 0.02764 to 0.02572, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.0369 - val_loss: 0.0257\n",
      "\n",
      "Epoch 01144: LearningRateScheduler reducing learning rate to 0.0002905440115845032.\n",
      "Epoch 1144/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0495\n",
      "Epoch 01144: val_loss did not improve from 0.02572\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0495 - val_loss: 0.0324\n",
      "\n",
      "Epoch 01145: LearningRateScheduler reducing learning rate to 0.00029052759358920985.\n",
      "Epoch 1145/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0493\n",
      "Epoch 01145: val_loss did not improve from 0.02572\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0493 - val_loss: 0.0354\n",
      "\n",
      "Epoch 01146: LearningRateScheduler reducing learning rate to 0.00029051116182291214.\n",
      "Epoch 1146/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0316\n",
      "Epoch 01146: val_loss did not improve from 0.02572\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0316 - val_loss: 0.0333\n",
      "\n",
      "Epoch 01147: LearningRateScheduler reducing learning rate to 0.0002904947162872376.\n",
      "Epoch 1147/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0441\n",
      "Epoch 01147: val_loss did not improve from 0.02572\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0441 - val_loss: 0.0308\n",
      "\n",
      "Epoch 01148: LearningRateScheduler reducing learning rate to 0.0002904782569838153.\n",
      "Epoch 1148/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0420\n",
      "Epoch 01148: val_loss improved from 0.02572 to 0.02559, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.0420 - val_loss: 0.0256\n",
      "\n",
      "Epoch 01149: LearningRateScheduler reducing learning rate to 0.00029046178391427556.\n",
      "Epoch 1149/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0537\n",
      "Epoch 01149: val_loss did not improve from 0.02559\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0537 - val_loss: 0.0276\n",
      "\n",
      "Epoch 01150: LearningRateScheduler reducing learning rate to 0.00029044529708025.\n",
      "Epoch 1150/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0441\n",
      "Epoch 01150: val_loss did not improve from 0.02559\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0441 - val_loss: 0.0310\n",
      "\n",
      "Epoch 01151: LearningRateScheduler reducing learning rate to 0.0002904287964833717.\n",
      "Epoch 1151/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0460\n",
      "Epoch 01151: val_loss did not improve from 0.02559\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0460 - val_loss: 0.0328\n",
      "\n",
      "Epoch 01152: LearningRateScheduler reducing learning rate to 0.0002904122821252751.\n",
      "Epoch 1152/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0384\n",
      "Epoch 01152: val_loss did not improve from 0.02559\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0384 - val_loss: 0.0359\n",
      "\n",
      "Epoch 01153: LearningRateScheduler reducing learning rate to 0.000290395754007596.\n",
      "Epoch 1153/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0410\n",
      "Epoch 01153: val_loss did not improve from 0.02559\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0410 - val_loss: 0.0356\n",
      "\n",
      "Epoch 01154: LearningRateScheduler reducing learning rate to 0.00029037921213197155.\n",
      "Epoch 1154/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0544\n",
      "Epoch 01154: val_loss did not improve from 0.02559\n",
      "3/3 [==============================] - 1s 228ms/step - loss: 0.0544 - val_loss: 0.0333\n",
      "\n",
      "Epoch 01155: LearningRateScheduler reducing learning rate to 0.00029036265650004027.\n",
      "Epoch 1155/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0432\n",
      "Epoch 01155: val_loss did not improve from 0.02559\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0432 - val_loss: 0.0301\n",
      "\n",
      "Epoch 01156: LearningRateScheduler reducing learning rate to 0.0002903460871134419.\n",
      "Epoch 1156/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0403\n",
      "Epoch 01156: val_loss did not improve from 0.02559\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0403 - val_loss: 0.0302\n",
      "\n",
      "Epoch 01157: LearningRateScheduler reducing learning rate to 0.0002903295039738179.\n",
      "Epoch 1157/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0476\n",
      "Epoch 01157: val_loss did not improve from 0.02559\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0476 - val_loss: 0.0322\n",
      "\n",
      "Epoch 01158: LearningRateScheduler reducing learning rate to 0.0002903129070828107.\n",
      "Epoch 1158/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0342\n",
      "Epoch 01158: val_loss did not improve from 0.02559\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0342 - val_loss: 0.0325\n",
      "\n",
      "Epoch 01159: LearningRateScheduler reducing learning rate to 0.0002902962964420643.\n",
      "Epoch 1159/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0423\n",
      "Epoch 01159: val_loss did not improve from 0.02559\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0423 - val_loss: 0.0324\n",
      "\n",
      "Epoch 01160: LearningRateScheduler reducing learning rate to 0.00029027967205322406.\n",
      "Epoch 1160/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0446\n",
      "Epoch 01160: val_loss did not improve from 0.02559\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0446 - val_loss: 0.0293\n",
      "\n",
      "Epoch 01161: LearningRateScheduler reducing learning rate to 0.0002902630339179367.\n",
      "Epoch 1161/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0444\n",
      "Epoch 01161: val_loss did not improve from 0.02559\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0444 - val_loss: 0.0261\n",
      "\n",
      "Epoch 01162: LearningRateScheduler reducing learning rate to 0.00029024638203785017.\n",
      "Epoch 1162/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0534\n",
      "Epoch 01162: val_loss did not improve from 0.02559\n",
      "3/3 [==============================] - 1s 227ms/step - loss: 0.0534 - val_loss: 0.0258\n",
      "\n",
      "Epoch 01163: LearningRateScheduler reducing learning rate to 0.00029022971641461386.\n",
      "Epoch 1163/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0351\n",
      "Epoch 01163: val_loss did not improve from 0.02559\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0351 - val_loss: 0.0276\n",
      "\n",
      "Epoch 01164: LearningRateScheduler reducing learning rate to 0.0002902130370498786.\n",
      "Epoch 1164/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0350\n",
      "Epoch 01164: val_loss did not improve from 0.02559\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0350 - val_loss: 0.0314\n",
      "\n",
      "Epoch 01165: LearningRateScheduler reducing learning rate to 0.00029019634394529656.\n",
      "Epoch 1165/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0398\n",
      "Epoch 01165: val_loss did not improve from 0.02559\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0398 - val_loss: 0.0325\n",
      "\n",
      "Epoch 01166: LearningRateScheduler reducing learning rate to 0.00029017963710252113.\n",
      "Epoch 1166/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0498\n",
      "Epoch 01166: val_loss did not improve from 0.02559\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0498 - val_loss: 0.0294\n",
      "\n",
      "Epoch 01167: LearningRateScheduler reducing learning rate to 0.00029016291652320727.\n",
      "Epoch 1167/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0382\n",
      "Epoch 01167: val_loss did not improve from 0.02559\n",
      "3/3 [==============================] - 1s 237ms/step - loss: 0.0382 - val_loss: 0.0274\n",
      "\n",
      "Epoch 01168: LearningRateScheduler reducing learning rate to 0.0002901461822090111.\n",
      "Epoch 1168/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0498\n",
      "Epoch 01168: val_loss did not improve from 0.02559\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0498 - val_loss: 0.0260\n",
      "\n",
      "Epoch 01169: LearningRateScheduler reducing learning rate to 0.0002901294341615902.\n",
      "Epoch 1169/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0372\n",
      "Epoch 01169: val_loss did not improve from 0.02559\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0372 - val_loss: 0.0256\n",
      "\n",
      "Epoch 01170: LearningRateScheduler reducing learning rate to 0.0002901126723826036.\n",
      "Epoch 1170/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0311\n",
      "Epoch 01170: val_loss did not improve from 0.02559\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0311 - val_loss: 0.0262\n",
      "\n",
      "Epoch 01171: LearningRateScheduler reducing learning rate to 0.0002900958968737115.\n",
      "Epoch 1171/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0383\n",
      "Epoch 01171: val_loss did not improve from 0.02559\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0383 - val_loss: 0.0276\n",
      "\n",
      "Epoch 01172: LearningRateScheduler reducing learning rate to 0.0002900791076365756.\n",
      "Epoch 1172/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0357\n",
      "Epoch 01172: val_loss did not improve from 0.02559\n",
      "3/3 [==============================] - 1s 226ms/step - loss: 0.0357 - val_loss: 0.0285\n",
      "\n",
      "Epoch 01173: LearningRateScheduler reducing learning rate to 0.0002900623046728589.\n",
      "Epoch 1173/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0369\n",
      "Epoch 01173: val_loss did not improve from 0.02559\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0369 - val_loss: 0.0274\n",
      "\n",
      "Epoch 01174: LearningRateScheduler reducing learning rate to 0.0002900454879842257.\n",
      "Epoch 1174/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0446\n",
      "Epoch 01174: val_loss did not improve from 0.02559\n",
      "3/3 [==============================] - 1s 228ms/step - loss: 0.0446 - val_loss: 0.0270\n",
      "\n",
      "Epoch 01175: LearningRateScheduler reducing learning rate to 0.0002900286575723418.\n",
      "Epoch 1175/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0387\n",
      "Epoch 01175: val_loss did not improve from 0.02559\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0387 - val_loss: 0.0274\n",
      "\n",
      "Epoch 01176: LearningRateScheduler reducing learning rate to 0.0002900118134388743.\n",
      "Epoch 1176/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0326\n",
      "Epoch 01176: val_loss did not improve from 0.02559\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0326 - val_loss: 0.0299\n",
      "\n",
      "Epoch 01177: LearningRateScheduler reducing learning rate to 0.00028999495558549166.\n",
      "Epoch 1177/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0378\n",
      "Epoch 01177: val_loss did not improve from 0.02559\n",
      "3/3 [==============================] - 1s 227ms/step - loss: 0.0378 - val_loss: 0.0301\n",
      "\n",
      "Epoch 01178: LearningRateScheduler reducing learning rate to 0.0002899780840138636.\n",
      "Epoch 1178/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0431\n",
      "Epoch 01178: val_loss did not improve from 0.02559\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0431 - val_loss: 0.0300\n",
      "\n",
      "Epoch 01179: LearningRateScheduler reducing learning rate to 0.00028996119872566145.\n",
      "Epoch 1179/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0363\n",
      "Epoch 01179: val_loss did not improve from 0.02559\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0363 - val_loss: 0.0297\n",
      "\n",
      "Epoch 01180: LearningRateScheduler reducing learning rate to 0.0002899442997225576.\n",
      "Epoch 1180/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0326\n",
      "Epoch 01180: val_loss did not improve from 0.02559\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0326 - val_loss: 0.0280\n",
      "\n",
      "Epoch 01181: LearningRateScheduler reducing learning rate to 0.00028992738700622594.\n",
      "Epoch 1181/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0313\n",
      "Epoch 01181: val_loss did not improve from 0.02559\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0313 - val_loss: 0.0272\n",
      "\n",
      "Epoch 01182: LearningRateScheduler reducing learning rate to 0.00028991046057834174.\n",
      "Epoch 1182/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0523\n",
      "Epoch 01182: val_loss did not improve from 0.02559\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0523 - val_loss: 0.0270\n",
      "\n",
      "Epoch 01183: LearningRateScheduler reducing learning rate to 0.0002898935204405817.\n",
      "Epoch 1183/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0331\n",
      "Epoch 01183: val_loss did not improve from 0.02559\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0331 - val_loss: 0.0313\n",
      "\n",
      "Epoch 01184: LearningRateScheduler reducing learning rate to 0.0002898765665946236.\n",
      "Epoch 1184/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0520\n",
      "Epoch 01184: val_loss did not improve from 0.02559\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0520 - val_loss: 0.0324\n",
      "\n",
      "Epoch 01185: LearningRateScheduler reducing learning rate to 0.0002898595990421469.\n",
      "Epoch 1185/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0367\n",
      "Epoch 01185: val_loss did not improve from 0.02559\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0367 - val_loss: 0.0315\n",
      "\n",
      "Epoch 01186: LearningRateScheduler reducing learning rate to 0.0002898426177848322.\n",
      "Epoch 1186/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0373\n",
      "Epoch 01186: val_loss did not improve from 0.02559\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0373 - val_loss: 0.0317\n",
      "\n",
      "Epoch 01187: LearningRateScheduler reducing learning rate to 0.00028982562282436154.\n",
      "Epoch 1187/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0265\n",
      "Epoch 01187: val_loss did not improve from 0.02559\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0265 - val_loss: 0.0313\n",
      "\n",
      "Epoch 01188: LearningRateScheduler reducing learning rate to 0.00028980861416241836.\n",
      "Epoch 1188/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0292\n",
      "Epoch 01188: val_loss did not improve from 0.02559\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0292 - val_loss: 0.0291\n",
      "\n",
      "Epoch 01189: LearningRateScheduler reducing learning rate to 0.00028979159180068736.\n",
      "Epoch 1189/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0353\n",
      "Epoch 01189: val_loss did not improve from 0.02559\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0353 - val_loss: 0.0278\n",
      "\n",
      "Epoch 01190: LearningRateScheduler reducing learning rate to 0.0002897745557408546.\n",
      "Epoch 1190/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0500\n",
      "Epoch 01190: val_loss did not improve from 0.02559\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0500 - val_loss: 0.0259\n",
      "\n",
      "Epoch 01191: LearningRateScheduler reducing learning rate to 0.0002897575059846077.\n",
      "Epoch 1191/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0449\n",
      "Epoch 01191: val_loss did not improve from 0.02559\n",
      "3/3 [==============================] - 1s 227ms/step - loss: 0.0449 - val_loss: 0.0264\n",
      "\n",
      "Epoch 01192: LearningRateScheduler reducing learning rate to 0.0002897404425336353.\n",
      "Epoch 1192/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0388\n",
      "Epoch 01192: val_loss improved from 0.02559 to 0.02542, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.0388 - val_loss: 0.0254\n",
      "\n",
      "Epoch 01193: LearningRateScheduler reducing learning rate to 0.0002897233653896277.\n",
      "Epoch 1193/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0277\n",
      "Epoch 01193: val_loss did not improve from 0.02542\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0277 - val_loss: 0.0273\n",
      "\n",
      "Epoch 01194: LearningRateScheduler reducing learning rate to 0.0002897062745542764.\n",
      "Epoch 1194/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0342\n",
      "Epoch 01194: val_loss did not improve from 0.02542\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0342 - val_loss: 0.0301\n",
      "\n",
      "Epoch 01195: LearningRateScheduler reducing learning rate to 0.00028968917002927417.\n",
      "Epoch 1195/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0532\n",
      "Epoch 01195: val_loss did not improve from 0.02542\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0532 - val_loss: 0.0320\n",
      "\n",
      "Epoch 01196: LearningRateScheduler reducing learning rate to 0.00028967205181631544.\n",
      "Epoch 1196/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0294\n",
      "Epoch 01196: val_loss did not improve from 0.02542\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0294 - val_loss: 0.0307\n",
      "\n",
      "Epoch 01197: LearningRateScheduler reducing learning rate to 0.00028965491991709565.\n",
      "Epoch 1197/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0350\n",
      "Epoch 01197: val_loss did not improve from 0.02542\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0350 - val_loss: 0.0282\n",
      "\n",
      "Epoch 01198: LearningRateScheduler reducing learning rate to 0.0002896377743333119.\n",
      "Epoch 1198/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0328\n",
      "Epoch 01198: val_loss did not improve from 0.02542\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0328 - val_loss: 0.0264\n",
      "\n",
      "Epoch 01199: LearningRateScheduler reducing learning rate to 0.0002896206150666624.\n",
      "Epoch 1199/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0343\n",
      "Epoch 01199: val_loss did not improve from 0.02542\n",
      "3/3 [==============================] - 1s 228ms/step - loss: 0.0343 - val_loss: 0.0266\n",
      "\n",
      "Epoch 01200: LearningRateScheduler reducing learning rate to 0.00028960344211884683.\n",
      "Epoch 1200/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0330\n",
      "Epoch 01200: val_loss did not improve from 0.02542\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0330 - val_loss: 0.0301\n",
      "\n",
      "Epoch 01201: LearningRateScheduler reducing learning rate to 0.00028958625549156626.\n",
      "Epoch 1201/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0397\n",
      "Epoch 01201: val_loss did not improve from 0.02542\n",
      "3/3 [==============================] - 1s 240ms/step - loss: 0.0397 - val_loss: 0.0311\n",
      "\n",
      "Epoch 01202: LearningRateScheduler reducing learning rate to 0.00028956905518652293.\n",
      "Epoch 1202/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0277\n",
      "Epoch 01202: val_loss did not improve from 0.02542\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0277 - val_loss: 0.0290\n",
      "\n",
      "Epoch 01203: LearningRateScheduler reducing learning rate to 0.00028955184120542073.\n",
      "Epoch 1203/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0523\n",
      "Epoch 01203: val_loss did not improve from 0.02542\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0523 - val_loss: 0.0278\n",
      "\n",
      "Epoch 01204: LearningRateScheduler reducing learning rate to 0.00028953461354996467.\n",
      "Epoch 1204/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0322\n",
      "Epoch 01204: val_loss did not improve from 0.02542\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0322 - val_loss: 0.0299\n",
      "\n",
      "Epoch 01205: LearningRateScheduler reducing learning rate to 0.0002895173722218612.\n",
      "Epoch 1205/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0494\n",
      "Epoch 01205: val_loss did not improve from 0.02542\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0494 - val_loss: 0.0328\n",
      "\n",
      "Epoch 01206: LearningRateScheduler reducing learning rate to 0.0002895001172228181.\n",
      "Epoch 1206/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0345\n",
      "Epoch 01206: val_loss did not improve from 0.02542\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0345 - val_loss: 0.0352\n",
      "\n",
      "Epoch 01207: LearningRateScheduler reducing learning rate to 0.0002894828485545445.\n",
      "Epoch 1207/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0459\n",
      "Epoch 01207: val_loss did not improve from 0.02542\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0459 - val_loss: 0.0352\n",
      "\n",
      "Epoch 01208: LearningRateScheduler reducing learning rate to 0.00028946556621875096.\n",
      "Epoch 1208/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0358\n",
      "Epoch 01208: val_loss did not improve from 0.02542\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0358 - val_loss: 0.0306\n",
      "\n",
      "Epoch 01209: LearningRateScheduler reducing learning rate to 0.0002894482702171493.\n",
      "Epoch 1209/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0265\n",
      "Epoch 01209: val_loss did not improve from 0.02542\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0265 - val_loss: 0.0278\n",
      "\n",
      "Epoch 01210: LearningRateScheduler reducing learning rate to 0.00028943096055145275.\n",
      "Epoch 1210/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0379\n",
      "Epoch 01210: val_loss did not improve from 0.02542\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0379 - val_loss: 0.0268\n",
      "\n",
      "Epoch 01211: LearningRateScheduler reducing learning rate to 0.0002894136372233758.\n",
      "Epoch 1211/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0289\n",
      "Epoch 01211: val_loss did not improve from 0.02542\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0289 - val_loss: 0.0268\n",
      "\n",
      "Epoch 01212: LearningRateScheduler reducing learning rate to 0.0002893963002346345.\n",
      "Epoch 1212/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0341\n",
      "Epoch 01212: val_loss did not improve from 0.02542\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0341 - val_loss: 0.0287\n",
      "\n",
      "Epoch 01213: LearningRateScheduler reducing learning rate to 0.000289378949586946.\n",
      "Epoch 1213/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0467\n",
      "Epoch 01213: val_loss did not improve from 0.02542\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0467 - val_loss: 0.0287\n",
      "\n",
      "Epoch 01214: LearningRateScheduler reducing learning rate to 0.000289361585282029.\n",
      "Epoch 1214/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0343\n",
      "Epoch 01214: val_loss did not improve from 0.02542\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0343 - val_loss: 0.0267\n",
      "\n",
      "Epoch 01215: LearningRateScheduler reducing learning rate to 0.0002893442073216034.\n",
      "Epoch 1215/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0497\n",
      "Epoch 01215: val_loss did not improve from 0.02542\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0497 - val_loss: 0.0270\n",
      "\n",
      "Epoch 01216: LearningRateScheduler reducing learning rate to 0.00028932681570739054.\n",
      "Epoch 1216/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0392\n",
      "Epoch 01216: val_loss did not improve from 0.02542\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0392 - val_loss: 0.0285\n",
      "\n",
      "Epoch 01217: LearningRateScheduler reducing learning rate to 0.00028930941044111314.\n",
      "Epoch 1217/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0212\n",
      "Epoch 01217: val_loss did not improve from 0.02542\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0212 - val_loss: 0.0294\n",
      "\n",
      "Epoch 01218: LearningRateScheduler reducing learning rate to 0.0002892919915244952.\n",
      "Epoch 1218/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0333\n",
      "Epoch 01218: val_loss did not improve from 0.02542\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0333 - val_loss: 0.0302\n",
      "\n",
      "Epoch 01219: LearningRateScheduler reducing learning rate to 0.0002892745589592622.\n",
      "Epoch 1219/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0362\n",
      "Epoch 01219: val_loss did not improve from 0.02542\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0362 - val_loss: 0.0285\n",
      "\n",
      "Epoch 01220: LearningRateScheduler reducing learning rate to 0.0002892571127471407.\n",
      "Epoch 1220/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0413\n",
      "Epoch 01220: val_loss did not improve from 0.02542\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0413 - val_loss: 0.0280\n",
      "\n",
      "Epoch 01221: LearningRateScheduler reducing learning rate to 0.00028923965288985893.\n",
      "Epoch 1221/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0484\n",
      "Epoch 01221: val_loss did not improve from 0.02542\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0484 - val_loss: 0.0288\n",
      "\n",
      "Epoch 01222: LearningRateScheduler reducing learning rate to 0.0002892221793891463.\n",
      "Epoch 1222/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0309\n",
      "Epoch 01222: val_loss did not improve from 0.02542\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0309 - val_loss: 0.0308\n",
      "\n",
      "Epoch 01223: LearningRateScheduler reducing learning rate to 0.0002892046922467335.\n",
      "Epoch 1223/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0436\n",
      "Epoch 01223: val_loss did not improve from 0.02542\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0436 - val_loss: 0.0325\n",
      "\n",
      "Epoch 01224: LearningRateScheduler reducing learning rate to 0.00028918719146435283.\n",
      "Epoch 1224/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0322\n",
      "Epoch 01224: val_loss did not improve from 0.02542\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0322 - val_loss: 0.0293\n",
      "\n",
      "Epoch 01225: LearningRateScheduler reducing learning rate to 0.0002891696770437377.\n",
      "Epoch 1225/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0295\n",
      "Epoch 01225: val_loss improved from 0.02542 to 0.02346, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 11s 4s/step - loss: 0.0295 - val_loss: 0.0235\n",
      "\n",
      "Epoch 01226: LearningRateScheduler reducing learning rate to 0.000289152148986623.\n",
      "Epoch 1226/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0296\n",
      "Epoch 01226: val_loss did not improve from 0.02346\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0296 - val_loss: 0.0248\n",
      "\n",
      "Epoch 01227: LearningRateScheduler reducing learning rate to 0.0002891346072947448.\n",
      "Epoch 1227/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0384\n",
      "Epoch 01227: val_loss did not improve from 0.02346\n",
      "3/3 [==============================] - 1s 237ms/step - loss: 0.0384 - val_loss: 0.0278\n",
      "\n",
      "Epoch 01228: LearningRateScheduler reducing learning rate to 0.00028911705196984073.\n",
      "Epoch 1228/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0430\n",
      "Epoch 01228: val_loss did not improve from 0.02346\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0430 - val_loss: 0.0286\n",
      "\n",
      "Epoch 01229: LearningRateScheduler reducing learning rate to 0.00028909948301364977.\n",
      "Epoch 1229/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0415\n",
      "Epoch 01229: val_loss did not improve from 0.02346\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0415 - val_loss: 0.0288\n",
      "\n",
      "Epoch 01230: LearningRateScheduler reducing learning rate to 0.00028908190042791203.\n",
      "Epoch 1230/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0407\n",
      "Epoch 01230: val_loss did not improve from 0.02346\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0407 - val_loss: 0.0292\n",
      "\n",
      "Epoch 01231: LearningRateScheduler reducing learning rate to 0.0002890643042143692.\n",
      "Epoch 1231/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0412\n",
      "Epoch 01231: val_loss did not improve from 0.02346\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0412 - val_loss: 0.0279\n",
      "\n",
      "Epoch 01232: LearningRateScheduler reducing learning rate to 0.0002890466943747641.\n",
      "Epoch 1232/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0328\n",
      "Epoch 01232: val_loss did not improve from 0.02346\n",
      "3/3 [==============================] - 1s 228ms/step - loss: 0.0328 - val_loss: 0.0275\n",
      "\n",
      "Epoch 01233: LearningRateScheduler reducing learning rate to 0.0002890290709108412.\n",
      "Epoch 1233/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0352\n",
      "Epoch 01233: val_loss did not improve from 0.02346\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0352 - val_loss: 0.0297\n",
      "\n",
      "Epoch 01234: LearningRateScheduler reducing learning rate to 0.000289011433824346.\n",
      "Epoch 1234/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0335\n",
      "Epoch 01234: val_loss did not improve from 0.02346\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0335 - val_loss: 0.0314\n",
      "\n",
      "Epoch 01235: LearningRateScheduler reducing learning rate to 0.0002889937831170255.\n",
      "Epoch 1235/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0389\n",
      "Epoch 01235: val_loss did not improve from 0.02346\n",
      "3/3 [==============================] - 1s 237ms/step - loss: 0.0389 - val_loss: 0.0310\n",
      "\n",
      "Epoch 01236: LearningRateScheduler reducing learning rate to 0.00028897611879062825.\n",
      "Epoch 1236/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0393\n",
      "Epoch 01236: val_loss did not improve from 0.02346\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0393 - val_loss: 0.0285\n",
      "\n",
      "Epoch 01237: LearningRateScheduler reducing learning rate to 0.00028895844084690365.\n",
      "Epoch 1237/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0402\n",
      "Epoch 01237: val_loss did not improve from 0.02346\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0402 - val_loss: 0.0251\n",
      "\n",
      "Epoch 01238: LearningRateScheduler reducing learning rate to 0.00028894074928760295.\n",
      "Epoch 1238/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0357\n",
      "Epoch 01238: val_loss did not improve from 0.02346\n",
      "3/3 [==============================] - 1s 239ms/step - loss: 0.0357 - val_loss: 0.0258\n",
      "\n",
      "Epoch 01239: LearningRateScheduler reducing learning rate to 0.00028892304411447836.\n",
      "Epoch 1239/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0480\n",
      "Epoch 01239: val_loss did not improve from 0.02346\n",
      "3/3 [==============================] - 1s 227ms/step - loss: 0.0480 - val_loss: 0.0279\n",
      "\n",
      "Epoch 01240: LearningRateScheduler reducing learning rate to 0.0002889053253292838.\n",
      "Epoch 1240/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0417\n",
      "Epoch 01240: val_loss did not improve from 0.02346\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0417 - val_loss: 0.0298\n",
      "\n",
      "Epoch 01241: LearningRateScheduler reducing learning rate to 0.0002888875929337743.\n",
      "Epoch 1241/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0358\n",
      "Epoch 01241: val_loss did not improve from 0.02346\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0358 - val_loss: 0.0283\n",
      "\n",
      "Epoch 01242: LearningRateScheduler reducing learning rate to 0.0002888698469297062.\n",
      "Epoch 1242/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0507\n",
      "Epoch 01242: val_loss did not improve from 0.02346\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0507 - val_loss: 0.0259\n",
      "\n",
      "Epoch 01243: LearningRateScheduler reducing learning rate to 0.0002888520873188374.\n",
      "Epoch 1243/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0334\n",
      "Epoch 01243: val_loss did not improve from 0.02346\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0334 - val_loss: 0.0255\n",
      "\n",
      "Epoch 01244: LearningRateScheduler reducing learning rate to 0.000288834314102927.\n",
      "Epoch 1244/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0420\n",
      "Epoch 01244: val_loss did not improve from 0.02346\n",
      "3/3 [==============================] - 1s 237ms/step - loss: 0.0420 - val_loss: 0.0275\n",
      "\n",
      "Epoch 01245: LearningRateScheduler reducing learning rate to 0.00028881652728373545.\n",
      "Epoch 1245/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0301\n",
      "Epoch 01245: val_loss did not improve from 0.02346\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0301 - val_loss: 0.0276\n",
      "\n",
      "Epoch 01246: LearningRateScheduler reducing learning rate to 0.00028879872686302457.\n",
      "Epoch 1246/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0442\n",
      "Epoch 01246: val_loss did not improve from 0.02346\n",
      "3/3 [==============================] - 1s 237ms/step - loss: 0.0442 - val_loss: 0.0265\n",
      "\n",
      "Epoch 01247: LearningRateScheduler reducing learning rate to 0.00028878091284255755.\n",
      "Epoch 1247/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0457\n",
      "Epoch 01247: val_loss did not improve from 0.02346\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0457 - val_loss: 0.0257\n",
      "\n",
      "Epoch 01248: LearningRateScheduler reducing learning rate to 0.000288763085224099.\n",
      "Epoch 1248/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0387\n",
      "Epoch 01248: val_loss did not improve from 0.02346\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0387 - val_loss: 0.0252\n",
      "\n",
      "Epoch 01249: LearningRateScheduler reducing learning rate to 0.00028874524400941464.\n",
      "Epoch 1249/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0375\n",
      "Epoch 01249: val_loss did not improve from 0.02346\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0375 - val_loss: 0.0279\n",
      "\n",
      "Epoch 01250: LearningRateScheduler reducing learning rate to 0.0002887273892002717.\n",
      "Epoch 1250/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0346\n",
      "Epoch 01250: val_loss did not improve from 0.02346\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0346 - val_loss: 0.0251\n",
      "\n",
      "Epoch 01251: LearningRateScheduler reducing learning rate to 0.00028870952079843883.\n",
      "Epoch 1251/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0472\n",
      "Epoch 01251: val_loss did not improve from 0.02346\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0472 - val_loss: 0.0235\n",
      "\n",
      "Epoch 01252: LearningRateScheduler reducing learning rate to 0.0002886916388056859.\n",
      "Epoch 1252/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0401\n",
      "Epoch 01252: val_loss did not improve from 0.02346\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0401 - val_loss: 0.0246\n",
      "\n",
      "Epoch 01253: LearningRateScheduler reducing learning rate to 0.00028867374322378414.\n",
      "Epoch 1253/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0291\n",
      "Epoch 01253: val_loss did not improve from 0.02346\n",
      "3/3 [==============================] - 1s 239ms/step - loss: 0.0291 - val_loss: 0.0251\n",
      "\n",
      "Epoch 01254: LearningRateScheduler reducing learning rate to 0.0002886558340545062.\n",
      "Epoch 1254/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0322\n",
      "Epoch 01254: val_loss did not improve from 0.02346\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0322 - val_loss: 0.0256\n",
      "\n",
      "Epoch 01255: LearningRateScheduler reducing learning rate to 0.00028863791129962594.\n",
      "Epoch 1255/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0281\n",
      "Epoch 01255: val_loss did not improve from 0.02346\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0281 - val_loss: 0.0265\n",
      "\n",
      "Epoch 01256: LearningRateScheduler reducing learning rate to 0.0002886199749609187.\n",
      "Epoch 1256/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0444\n",
      "Epoch 01256: val_loss did not improve from 0.02346\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0444 - val_loss: 0.0257\n",
      "\n",
      "Epoch 01257: LearningRateScheduler reducing learning rate to 0.00028860202504016113.\n",
      "Epoch 1257/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0492\n",
      "Epoch 01257: val_loss did not improve from 0.02346\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0492 - val_loss: 0.0240\n",
      "\n",
      "Epoch 01258: LearningRateScheduler reducing learning rate to 0.00028858406153913115.\n",
      "Epoch 1258/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0274\n",
      "Epoch 01258: val_loss did not improve from 0.02346\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0274 - val_loss: 0.0245\n",
      "\n",
      "Epoch 01259: LearningRateScheduler reducing learning rate to 0.0002885660844596082.\n",
      "Epoch 1259/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0441\n",
      "Epoch 01259: val_loss did not improve from 0.02346\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0441 - val_loss: 0.0241\n",
      "\n",
      "Epoch 01260: LearningRateScheduler reducing learning rate to 0.00028854809380337285.\n",
      "Epoch 1260/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0263\n",
      "Epoch 01260: val_loss did not improve from 0.02346\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0263 - val_loss: 0.0252\n",
      "\n",
      "Epoch 01261: LearningRateScheduler reducing learning rate to 0.00028853008957220713.\n",
      "Epoch 1261/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0557\n",
      "Epoch 01261: val_loss did not improve from 0.02346\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0557 - val_loss: 0.0258\n",
      "\n",
      "Epoch 01262: LearningRateScheduler reducing learning rate to 0.00028851207176789446.\n",
      "Epoch 1262/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0353\n",
      "Epoch 01262: val_loss improved from 0.02346 to 0.02182, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.0353 - val_loss: 0.0218\n",
      "\n",
      "Epoch 01263: LearningRateScheduler reducing learning rate to 0.00028849404039221944.\n",
      "Epoch 1263/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0325\n",
      "Epoch 01263: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0325 - val_loss: 0.0242\n",
      "\n",
      "Epoch 01264: LearningRateScheduler reducing learning rate to 0.00028847599544696824.\n",
      "Epoch 1264/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0300\n",
      "Epoch 01264: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0300 - val_loss: 0.0256\n",
      "\n",
      "Epoch 01265: LearningRateScheduler reducing learning rate to 0.00028845793693392817.\n",
      "Epoch 1265/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0255\n",
      "Epoch 01265: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 227ms/step - loss: 0.0255 - val_loss: 0.0245\n",
      "\n",
      "Epoch 01266: LearningRateScheduler reducing learning rate to 0.000288439864854888.\n",
      "Epoch 1266/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0346\n",
      "Epoch 01266: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0346 - val_loss: 0.0245\n",
      "\n",
      "Epoch 01267: LearningRateScheduler reducing learning rate to 0.0002884217792116378.\n",
      "Epoch 1267/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0394\n",
      "Epoch 01267: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0394 - val_loss: 0.0242\n",
      "\n",
      "Epoch 01268: LearningRateScheduler reducing learning rate to 0.000288403680005969.\n",
      "Epoch 1268/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0292\n",
      "Epoch 01268: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 237ms/step - loss: 0.0292 - val_loss: 0.0256\n",
      "\n",
      "Epoch 01269: LearningRateScheduler reducing learning rate to 0.0002883855672396744.\n",
      "Epoch 1269/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0251\n",
      "Epoch 01269: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0251 - val_loss: 0.0274\n",
      "\n",
      "Epoch 01270: LearningRateScheduler reducing learning rate to 0.000288367440914548.\n",
      "Epoch 1270/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0337\n",
      "Epoch 01270: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0337 - val_loss: 0.0278\n",
      "\n",
      "Epoch 01271: LearningRateScheduler reducing learning rate to 0.0002883493010323854.\n",
      "Epoch 1271/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0312\n",
      "Epoch 01271: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0312 - val_loss: 0.0251\n",
      "\n",
      "Epoch 01272: LearningRateScheduler reducing learning rate to 0.0002883311475949833.\n",
      "Epoch 1272/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0415\n",
      "Epoch 01272: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0415 - val_loss: 0.0240\n",
      "\n",
      "Epoch 01273: LearningRateScheduler reducing learning rate to 0.00028831298060413987.\n",
      "Epoch 1273/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0276\n",
      "Epoch 01273: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 228ms/step - loss: 0.0276 - val_loss: 0.0252\n",
      "\n",
      "Epoch 01274: LearningRateScheduler reducing learning rate to 0.00028829480006165465.\n",
      "Epoch 1274/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0448\n",
      "Epoch 01274: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0448 - val_loss: 0.0257\n",
      "\n",
      "Epoch 01275: LearningRateScheduler reducing learning rate to 0.00028827660596932833.\n",
      "Epoch 1275/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0533\n",
      "Epoch 01275: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 227ms/step - loss: 0.0533 - val_loss: 0.0296\n",
      "\n",
      "Epoch 01276: LearningRateScheduler reducing learning rate to 0.00028825839832896324.\n",
      "Epoch 1276/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0372\n",
      "Epoch 01276: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0372 - val_loss: 0.0293\n",
      "\n",
      "Epoch 01277: LearningRateScheduler reducing learning rate to 0.00028824017714236273.\n",
      "Epoch 1277/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0334\n",
      "Epoch 01277: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0334 - val_loss: 0.0274\n",
      "\n",
      "Epoch 01278: LearningRateScheduler reducing learning rate to 0.00028822194241133177.\n",
      "Epoch 1278/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0265\n",
      "Epoch 01278: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0265 - val_loss: 0.0256\n",
      "\n",
      "Epoch 01279: LearningRateScheduler reducing learning rate to 0.00028820369413767643.\n",
      "Epoch 1279/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0468\n",
      "Epoch 01279: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 228ms/step - loss: 0.0468 - val_loss: 0.0244\n",
      "\n",
      "Epoch 01280: LearningRateScheduler reducing learning rate to 0.0002881854323232044.\n",
      "Epoch 1280/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0434\n",
      "Epoch 01280: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0434 - val_loss: 0.0241\n",
      "\n",
      "Epoch 01281: LearningRateScheduler reducing learning rate to 0.00028816715696972447.\n",
      "Epoch 1281/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0359\n",
      "Epoch 01281: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0359 - val_loss: 0.0247\n",
      "\n",
      "Epoch 01282: LearningRateScheduler reducing learning rate to 0.0002881488680790468.\n",
      "Epoch 1282/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0377\n",
      "Epoch 01282: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0377 - val_loss: 0.0263\n",
      "\n",
      "Epoch 01283: LearningRateScheduler reducing learning rate to 0.00028813056565298305.\n",
      "Epoch 1283/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0363\n",
      "Epoch 01283: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0363 - val_loss: 0.0271\n",
      "\n",
      "Epoch 01284: LearningRateScheduler reducing learning rate to 0.0002881122496933461.\n",
      "Epoch 1284/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0271\n",
      "Epoch 01284: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 237ms/step - loss: 0.0271 - val_loss: 0.0315\n",
      "\n",
      "Epoch 01285: LearningRateScheduler reducing learning rate to 0.00028809392020195013.\n",
      "Epoch 1285/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0414\n",
      "Epoch 01285: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0414 - val_loss: 0.0339\n",
      "\n",
      "Epoch 01286: LearningRateScheduler reducing learning rate to 0.0002880755771806108.\n",
      "Epoch 1286/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0371\n",
      "Epoch 01286: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 237ms/step - loss: 0.0371 - val_loss: 0.0357\n",
      "\n",
      "Epoch 01287: LearningRateScheduler reducing learning rate to 0.0002880572206311449.\n",
      "Epoch 1287/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0526\n",
      "Epoch 01287: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0526 - val_loss: 0.0355\n",
      "\n",
      "Epoch 01288: LearningRateScheduler reducing learning rate to 0.00028803885055537083.\n",
      "Epoch 1288/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0488\n",
      "Epoch 01288: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0488 - val_loss: 0.0282\n",
      "\n",
      "Epoch 01289: LearningRateScheduler reducing learning rate to 0.0002880204669551081.\n",
      "Epoch 1289/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0238\n",
      "Epoch 01289: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0238 - val_loss: 0.0301\n",
      "\n",
      "Epoch 01290: LearningRateScheduler reducing learning rate to 0.0002880020698321777.\n",
      "Epoch 1290/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0325\n",
      "Epoch 01290: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0325 - val_loss: 0.0307\n",
      "\n",
      "Epoch 01291: LearningRateScheduler reducing learning rate to 0.00028798365918840185.\n",
      "Epoch 1291/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0288\n",
      "Epoch 01291: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0288 - val_loss: 0.0295\n",
      "\n",
      "Epoch 01292: LearningRateScheduler reducing learning rate to 0.00028796523502560423.\n",
      "Epoch 1292/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0466\n",
      "Epoch 01292: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0466 - val_loss: 0.0286\n",
      "\n",
      "Epoch 01293: LearningRateScheduler reducing learning rate to 0.00028794679734560976.\n",
      "Epoch 1293/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0533\n",
      "Epoch 01293: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0533 - val_loss: 0.0288\n",
      "\n",
      "Epoch 01294: LearningRateScheduler reducing learning rate to 0.0002879283461502447.\n",
      "Epoch 1294/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0311\n",
      "Epoch 01294: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0311 - val_loss: 0.0319\n",
      "\n",
      "Epoch 01295: LearningRateScheduler reducing learning rate to 0.0002879098814413368.\n",
      "Epoch 1295/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0505\n",
      "Epoch 01295: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0505 - val_loss: 0.0340\n",
      "\n",
      "Epoch 01296: LearningRateScheduler reducing learning rate to 0.0002878914032207149.\n",
      "Epoch 1296/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0375\n",
      "Epoch 01296: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0375 - val_loss: 0.0380\n",
      "\n",
      "Epoch 01297: LearningRateScheduler reducing learning rate to 0.00028787291149020937.\n",
      "Epoch 1297/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0399\n",
      "Epoch 01297: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0399 - val_loss: 0.0404\n",
      "\n",
      "Epoch 01298: LearningRateScheduler reducing learning rate to 0.0002878544062516519.\n",
      "Epoch 1298/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0408\n",
      "Epoch 01298: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0408 - val_loss: 0.0378\n",
      "\n",
      "Epoch 01299: LearningRateScheduler reducing learning rate to 0.0002878358875068754.\n",
      "Epoch 1299/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0476\n",
      "Epoch 01299: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0476 - val_loss: 0.0346\n",
      "\n",
      "Epoch 01300: LearningRateScheduler reducing learning rate to 0.0002878173552577142.\n",
      "Epoch 1300/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0476\n",
      "Epoch 01300: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0476 - val_loss: 0.0333\n",
      "\n",
      "Epoch 01301: LearningRateScheduler reducing learning rate to 0.00028779880950600403.\n",
      "Epoch 1301/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0374\n",
      "Epoch 01301: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0374 - val_loss: 0.0325\n",
      "\n",
      "Epoch 01302: LearningRateScheduler reducing learning rate to 0.0002877802502535818.\n",
      "Epoch 1302/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0380\n",
      "Epoch 01302: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 225ms/step - loss: 0.0380 - val_loss: 0.0323\n",
      "\n",
      "Epoch 01303: LearningRateScheduler reducing learning rate to 0.00028776167750228603.\n",
      "Epoch 1303/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0446\n",
      "Epoch 01303: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0446 - val_loss: 0.0306\n",
      "\n",
      "Epoch 01304: LearningRateScheduler reducing learning rate to 0.00028774309125395613.\n",
      "Epoch 1304/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0402\n",
      "Epoch 01304: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0402 - val_loss: 0.0315\n",
      "\n",
      "Epoch 01305: LearningRateScheduler reducing learning rate to 0.00028772449151043326.\n",
      "Epoch 1305/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0388\n",
      "Epoch 01305: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0388 - val_loss: 0.0307\n",
      "\n",
      "Epoch 01306: LearningRateScheduler reducing learning rate to 0.00028770587827355975.\n",
      "Epoch 1306/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0270\n",
      "Epoch 01306: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0270 - val_loss: 0.0299\n",
      "\n",
      "Epoch 01307: LearningRateScheduler reducing learning rate to 0.0002876872515451793.\n",
      "Epoch 1307/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0401\n",
      "Epoch 01307: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 237ms/step - loss: 0.0401 - val_loss: 0.0305\n",
      "\n",
      "Epoch 01308: LearningRateScheduler reducing learning rate to 0.0002876686113271369.\n",
      "Epoch 1308/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0346\n",
      "Epoch 01308: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0346 - val_loss: 0.0305\n",
      "\n",
      "Epoch 01309: LearningRateScheduler reducing learning rate to 0.0002876499576212789.\n",
      "Epoch 1309/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0382\n",
      "Epoch 01309: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 237ms/step - loss: 0.0382 - val_loss: 0.0275\n",
      "\n",
      "Epoch 01310: LearningRateScheduler reducing learning rate to 0.00028763129042945305.\n",
      "Epoch 1310/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0407\n",
      "Epoch 01310: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 228ms/step - loss: 0.0407 - val_loss: 0.0257\n",
      "\n",
      "Epoch 01311: LearningRateScheduler reducing learning rate to 0.00028761260975350834.\n",
      "Epoch 1311/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0440\n",
      "Epoch 01311: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0440 - val_loss: 0.0295\n",
      "\n",
      "Epoch 01312: LearningRateScheduler reducing learning rate to 0.0002875939155952951.\n",
      "Epoch 1312/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0489\n",
      "Epoch 01312: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0489 - val_loss: 0.0312\n",
      "\n",
      "Epoch 01313: LearningRateScheduler reducing learning rate to 0.0002875752079566651.\n",
      "Epoch 1313/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0294\n",
      "Epoch 01313: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0294 - val_loss: 0.0315\n",
      "\n",
      "Epoch 01314: LearningRateScheduler reducing learning rate to 0.0002875564868394713.\n",
      "Epoch 1314/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0350\n",
      "Epoch 01314: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 239ms/step - loss: 0.0350 - val_loss: 0.0268\n",
      "\n",
      "Epoch 01315: LearningRateScheduler reducing learning rate to 0.0002875377522455681.\n",
      "Epoch 1315/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0278\n",
      "Epoch 01315: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0278 - val_loss: 0.0271\n",
      "\n",
      "Epoch 01316: LearningRateScheduler reducing learning rate to 0.0002875190041768113.\n",
      "Epoch 1316/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0338\n",
      "Epoch 01316: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0338 - val_loss: 0.0248\n",
      "\n",
      "Epoch 01317: LearningRateScheduler reducing learning rate to 0.00028750024263505777.\n",
      "Epoch 1317/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0459\n",
      "Epoch 01317: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0459 - val_loss: 0.0317\n",
      "\n",
      "Epoch 01318: LearningRateScheduler reducing learning rate to 0.000287481467622166.\n",
      "Epoch 1318/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0247\n",
      "Epoch 01318: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0247 - val_loss: 0.0315\n",
      "\n",
      "Epoch 01319: LearningRateScheduler reducing learning rate to 0.00028746267913999564.\n",
      "Epoch 1319/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0363\n",
      "Epoch 01319: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 238ms/step - loss: 0.0363 - val_loss: 0.0329\n",
      "\n",
      "Epoch 01320: LearningRateScheduler reducing learning rate to 0.0002874438771904078.\n",
      "Epoch 1320/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0390\n",
      "Epoch 01320: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0390 - val_loss: 0.0351\n",
      "\n",
      "Epoch 01321: LearningRateScheduler reducing learning rate to 0.00028742506177526476.\n",
      "Epoch 1321/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0444\n",
      "Epoch 01321: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0444 - val_loss: 0.0373\n",
      "\n",
      "Epoch 01322: LearningRateScheduler reducing learning rate to 0.0002874062328964303.\n",
      "Epoch 1322/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0377\n",
      "Epoch 01322: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0377 - val_loss: 0.0459\n",
      "\n",
      "Epoch 01323: LearningRateScheduler reducing learning rate to 0.0002873873905557695.\n",
      "Epoch 1323/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0264\n",
      "Epoch 01323: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0264 - val_loss: 0.0556\n",
      "\n",
      "Epoch 01324: LearningRateScheduler reducing learning rate to 0.0002873685347551486.\n",
      "Epoch 1324/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0357\n",
      "Epoch 01324: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0357 - val_loss: 0.0628\n",
      "\n",
      "Epoch 01325: LearningRateScheduler reducing learning rate to 0.00028734966549643547.\n",
      "Epoch 1325/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0481\n",
      "Epoch 01325: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0481 - val_loss: 0.0678\n",
      "\n",
      "Epoch 01326: LearningRateScheduler reducing learning rate to 0.00028733078278149905.\n",
      "Epoch 1326/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0329\n",
      "Epoch 01326: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0329 - val_loss: 0.0687\n",
      "\n",
      "Epoch 01327: LearningRateScheduler reducing learning rate to 0.00028731188661220976.\n",
      "Epoch 1327/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0299\n",
      "Epoch 01327: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0299 - val_loss: 0.0661\n",
      "\n",
      "Epoch 01328: LearningRateScheduler reducing learning rate to 0.0002872929769904393.\n",
      "Epoch 1328/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0361\n",
      "Epoch 01328: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0361 - val_loss: 0.0628\n",
      "\n",
      "Epoch 01329: LearningRateScheduler reducing learning rate to 0.00028727405391806073.\n",
      "Epoch 1329/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0308\n",
      "Epoch 01329: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 238ms/step - loss: 0.0308 - val_loss: 0.0609\n",
      "\n",
      "Epoch 01330: LearningRateScheduler reducing learning rate to 0.0002872551173969483.\n",
      "Epoch 1330/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0380\n",
      "Epoch 01330: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0380 - val_loss: 0.0566\n",
      "\n",
      "Epoch 01331: LearningRateScheduler reducing learning rate to 0.0002872361674289779.\n",
      "Epoch 1331/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0338\n",
      "Epoch 01331: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0338 - val_loss: 0.0547\n",
      "\n",
      "Epoch 01332: LearningRateScheduler reducing learning rate to 0.0002872172040160265.\n",
      "Epoch 1332/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0384\n",
      "Epoch 01332: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0384 - val_loss: 0.0559\n",
      "\n",
      "Epoch 01333: LearningRateScheduler reducing learning rate to 0.00028719822715997243.\n",
      "Epoch 1333/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0316\n",
      "Epoch 01333: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0316 - val_loss: 0.0574\n",
      "\n",
      "Epoch 01334: LearningRateScheduler reducing learning rate to 0.0002871792368626954.\n",
      "Epoch 1334/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0408\n",
      "Epoch 01334: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 238ms/step - loss: 0.0408 - val_loss: 0.0536\n",
      "\n",
      "Epoch 01335: LearningRateScheduler reducing learning rate to 0.0002871602331260765.\n",
      "Epoch 1335/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0392\n",
      "Epoch 01335: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0392 - val_loss: 0.0519\n",
      "\n",
      "Epoch 01336: LearningRateScheduler reducing learning rate to 0.000287141215951998.\n",
      "Epoch 1336/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0485\n",
      "Epoch 01336: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 238ms/step - loss: 0.0485 - val_loss: 0.0479\n",
      "\n",
      "Epoch 01337: LearningRateScheduler reducing learning rate to 0.0002871221853423437.\n",
      "Epoch 1337/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0324\n",
      "Epoch 01337: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0324 - val_loss: 0.0469\n",
      "\n",
      "Epoch 01338: LearningRateScheduler reducing learning rate to 0.0002871031412989986.\n",
      "Epoch 1338/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0349\n",
      "Epoch 01338: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0349 - val_loss: 0.0475\n",
      "\n",
      "Epoch 01339: LearningRateScheduler reducing learning rate to 0.00028708408382384896.\n",
      "Epoch 1339/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0332\n",
      "Epoch 01339: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0332 - val_loss: 0.0474\n",
      "\n",
      "Epoch 01340: LearningRateScheduler reducing learning rate to 0.00028706501291878256.\n",
      "Epoch 1340/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0362\n",
      "Epoch 01340: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0362 - val_loss: 0.0434\n",
      "\n",
      "Epoch 01341: LearningRateScheduler reducing learning rate to 0.0002870459285856884.\n",
      "Epoch 1341/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0574\n",
      "Epoch 01341: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0574 - val_loss: 0.0499\n",
      "\n",
      "Epoch 01342: LearningRateScheduler reducing learning rate to 0.00028702683082645685.\n",
      "Epoch 1342/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0520\n",
      "Epoch 01342: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0520 - val_loss: 0.0782\n",
      "\n",
      "Epoch 01343: LearningRateScheduler reducing learning rate to 0.0002870077196429795.\n",
      "Epoch 1343/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0394\n",
      "Epoch 01343: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0394 - val_loss: 0.2661\n",
      "\n",
      "Epoch 01344: LearningRateScheduler reducing learning rate to 0.0002869885950371495.\n",
      "Epoch 1344/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0365\n",
      "Epoch 01344: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 243ms/step - loss: 0.0365 - val_loss: 0.5373\n",
      "\n",
      "Epoch 01345: LearningRateScheduler reducing learning rate to 0.000286969457010861.\n",
      "Epoch 1345/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0467\n",
      "Epoch 01345: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0467 - val_loss: 0.6814\n",
      "\n",
      "Epoch 01346: LearningRateScheduler reducing learning rate to 0.00028695030556600984.\n",
      "Epoch 1346/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0336\n",
      "Epoch 01346: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0336 - val_loss: 0.7826\n",
      "\n",
      "Epoch 01347: LearningRateScheduler reducing learning rate to 0.00028693114070449296.\n",
      "Epoch 1347/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0337\n",
      "Epoch 01347: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0337 - val_loss: 0.6405\n",
      "\n",
      "Epoch 01348: LearningRateScheduler reducing learning rate to 0.0002869119624282086.\n",
      "Epoch 1348/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0323\n",
      "Epoch 01348: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0323 - val_loss: 0.2822\n",
      "\n",
      "Epoch 01349: LearningRateScheduler reducing learning rate to 0.0002868927707390565.\n",
      "Epoch 1349/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0269\n",
      "Epoch 01349: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0269 - val_loss: 0.0988\n",
      "\n",
      "Epoch 01350: LearningRateScheduler reducing learning rate to 0.0002868735656389377.\n",
      "Epoch 1350/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0268\n",
      "Epoch 01350: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0268 - val_loss: 0.0715\n",
      "\n",
      "Epoch 01351: LearningRateScheduler reducing learning rate to 0.0002868543471297543.\n",
      "Epoch 1351/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0303\n",
      "Epoch 01351: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0303 - val_loss: 0.0906\n",
      "\n",
      "Epoch 01352: LearningRateScheduler reducing learning rate to 0.0002868351152134102.\n",
      "Epoch 1352/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0347\n",
      "Epoch 01352: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0347 - val_loss: 0.0677\n",
      "\n",
      "Epoch 01353: LearningRateScheduler reducing learning rate to 0.00028681586989181017.\n",
      "Epoch 1353/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0387\n",
      "Epoch 01353: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0387 - val_loss: 0.0435\n",
      "\n",
      "Epoch 01354: LearningRateScheduler reducing learning rate to 0.0002867966111668606.\n",
      "Epoch 1354/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0310\n",
      "Epoch 01354: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0310 - val_loss: 0.0309\n",
      "\n",
      "Epoch 01355: LearningRateScheduler reducing learning rate to 0.00028677733904046903.\n",
      "Epoch 1355/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0370\n",
      "Epoch 01355: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0370 - val_loss: 0.0294\n",
      "\n",
      "Epoch 01356: LearningRateScheduler reducing learning rate to 0.0002867580535145445.\n",
      "Epoch 1356/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0413\n",
      "Epoch 01356: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0413 - val_loss: 0.0291\n",
      "\n",
      "Epoch 01357: LearningRateScheduler reducing learning rate to 0.0002867387545909972.\n",
      "Epoch 1357/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0246\n",
      "Epoch 01357: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0246 - val_loss: 0.0288\n",
      "\n",
      "Epoch 01358: LearningRateScheduler reducing learning rate to 0.0002867194422717388.\n",
      "Epoch 1358/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0272\n",
      "Epoch 01358: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 241ms/step - loss: 0.0272 - val_loss: 0.0291\n",
      "\n",
      "Epoch 01359: LearningRateScheduler reducing learning rate to 0.0002867001165586822.\n",
      "Epoch 1359/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0335\n",
      "Epoch 01359: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 239ms/step - loss: 0.0335 - val_loss: 0.0291\n",
      "\n",
      "Epoch 01360: LearningRateScheduler reducing learning rate to 0.0002866807774537417.\n",
      "Epoch 1360/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0374\n",
      "Epoch 01360: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 237ms/step - loss: 0.0374 - val_loss: 0.0293\n",
      "\n",
      "Epoch 01361: LearningRateScheduler reducing learning rate to 0.00028666142495883284.\n",
      "Epoch 1361/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0227\n",
      "Epoch 01361: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0227 - val_loss: 0.0325\n",
      "\n",
      "Epoch 01362: LearningRateScheduler reducing learning rate to 0.0002866420590758725.\n",
      "Epoch 1362/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0449\n",
      "Epoch 01362: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0449 - val_loss: 0.0341\n",
      "\n",
      "Epoch 01363: LearningRateScheduler reducing learning rate to 0.00028662267980677895.\n",
      "Epoch 1363/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0463\n",
      "Epoch 01363: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0463 - val_loss: 0.0363\n",
      "\n",
      "Epoch 01364: LearningRateScheduler reducing learning rate to 0.0002866032871534718.\n",
      "Epoch 1364/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0407\n",
      "Epoch 01364: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0407 - val_loss: 0.0345\n",
      "\n",
      "Epoch 01365: LearningRateScheduler reducing learning rate to 0.00028658388111787184.\n",
      "Epoch 1365/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0359\n",
      "Epoch 01365: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0359 - val_loss: 0.0332\n",
      "\n",
      "Epoch 01366: LearningRateScheduler reducing learning rate to 0.0002865644617019014.\n",
      "Epoch 1366/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0300\n",
      "Epoch 01366: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0300 - val_loss: 0.0328\n",
      "\n",
      "Epoch 01367: LearningRateScheduler reducing learning rate to 0.00028654502890748386.\n",
      "Epoch 1367/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0397\n",
      "Epoch 01367: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0397 - val_loss: 0.0331\n",
      "\n",
      "Epoch 01368: LearningRateScheduler reducing learning rate to 0.0002865255827365442.\n",
      "Epoch 1368/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0293\n",
      "Epoch 01368: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0293 - val_loss: 0.0337\n",
      "\n",
      "Epoch 01369: LearningRateScheduler reducing learning rate to 0.0002865061231910087.\n",
      "Epoch 1369/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0462\n",
      "Epoch 01369: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 239ms/step - loss: 0.0462 - val_loss: 0.0343\n",
      "\n",
      "Epoch 01370: LearningRateScheduler reducing learning rate to 0.00028648665027280454.\n",
      "Epoch 1370/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0420\n",
      "Epoch 01370: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0420 - val_loss: 0.0335\n",
      "\n",
      "Epoch 01371: LearningRateScheduler reducing learning rate to 0.00028646716398386093.\n",
      "Epoch 1371/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0278\n",
      "Epoch 01371: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0278 - val_loss: 0.0349\n",
      "\n",
      "Epoch 01372: LearningRateScheduler reducing learning rate to 0.0002864476643261078.\n",
      "Epoch 1372/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0298\n",
      "Epoch 01372: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 237ms/step - loss: 0.0298 - val_loss: 0.0369\n",
      "\n",
      "Epoch 01373: LearningRateScheduler reducing learning rate to 0.00028642815130147675.\n",
      "Epoch 1373/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0262\n",
      "Epoch 01373: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 239ms/step - loss: 0.0262 - val_loss: 0.0394\n",
      "\n",
      "Epoch 01374: LearningRateScheduler reducing learning rate to 0.0002864086249119006.\n",
      "Epoch 1374/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0336\n",
      "Epoch 01374: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 239ms/step - loss: 0.0336 - val_loss: 0.0400\n",
      "\n",
      "Epoch 01375: LearningRateScheduler reducing learning rate to 0.00028638908515931337.\n",
      "Epoch 1375/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0335\n",
      "Epoch 01375: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0335 - val_loss: 0.0392\n",
      "\n",
      "Epoch 01376: LearningRateScheduler reducing learning rate to 0.00028636953204565056.\n",
      "Epoch 1376/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0506\n",
      "Epoch 01376: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0506 - val_loss: 0.0364\n",
      "\n",
      "Epoch 01377: LearningRateScheduler reducing learning rate to 0.00028634996557284903.\n",
      "Epoch 1377/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0474\n",
      "Epoch 01377: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0474 - val_loss: 0.0313\n",
      "\n",
      "Epoch 01378: LearningRateScheduler reducing learning rate to 0.0002863303857428468.\n",
      "Epoch 1378/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0338\n",
      "Epoch 01378: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0338 - val_loss: 0.0303\n",
      "\n",
      "Epoch 01379: LearningRateScheduler reducing learning rate to 0.0002863107925575833.\n",
      "Epoch 1379/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0310\n",
      "Epoch 01379: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0310 - val_loss: 0.0310\n",
      "\n",
      "Epoch 01380: LearningRateScheduler reducing learning rate to 0.00028629118601899934.\n",
      "Epoch 1380/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0414\n",
      "Epoch 01380: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0414 - val_loss: 0.0302\n",
      "\n",
      "Epoch 01381: LearningRateScheduler reducing learning rate to 0.00028627156612903693.\n",
      "Epoch 1381/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0525\n",
      "Epoch 01381: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0525 - val_loss: 0.0273\n",
      "\n",
      "Epoch 01382: LearningRateScheduler reducing learning rate to 0.0002862519328896395.\n",
      "Epoch 1382/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0314\n",
      "Epoch 01382: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 226ms/step - loss: 0.0314 - val_loss: 0.0245\n",
      "\n",
      "Epoch 01383: LearningRateScheduler reducing learning rate to 0.00028623228630275183.\n",
      "Epoch 1383/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0336\n",
      "Epoch 01383: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0336 - val_loss: 0.0248\n",
      "\n",
      "Epoch 01384: LearningRateScheduler reducing learning rate to 0.0002862126263703198.\n",
      "Epoch 1384/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0420\n",
      "Epoch 01384: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0420 - val_loss: 0.0260\n",
      "\n",
      "Epoch 01385: LearningRateScheduler reducing learning rate to 0.0002861929530942909.\n",
      "Epoch 1385/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0246\n",
      "Epoch 01385: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0246 - val_loss: 0.0291\n",
      "\n",
      "Epoch 01386: LearningRateScheduler reducing learning rate to 0.0002861732664766138.\n",
      "Epoch 1386/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0374\n",
      "Epoch 01386: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 227ms/step - loss: 0.0374 - val_loss: 0.0299\n",
      "\n",
      "Epoch 01387: LearningRateScheduler reducing learning rate to 0.00028615356651923846.\n",
      "Epoch 1387/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0357\n",
      "Epoch 01387: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0357 - val_loss: 0.0287\n",
      "\n",
      "Epoch 01388: LearningRateScheduler reducing learning rate to 0.00028613385322411624.\n",
      "Epoch 1388/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0352\n",
      "Epoch 01388: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 237ms/step - loss: 0.0352 - val_loss: 0.0295\n",
      "\n",
      "Epoch 01389: LearningRateScheduler reducing learning rate to 0.0002861141265931997.\n",
      "Epoch 1389/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0352\n",
      "Epoch 01389: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0352 - val_loss: 0.0307\n",
      "\n",
      "Epoch 01390: LearningRateScheduler reducing learning rate to 0.00028609438662844294.\n",
      "Epoch 1390/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0381\n",
      "Epoch 01390: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0381 - val_loss: 0.0295\n",
      "\n",
      "Epoch 01391: LearningRateScheduler reducing learning rate to 0.0002860746333318012.\n",
      "Epoch 1391/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0263\n",
      "Epoch 01391: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0263 - val_loss: 0.0304\n",
      "\n",
      "Epoch 01392: LearningRateScheduler reducing learning rate to 0.0002860548667052311.\n",
      "Epoch 1392/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0348\n",
      "Epoch 01392: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0348 - val_loss: 0.0301\n",
      "\n",
      "Epoch 01393: LearningRateScheduler reducing learning rate to 0.00028603508675069053.\n",
      "Epoch 1393/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0287\n",
      "Epoch 01393: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0287 - val_loss: 0.0255\n",
      "\n",
      "Epoch 01394: LearningRateScheduler reducing learning rate to 0.0002860152934701388.\n",
      "Epoch 1394/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0287\n",
      "Epoch 01394: val_loss did not improve from 0.02182\n",
      "3/3 [==============================] - 1s 239ms/step - loss: 0.0287 - val_loss: 0.0221\n",
      "\n",
      "Epoch 01395: LearningRateScheduler reducing learning rate to 0.0002859954868655364.\n",
      "Epoch 1395/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0443\n",
      "Epoch 01395: val_loss improved from 0.02182 to 0.02033, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.0443 - val_loss: 0.0203\n",
      "\n",
      "Epoch 01396: LearningRateScheduler reducing learning rate to 0.00028597566693884524.\n",
      "Epoch 1396/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0425\n",
      "Epoch 01396: val_loss improved from 0.02033 to 0.01964, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.0425 - val_loss: 0.0196\n",
      "\n",
      "Epoch 01397: LearningRateScheduler reducing learning rate to 0.00028595583369202856.\n",
      "Epoch 1397/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0232\n",
      "Epoch 01397: val_loss improved from 0.01964 to 0.01884, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.0232 - val_loss: 0.0188\n",
      "\n",
      "Epoch 01398: LearningRateScheduler reducing learning rate to 0.0002859359871270509.\n",
      "Epoch 1398/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0327\n",
      "Epoch 01398: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0327 - val_loss: 0.0189\n",
      "\n",
      "Epoch 01399: LearningRateScheduler reducing learning rate to 0.0002859161272458781.\n",
      "Epoch 1399/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0407\n",
      "Epoch 01399: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0407 - val_loss: 0.0202\n",
      "\n",
      "Epoch 01400: LearningRateScheduler reducing learning rate to 0.0002858962540504773.\n",
      "Epoch 1400/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0305\n",
      "Epoch 01400: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0305 - val_loss: 0.0224\n",
      "\n",
      "Epoch 01401: LearningRateScheduler reducing learning rate to 0.00028587636754281705.\n",
      "Epoch 1401/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0354\n",
      "Epoch 01401: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 240ms/step - loss: 0.0354 - val_loss: 0.0264\n",
      "\n",
      "Epoch 01402: LearningRateScheduler reducing learning rate to 0.00028585646772486705.\n",
      "Epoch 1402/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0270\n",
      "Epoch 01402: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 226ms/step - loss: 0.0270 - val_loss: 0.0322\n",
      "\n",
      "Epoch 01403: LearningRateScheduler reducing learning rate to 0.0002858365545985985.\n",
      "Epoch 1403/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0300\n",
      "Epoch 01403: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0300 - val_loss: 0.0364\n",
      "\n",
      "Epoch 01404: LearningRateScheduler reducing learning rate to 0.00028581662816598384.\n",
      "Epoch 1404/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0391\n",
      "Epoch 01404: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0391 - val_loss: 0.0376\n",
      "\n",
      "Epoch 01405: LearningRateScheduler reducing learning rate to 0.0002857966884289968.\n",
      "Epoch 1405/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0277\n",
      "Epoch 01405: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0277 - val_loss: 0.0363\n",
      "\n",
      "Epoch 01406: LearningRateScheduler reducing learning rate to 0.00028577673538961244.\n",
      "Epoch 1406/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0263\n",
      "Epoch 01406: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0263 - val_loss: 0.0343\n",
      "\n",
      "Epoch 01407: LearningRateScheduler reducing learning rate to 0.00028575676904980715.\n",
      "Epoch 1407/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0296\n",
      "Epoch 01407: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0296 - val_loss: 0.0330\n",
      "\n",
      "Epoch 01408: LearningRateScheduler reducing learning rate to 0.0002857367894115588.\n",
      "Epoch 1408/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0261\n",
      "Epoch 01408: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0261 - val_loss: 0.0286\n",
      "\n",
      "Epoch 01409: LearningRateScheduler reducing learning rate to 0.00028571679647684616.\n",
      "Epoch 1409/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0546\n",
      "Epoch 01409: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0546 - val_loss: 0.0263\n",
      "\n",
      "Epoch 01410: LearningRateScheduler reducing learning rate to 0.00028569679024764976.\n",
      "Epoch 1410/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0234\n",
      "Epoch 01410: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0234 - val_loss: 0.0261\n",
      "\n",
      "Epoch 01411: LearningRateScheduler reducing learning rate to 0.0002856767707259513.\n",
      "Epoch 1411/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0433\n",
      "Epoch 01411: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0433 - val_loss: 0.0226\n",
      "\n",
      "Epoch 01412: LearningRateScheduler reducing learning rate to 0.00028565673791373355.\n",
      "Epoch 1412/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0360\n",
      "Epoch 01412: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0360 - val_loss: 0.0225\n",
      "\n",
      "Epoch 01413: LearningRateScheduler reducing learning rate to 0.00028563669181298095.\n",
      "Epoch 1413/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0282\n",
      "Epoch 01413: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0282 - val_loss: 0.0253\n",
      "\n",
      "Epoch 01414: LearningRateScheduler reducing learning rate to 0.0002856166324256791.\n",
      "Epoch 1414/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0348\n",
      "Epoch 01414: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0348 - val_loss: 0.0284\n",
      "\n",
      "Epoch 01415: LearningRateScheduler reducing learning rate to 0.00028559655975381496.\n",
      "Epoch 1415/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0406\n",
      "Epoch 01415: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0406 - val_loss: 0.0297\n",
      "\n",
      "Epoch 01416: LearningRateScheduler reducing learning rate to 0.0002855764737993767.\n",
      "Epoch 1416/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0350\n",
      "Epoch 01416: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0350 - val_loss: 0.0288\n",
      "\n",
      "Epoch 01417: LearningRateScheduler reducing learning rate to 0.00028555637456435394.\n",
      "Epoch 1417/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0418\n",
      "Epoch 01417: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 226ms/step - loss: 0.0418 - val_loss: 0.0286\n",
      "\n",
      "Epoch 01418: LearningRateScheduler reducing learning rate to 0.00028553626205073747.\n",
      "Epoch 1418/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0273\n",
      "Epoch 01418: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 237ms/step - loss: 0.0273 - val_loss: 0.0275\n",
      "\n",
      "Epoch 01419: LearningRateScheduler reducing learning rate to 0.0002855161362605196.\n",
      "Epoch 1419/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0333\n",
      "Epoch 01419: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0333 - val_loss: 0.0274\n",
      "\n",
      "Epoch 01420: LearningRateScheduler reducing learning rate to 0.0002854959971956938.\n",
      "Epoch 1420/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0348\n",
      "Epoch 01420: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0348 - val_loss: 0.0278\n",
      "\n",
      "Epoch 01421: LearningRateScheduler reducing learning rate to 0.0002854758448582548.\n",
      "Epoch 1421/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0366\n",
      "Epoch 01421: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0366 - val_loss: 0.0282\n",
      "\n",
      "Epoch 01422: LearningRateScheduler reducing learning rate to 0.00028545567925019884.\n",
      "Epoch 1422/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0338\n",
      "Epoch 01422: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0338 - val_loss: 0.0254\n",
      "\n",
      "Epoch 01423: LearningRateScheduler reducing learning rate to 0.00028543550037352333.\n",
      "Epoch 1423/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0383\n",
      "Epoch 01423: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 240ms/step - loss: 0.0383 - val_loss: 0.0252\n",
      "\n",
      "Epoch 01424: LearningRateScheduler reducing learning rate to 0.000285415308230227.\n",
      "Epoch 1424/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0267\n",
      "Epoch 01424: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0267 - val_loss: 0.0263\n",
      "\n",
      "Epoch 01425: LearningRateScheduler reducing learning rate to 0.00028539510282231005.\n",
      "Epoch 1425/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0293\n",
      "Epoch 01425: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0293 - val_loss: 0.0288\n",
      "\n",
      "Epoch 01426: LearningRateScheduler reducing learning rate to 0.00028537488415177374.\n",
      "Epoch 1426/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0295\n",
      "Epoch 01426: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0295 - val_loss: 0.0293\n",
      "\n",
      "Epoch 01427: LearningRateScheduler reducing learning rate to 0.0002853546522206208.\n",
      "Epoch 1427/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0185\n",
      "Epoch 01427: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0185 - val_loss: 0.0302\n",
      "\n",
      "Epoch 01428: LearningRateScheduler reducing learning rate to 0.0002853344070308553.\n",
      "Epoch 1428/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0290\n",
      "Epoch 01428: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0290 - val_loss: 0.0284\n",
      "\n",
      "Epoch 01429: LearningRateScheduler reducing learning rate to 0.0002853141485844826.\n",
      "Epoch 1429/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0258\n",
      "Epoch 01429: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 237ms/step - loss: 0.0258 - val_loss: 0.0265\n",
      "\n",
      "Epoch 01430: LearningRateScheduler reducing learning rate to 0.0002852938768835092.\n",
      "Epoch 1430/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0395\n",
      "Epoch 01430: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 240ms/step - loss: 0.0395 - val_loss: 0.0256\n",
      "\n",
      "Epoch 01431: LearningRateScheduler reducing learning rate to 0.0002852735919299432.\n",
      "Epoch 1431/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0214\n",
      "Epoch 01431: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0214 - val_loss: 0.0266\n",
      "\n",
      "Epoch 01432: LearningRateScheduler reducing learning rate to 0.0002852532937257938.\n",
      "Epoch 1432/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0314\n",
      "Epoch 01432: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 228ms/step - loss: 0.0314 - val_loss: 0.0232\n",
      "\n",
      "Epoch 01433: LearningRateScheduler reducing learning rate to 0.00028523298227307165.\n",
      "Epoch 1433/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0246\n",
      "Epoch 01433: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0246 - val_loss: 0.0220\n",
      "\n",
      "Epoch 01434: LearningRateScheduler reducing learning rate to 0.0002852126575737886.\n",
      "Epoch 1434/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0353\n",
      "Epoch 01434: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 238ms/step - loss: 0.0353 - val_loss: 0.0241\n",
      "\n",
      "Epoch 01435: LearningRateScheduler reducing learning rate to 0.00028519231962995784.\n",
      "Epoch 1435/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0416\n",
      "Epoch 01435: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0416 - val_loss: 0.0268\n",
      "\n",
      "Epoch 01436: LearningRateScheduler reducing learning rate to 0.00028517196844359395.\n",
      "Epoch 1436/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0351\n",
      "Epoch 01436: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0351 - val_loss: 0.0266\n",
      "\n",
      "Epoch 01437: LearningRateScheduler reducing learning rate to 0.0002851516040167127.\n",
      "Epoch 1437/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0267\n",
      "Epoch 01437: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0267 - val_loss: 0.0251\n",
      "\n",
      "Epoch 01438: LearningRateScheduler reducing learning rate to 0.0002851312263513312.\n",
      "Epoch 1438/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0368\n",
      "Epoch 01438: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0368 - val_loss: 0.0246\n",
      "\n",
      "Epoch 01439: LearningRateScheduler reducing learning rate to 0.00028511083544946806.\n",
      "Epoch 1439/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0507\n",
      "Epoch 01439: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0507 - val_loss: 0.0237\n",
      "\n",
      "Epoch 01440: LearningRateScheduler reducing learning rate to 0.0002850904313131429.\n",
      "Epoch 1440/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0278\n",
      "Epoch 01440: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0278 - val_loss: 0.0252\n",
      "\n",
      "Epoch 01441: LearningRateScheduler reducing learning rate to 0.00028507001394437686.\n",
      "Epoch 1441/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0270\n",
      "Epoch 01441: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0270 - val_loss: 0.0265\n",
      "\n",
      "Epoch 01442: LearningRateScheduler reducing learning rate to 0.0002850495833451923.\n",
      "Epoch 1442/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0230\n",
      "Epoch 01442: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0230 - val_loss: 0.0291\n",
      "\n",
      "Epoch 01443: LearningRateScheduler reducing learning rate to 0.00028502913951761297.\n",
      "Epoch 1443/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0330\n",
      "Epoch 01443: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 239ms/step - loss: 0.0330 - val_loss: 0.0326\n",
      "\n",
      "Epoch 01444: LearningRateScheduler reducing learning rate to 0.0002850086824636638.\n",
      "Epoch 1444/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0329\n",
      "Epoch 01444: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0329 - val_loss: 0.0315\n",
      "\n",
      "Epoch 01445: LearningRateScheduler reducing learning rate to 0.0002849882121853711.\n",
      "Epoch 1445/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0294\n",
      "Epoch 01445: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0294 - val_loss: 0.0298\n",
      "\n",
      "Epoch 01446: LearningRateScheduler reducing learning rate to 0.0002849677286847627.\n",
      "Epoch 1446/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0369\n",
      "Epoch 01446: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0369 - val_loss: 0.0272\n",
      "\n",
      "Epoch 01447: LearningRateScheduler reducing learning rate to 0.00028494723196386724.\n",
      "Epoch 1447/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0241\n",
      "Epoch 01447: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0241 - val_loss: 0.0247\n",
      "\n",
      "Epoch 01448: LearningRateScheduler reducing learning rate to 0.00028492672202471515.\n",
      "Epoch 1448/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0287\n",
      "Epoch 01448: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0287 - val_loss: 0.0225\n",
      "\n",
      "Epoch 01449: LearningRateScheduler reducing learning rate to 0.000284906198869338.\n",
      "Epoch 1449/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0381\n",
      "Epoch 01449: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0381 - val_loss: 0.0249\n",
      "\n",
      "Epoch 01450: LearningRateScheduler reducing learning rate to 0.00028488566249976855.\n",
      "Epoch 1450/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0166\n",
      "Epoch 01450: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0166 - val_loss: 0.0276\n",
      "\n",
      "Epoch 01451: LearningRateScheduler reducing learning rate to 0.00028486511291804105.\n",
      "Epoch 1451/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0250\n",
      "Epoch 01451: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0250 - val_loss: 0.0286\n",
      "\n",
      "Epoch 01452: LearningRateScheduler reducing learning rate to 0.000284844550126191.\n",
      "Epoch 1452/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0306\n",
      "Epoch 01452: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0306 - val_loss: 0.0291\n",
      "\n",
      "Epoch 01453: LearningRateScheduler reducing learning rate to 0.0002848239741262551.\n",
      "Epoch 1453/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0355\n",
      "Epoch 01453: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0355 - val_loss: 0.0290\n",
      "\n",
      "Epoch 01454: LearningRateScheduler reducing learning rate to 0.0002848033849202715.\n",
      "Epoch 1454/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0245\n",
      "Epoch 01454: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 228ms/step - loss: 0.0245 - val_loss: 0.0288\n",
      "\n",
      "Epoch 01455: LearningRateScheduler reducing learning rate to 0.0002847827825102797.\n",
      "Epoch 1455/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0253\n",
      "Epoch 01455: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0253 - val_loss: 0.0291\n",
      "\n",
      "Epoch 01456: LearningRateScheduler reducing learning rate to 0.00028476216689832023.\n",
      "Epoch 1456/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0297\n",
      "Epoch 01456: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0297 - val_loss: 0.0297\n",
      "\n",
      "Epoch 01457: LearningRateScheduler reducing learning rate to 0.0002847415380864353.\n",
      "Epoch 1457/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0233\n",
      "Epoch 01457: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0233 - val_loss: 0.0303\n",
      "\n",
      "Epoch 01458: LearningRateScheduler reducing learning rate to 0.0002847208960766681.\n",
      "Epoch 1458/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0411\n",
      "Epoch 01458: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 245ms/step - loss: 0.0411 - val_loss: 0.0323\n",
      "\n",
      "Epoch 01459: LearningRateScheduler reducing learning rate to 0.0002847002408710633.\n",
      "Epoch 1459/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0254\n",
      "Epoch 01459: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0254 - val_loss: 0.0314\n",
      "\n",
      "Epoch 01460: LearningRateScheduler reducing learning rate to 0.00028467957247166694.\n",
      "Epoch 1460/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0277\n",
      "Epoch 01460: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0277 - val_loss: 0.0292\n",
      "\n",
      "Epoch 01461: LearningRateScheduler reducing learning rate to 0.0002846588908805262.\n",
      "Epoch 1461/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0318\n",
      "Epoch 01461: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 228ms/step - loss: 0.0318 - val_loss: 0.0281\n",
      "\n",
      "Epoch 01462: LearningRateScheduler reducing learning rate to 0.0002846381960996896.\n",
      "Epoch 1462/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0466\n",
      "Epoch 01462: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0466 - val_loss: 0.0261\n",
      "\n",
      "Epoch 01463: LearningRateScheduler reducing learning rate to 0.0002846174881312071.\n",
      "Epoch 1463/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0276\n",
      "Epoch 01463: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 237ms/step - loss: 0.0276 - val_loss: 0.0259\n",
      "\n",
      "Epoch 01464: LearningRateScheduler reducing learning rate to 0.0002845967669771298.\n",
      "Epoch 1464/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0349\n",
      "Epoch 01464: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0349 - val_loss: 0.0268\n",
      "\n",
      "Epoch 01465: LearningRateScheduler reducing learning rate to 0.00028457603263951016.\n",
      "Epoch 1465/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0494\n",
      "Epoch 01465: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0494 - val_loss: 0.0279\n",
      "\n",
      "Epoch 01466: LearningRateScheduler reducing learning rate to 0.00028455528512040204.\n",
      "Epoch 1466/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0380\n",
      "Epoch 01466: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0380 - val_loss: 0.0276\n",
      "\n",
      "Epoch 01467: LearningRateScheduler reducing learning rate to 0.00028453452442186045.\n",
      "Epoch 1467/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0236\n",
      "Epoch 01467: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0236 - val_loss: 0.0250\n",
      "\n",
      "Epoch 01468: LearningRateScheduler reducing learning rate to 0.00028451375054594193.\n",
      "Epoch 1468/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0238\n",
      "Epoch 01468: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0238 - val_loss: 0.0213\n",
      "\n",
      "Epoch 01469: LearningRateScheduler reducing learning rate to 0.000284492963494704.\n",
      "Epoch 1469/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0353\n",
      "Epoch 01469: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0353 - val_loss: 0.0221\n",
      "\n",
      "Epoch 01470: LearningRateScheduler reducing learning rate to 0.00028447216327020576.\n",
      "Epoch 1470/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0262\n",
      "Epoch 01470: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0262 - val_loss: 0.0273\n",
      "\n",
      "Epoch 01471: LearningRateScheduler reducing learning rate to 0.0002844513498745075.\n",
      "Epoch 1471/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0348\n",
      "Epoch 01471: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 240ms/step - loss: 0.0348 - val_loss: 0.0332\n",
      "\n",
      "Epoch 01472: LearningRateScheduler reducing learning rate to 0.00028443052330967084.\n",
      "Epoch 1472/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0302\n",
      "Epoch 01472: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 225ms/step - loss: 0.0302 - val_loss: 0.0384\n",
      "\n",
      "Epoch 01473: LearningRateScheduler reducing learning rate to 0.0002844096835777587.\n",
      "Epoch 1473/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0220\n",
      "Epoch 01473: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0220 - val_loss: 0.0388\n",
      "\n",
      "Epoch 01474: LearningRateScheduler reducing learning rate to 0.0002843888306808353.\n",
      "Epoch 1474/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0220\n",
      "Epoch 01474: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0220 - val_loss: 0.0356\n",
      "\n",
      "Epoch 01475: LearningRateScheduler reducing learning rate to 0.0002843679646209662.\n",
      "Epoch 1475/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0324\n",
      "Epoch 01475: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 237ms/step - loss: 0.0324 - val_loss: 0.0297\n",
      "\n",
      "Epoch 01476: LearningRateScheduler reducing learning rate to 0.00028434708540021815.\n",
      "Epoch 1476/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0199\n",
      "Epoch 01476: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0199 - val_loss: 0.0289\n",
      "\n",
      "Epoch 01477: LearningRateScheduler reducing learning rate to 0.00028432619302065935.\n",
      "Epoch 1477/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0257\n",
      "Epoch 01477: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0257 - val_loss: 0.0299\n",
      "\n",
      "Epoch 01478: LearningRateScheduler reducing learning rate to 0.00028430528748435923.\n",
      "Epoch 1478/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0264\n",
      "Epoch 01478: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 237ms/step - loss: 0.0264 - val_loss: 0.0303\n",
      "\n",
      "Epoch 01479: LearningRateScheduler reducing learning rate to 0.0002842843687933886.\n",
      "Epoch 1479/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0308\n",
      "Epoch 01479: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0308 - val_loss: 0.0295\n",
      "\n",
      "Epoch 01480: LearningRateScheduler reducing learning rate to 0.0002842634369498193.\n",
      "Epoch 1480/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0365\n",
      "Epoch 01480: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 239ms/step - loss: 0.0365 - val_loss: 0.0274\n",
      "\n",
      "Epoch 01481: LearningRateScheduler reducing learning rate to 0.0002842424919557249.\n",
      "Epoch 1481/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0298\n",
      "Epoch 01481: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0298 - val_loss: 0.0258\n",
      "\n",
      "Epoch 01482: LearningRateScheduler reducing learning rate to 0.00028422153381317994.\n",
      "Epoch 1482/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0302\n",
      "Epoch 01482: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0302 - val_loss: 0.0267\n",
      "\n",
      "Epoch 01483: LearningRateScheduler reducing learning rate to 0.0002842005625242604.\n",
      "Epoch 1483/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0201\n",
      "Epoch 01483: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0201 - val_loss: 0.0262\n",
      "\n",
      "Epoch 01484: LearningRateScheduler reducing learning rate to 0.0002841795780910435.\n",
      "Epoch 1484/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0234\n",
      "Epoch 01484: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0234 - val_loss: 0.0262\n",
      "\n",
      "Epoch 01485: LearningRateScheduler reducing learning rate to 0.0002841585805156078.\n",
      "Epoch 1485/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0367\n",
      "Epoch 01485: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0367 - val_loss: 0.0237\n",
      "\n",
      "Epoch 01486: LearningRateScheduler reducing learning rate to 0.00028413756980003323.\n",
      "Epoch 1486/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0207\n",
      "Epoch 01486: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0207 - val_loss: 0.0233\n",
      "\n",
      "Epoch 01487: LearningRateScheduler reducing learning rate to 0.00028411654594640094.\n",
      "Epoch 1487/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0463\n",
      "Epoch 01487: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0463 - val_loss: 0.0252\n",
      "\n",
      "Epoch 01488: LearningRateScheduler reducing learning rate to 0.00028409550895679326.\n",
      "Epoch 1488/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0285\n",
      "Epoch 01488: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0285 - val_loss: 0.0247\n",
      "\n",
      "Epoch 01489: LearningRateScheduler reducing learning rate to 0.0002840744588332941.\n",
      "Epoch 1489/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0513\n",
      "Epoch 01489: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0513 - val_loss: 0.0249\n",
      "\n",
      "Epoch 01490: LearningRateScheduler reducing learning rate to 0.0002840533955779885.\n",
      "Epoch 1490/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0355\n",
      "Epoch 01490: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0355 - val_loss: 0.0253\n",
      "\n",
      "Epoch 01491: LearningRateScheduler reducing learning rate to 0.0002840323191929627.\n",
      "Epoch 1491/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0409\n",
      "Epoch 01491: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0409 - val_loss: 0.0308\n",
      "\n",
      "Epoch 01492: LearningRateScheduler reducing learning rate to 0.0002840112296803045.\n",
      "Epoch 1492/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0338\n",
      "Epoch 01492: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0338 - val_loss: 0.0358\n",
      "\n",
      "Epoch 01493: LearningRateScheduler reducing learning rate to 0.0002839901270421028.\n",
      "Epoch 1493/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0351\n",
      "Epoch 01493: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0351 - val_loss: 0.0371\n",
      "\n",
      "Epoch 01494: LearningRateScheduler reducing learning rate to 0.0002839690112804479.\n",
      "Epoch 1494/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0366\n",
      "Epoch 01494: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0366 - val_loss: 0.0350\n",
      "\n",
      "Epoch 01495: LearningRateScheduler reducing learning rate to 0.0002839478823974314.\n",
      "Epoch 1495/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0284\n",
      "Epoch 01495: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0284 - val_loss: 0.0307\n",
      "\n",
      "Epoch 01496: LearningRateScheduler reducing learning rate to 0.00028392674039514606.\n",
      "Epoch 1496/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0255\n",
      "Epoch 01496: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0255 - val_loss: 0.0274\n",
      "\n",
      "Epoch 01497: LearningRateScheduler reducing learning rate to 0.0002839055852756861.\n",
      "Epoch 1497/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0486\n",
      "Epoch 01497: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0486 - val_loss: 0.0268\n",
      "\n",
      "Epoch 01498: LearningRateScheduler reducing learning rate to 0.00028388441704114704.\n",
      "Epoch 1498/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0247\n",
      "Epoch 01498: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0247 - val_loss: 0.0279\n",
      "\n",
      "Epoch 01499: LearningRateScheduler reducing learning rate to 0.0002838632356936255.\n",
      "Epoch 1499/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0402\n",
      "Epoch 01499: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0402 - val_loss: 0.0286\n",
      "\n",
      "Epoch 01500: LearningRateScheduler reducing learning rate to 0.0002838420412352197.\n",
      "Epoch 1500/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0285\n",
      "Epoch 01500: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 237ms/step - loss: 0.0285 - val_loss: 0.0275\n",
      "\n",
      "Epoch 01501: LearningRateScheduler reducing learning rate to 0.0002838208336680289.\n",
      "Epoch 1501/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0307\n",
      "Epoch 01501: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0307 - val_loss: 0.0222\n",
      "\n",
      "Epoch 01502: LearningRateScheduler reducing learning rate to 0.0002837996129941538.\n",
      "Epoch 1502/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0222\n",
      "Epoch 01502: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0222 - val_loss: 0.0234\n",
      "\n",
      "Epoch 01503: LearningRateScheduler reducing learning rate to 0.0002837783792156963.\n",
      "Epoch 1503/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0386\n",
      "Epoch 01503: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0386 - val_loss: 0.0245\n",
      "\n",
      "Epoch 01504: LearningRateScheduler reducing learning rate to 0.0002837571323347598.\n",
      "Epoch 1504/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0264\n",
      "Epoch 01504: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 237ms/step - loss: 0.0264 - val_loss: 0.0265\n",
      "\n",
      "Epoch 01505: LearningRateScheduler reducing learning rate to 0.00028373587235344867.\n",
      "Epoch 1505/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0283\n",
      "Epoch 01505: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0283 - val_loss: 0.0284\n",
      "\n",
      "Epoch 01506: LearningRateScheduler reducing learning rate to 0.0002837145992738688.\n",
      "Epoch 1506/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0259\n",
      "Epoch 01506: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0259 - val_loss: 0.0286\n",
      "\n",
      "Epoch 01507: LearningRateScheduler reducing learning rate to 0.00028369331309812746.\n",
      "Epoch 1507/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0206\n",
      "Epoch 01507: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0206 - val_loss: 0.0284\n",
      "\n",
      "Epoch 01508: LearningRateScheduler reducing learning rate to 0.00028367201382833296.\n",
      "Epoch 1508/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0425\n",
      "Epoch 01508: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0425 - val_loss: 0.0287\n",
      "\n",
      "Epoch 01509: LearningRateScheduler reducing learning rate to 0.00028365070146659513.\n",
      "Epoch 1509/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0467\n",
      "Epoch 01509: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0467 - val_loss: 0.0301\n",
      "\n",
      "Epoch 01510: LearningRateScheduler reducing learning rate to 0.000283629376015025.\n",
      "Epoch 1510/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0343\n",
      "Epoch 01510: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0343 - val_loss: 0.0299\n",
      "\n",
      "Epoch 01511: LearningRateScheduler reducing learning rate to 0.00028360803747573484.\n",
      "Epoch 1511/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0326\n",
      "Epoch 01511: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0326 - val_loss: 0.0297\n",
      "\n",
      "Epoch 01512: LearningRateScheduler reducing learning rate to 0.0002835866858508383.\n",
      "Epoch 1512/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0399\n",
      "Epoch 01512: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 227ms/step - loss: 0.0399 - val_loss: 0.0298\n",
      "\n",
      "Epoch 01513: LearningRateScheduler reducing learning rate to 0.00028356532114245036.\n",
      "Epoch 1513/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0289\n",
      "Epoch 01513: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 239ms/step - loss: 0.0289 - val_loss: 0.0279\n",
      "\n",
      "Epoch 01514: LearningRateScheduler reducing learning rate to 0.00028354394335268723.\n",
      "Epoch 1514/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0243\n",
      "Epoch 01514: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0243 - val_loss: 0.0265\n",
      "\n",
      "Epoch 01515: LearningRateScheduler reducing learning rate to 0.0002835225524836664.\n",
      "Epoch 1515/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0407\n",
      "Epoch 01515: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0407 - val_loss: 0.0268\n",
      "\n",
      "Epoch 01516: LearningRateScheduler reducing learning rate to 0.0002835011485375067.\n",
      "Epoch 1516/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0250\n",
      "Epoch 01516: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0250 - val_loss: 0.0272\n",
      "\n",
      "Epoch 01517: LearningRateScheduler reducing learning rate to 0.0002834797315163282.\n",
      "Epoch 1517/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0213\n",
      "Epoch 01517: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 239ms/step - loss: 0.0213 - val_loss: 0.0282\n",
      "\n",
      "Epoch 01518: LearningRateScheduler reducing learning rate to 0.0002834583014222524.\n",
      "Epoch 1518/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0229\n",
      "Epoch 01518: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0229 - val_loss: 0.0285\n",
      "\n",
      "Epoch 01519: LearningRateScheduler reducing learning rate to 0.0002834368582574019.\n",
      "Epoch 1519/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0289\n",
      "Epoch 01519: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 226ms/step - loss: 0.0289 - val_loss: 0.0297\n",
      "\n",
      "Epoch 01520: LearningRateScheduler reducing learning rate to 0.00028341540202390074.\n",
      "Epoch 1520/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0237\n",
      "Epoch 01520: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0237 - val_loss: 0.0297\n",
      "\n",
      "Epoch 01521: LearningRateScheduler reducing learning rate to 0.0002833939327238742.\n",
      "Epoch 1521/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0252\n",
      "Epoch 01521: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 228ms/step - loss: 0.0252 - val_loss: 0.0289\n",
      "\n",
      "Epoch 01522: LearningRateScheduler reducing learning rate to 0.00028337245035944895.\n",
      "Epoch 1522/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0274\n",
      "Epoch 01522: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0274 - val_loss: 0.0256\n",
      "\n",
      "Epoch 01523: LearningRateScheduler reducing learning rate to 0.0002833509549327527.\n",
      "Epoch 1523/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0320\n",
      "Epoch 01523: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0320 - val_loss: 0.0245\n",
      "\n",
      "Epoch 01524: LearningRateScheduler reducing learning rate to 0.00028332944644591477.\n",
      "Epoch 1524/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0284\n",
      "Epoch 01524: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 238ms/step - loss: 0.0284 - val_loss: 0.0229\n",
      "\n",
      "Epoch 01525: LearningRateScheduler reducing learning rate to 0.0002833079249010655.\n",
      "Epoch 1525/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0239\n",
      "Epoch 01525: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 238ms/step - loss: 0.0239 - val_loss: 0.0236\n",
      "\n",
      "Epoch 01526: LearningRateScheduler reducing learning rate to 0.00028328639030033675.\n",
      "Epoch 1526/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0203\n",
      "Epoch 01526: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0203 - val_loss: 0.0250\n",
      "\n",
      "Epoch 01527: LearningRateScheduler reducing learning rate to 0.00028326484264586154.\n",
      "Epoch 1527/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0326\n",
      "Epoch 01527: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0326 - val_loss: 0.0267\n",
      "\n",
      "Epoch 01528: LearningRateScheduler reducing learning rate to 0.00028324328193977423.\n",
      "Epoch 1528/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0424\n",
      "Epoch 01528: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0424 - val_loss: 0.0295\n",
      "\n",
      "Epoch 01529: LearningRateScheduler reducing learning rate to 0.0002832217081842104.\n",
      "Epoch 1529/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0412\n",
      "Epoch 01529: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0412 - val_loss: 0.0295\n",
      "\n",
      "Epoch 01530: LearningRateScheduler reducing learning rate to 0.000283200121381307.\n",
      "Epoch 1530/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0310\n",
      "Epoch 01530: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0310 - val_loss: 0.0281\n",
      "\n",
      "Epoch 01531: LearningRateScheduler reducing learning rate to 0.00028317852153320234.\n",
      "Epoch 1531/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0261\n",
      "Epoch 01531: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 238ms/step - loss: 0.0261 - val_loss: 0.0256\n",
      "\n",
      "Epoch 01532: LearningRateScheduler reducing learning rate to 0.00028315690864203586.\n",
      "Epoch 1532/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0211\n",
      "Epoch 01532: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0211 - val_loss: 0.0230\n",
      "\n",
      "Epoch 01533: LearningRateScheduler reducing learning rate to 0.00028313528270994834.\n",
      "Epoch 1533/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0336\n",
      "Epoch 01533: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0336 - val_loss: 0.0215\n",
      "\n",
      "Epoch 01534: LearningRateScheduler reducing learning rate to 0.00028311364373908194.\n",
      "Epoch 1534/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0374\n",
      "Epoch 01534: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0374 - val_loss: 0.0203\n",
      "\n",
      "Epoch 01535: LearningRateScheduler reducing learning rate to 0.00028309199173158007.\n",
      "Epoch 1535/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0277\n",
      "Epoch 01535: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0277 - val_loss: 0.0236\n",
      "\n",
      "Epoch 01536: LearningRateScheduler reducing learning rate to 0.0002830703266895873.\n",
      "Epoch 1536/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0216\n",
      "Epoch 01536: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 242ms/step - loss: 0.0216 - val_loss: 0.0271\n",
      "\n",
      "Epoch 01537: LearningRateScheduler reducing learning rate to 0.00028304864861524966.\n",
      "Epoch 1537/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0328\n",
      "Epoch 01537: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 237ms/step - loss: 0.0328 - val_loss: 0.0270\n",
      "\n",
      "Epoch 01538: LearningRateScheduler reducing learning rate to 0.0002830269575107144.\n",
      "Epoch 1538/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0352\n",
      "Epoch 01538: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0352 - val_loss: 0.0257\n",
      "\n",
      "Epoch 01539: LearningRateScheduler reducing learning rate to 0.0002830052533781302.\n",
      "Epoch 1539/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0294\n",
      "Epoch 01539: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0294 - val_loss: 0.0250\n",
      "\n",
      "Epoch 01540: LearningRateScheduler reducing learning rate to 0.0002829835362196467.\n",
      "Epoch 1540/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0274\n",
      "Epoch 01540: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 241ms/step - loss: 0.0274 - val_loss: 0.0243\n",
      "\n",
      "Epoch 01541: LearningRateScheduler reducing learning rate to 0.0002829618060374151.\n",
      "Epoch 1541/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0308\n",
      "Epoch 01541: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0308 - val_loss: 0.0227\n",
      "\n",
      "Epoch 01542: LearningRateScheduler reducing learning rate to 0.00028294006283358797.\n",
      "Epoch 1542/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0237\n",
      "Epoch 01542: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0237 - val_loss: 0.0240\n",
      "\n",
      "Epoch 01543: LearningRateScheduler reducing learning rate to 0.00028291830661031884.\n",
      "Epoch 1543/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0303\n",
      "Epoch 01543: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0303 - val_loss: 0.0288\n",
      "\n",
      "Epoch 01544: LearningRateScheduler reducing learning rate to 0.0002828965373697629.\n",
      "Epoch 1544/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0344\n",
      "Epoch 01544: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0344 - val_loss: 0.0321\n",
      "\n",
      "Epoch 01545: LearningRateScheduler reducing learning rate to 0.00028287475511407616.\n",
      "Epoch 1545/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0319\n",
      "Epoch 01545: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0319 - val_loss: 0.0331\n",
      "\n",
      "Epoch 01546: LearningRateScheduler reducing learning rate to 0.00028285295984541647.\n",
      "Epoch 1546/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0335\n",
      "Epoch 01546: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0335 - val_loss: 0.0317\n",
      "\n",
      "Epoch 01547: LearningRateScheduler reducing learning rate to 0.00028283115156594257.\n",
      "Epoch 1547/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0344\n",
      "Epoch 01547: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 238ms/step - loss: 0.0344 - val_loss: 0.0291\n",
      "\n",
      "Epoch 01548: LearningRateScheduler reducing learning rate to 0.0002828093302778147.\n",
      "Epoch 1548/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0361\n",
      "Epoch 01548: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0361 - val_loss: 0.0274\n",
      "\n",
      "Epoch 01549: LearningRateScheduler reducing learning rate to 0.00028278749598319424.\n",
      "Epoch 1549/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0430\n",
      "Epoch 01549: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 237ms/step - loss: 0.0430 - val_loss: 0.0267\n",
      "\n",
      "Epoch 01550: LearningRateScheduler reducing learning rate to 0.00028276564868424394.\n",
      "Epoch 1550/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0333\n",
      "Epoch 01550: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0333 - val_loss: 0.0264\n",
      "\n",
      "Epoch 01551: LearningRateScheduler reducing learning rate to 0.0002827437883831279.\n",
      "Epoch 1551/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0264\n",
      "Epoch 01551: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 239ms/step - loss: 0.0264 - val_loss: 0.0268\n",
      "\n",
      "Epoch 01552: LearningRateScheduler reducing learning rate to 0.0002827219150820113.\n",
      "Epoch 1552/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0290\n",
      "Epoch 01552: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0290 - val_loss: 0.0284\n",
      "\n",
      "Epoch 01553: LearningRateScheduler reducing learning rate to 0.00028270002878306083.\n",
      "Epoch 1553/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0471\n",
      "Epoch 01553: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0471 - val_loss: 0.0306\n",
      "\n",
      "Epoch 01554: LearningRateScheduler reducing learning rate to 0.00028267812948844435.\n",
      "Epoch 1554/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0195\n",
      "Epoch 01554: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0195 - val_loss: 0.0300\n",
      "\n",
      "Epoch 01555: LearningRateScheduler reducing learning rate to 0.000282656217200331.\n",
      "Epoch 1555/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0264\n",
      "Epoch 01555: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0264 - val_loss: 0.0266\n",
      "\n",
      "Epoch 01556: LearningRateScheduler reducing learning rate to 0.0002826342919208914.\n",
      "Epoch 1556/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0373\n",
      "Epoch 01556: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0373 - val_loss: 0.0228\n",
      "\n",
      "Epoch 01557: LearningRateScheduler reducing learning rate to 0.00028261235365229713.\n",
      "Epoch 1557/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0352\n",
      "Epoch 01557: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 238ms/step - loss: 0.0352 - val_loss: 0.0201\n",
      "\n",
      "Epoch 01558: LearningRateScheduler reducing learning rate to 0.0002825904023967213.\n",
      "Epoch 1558/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0256\n",
      "Epoch 01558: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0256 - val_loss: 0.0194\n",
      "\n",
      "Epoch 01559: LearningRateScheduler reducing learning rate to 0.0002825684381563382.\n",
      "Epoch 1559/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0177\n",
      "Epoch 01559: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0177 - val_loss: 0.0206\n",
      "\n",
      "Epoch 01560: LearningRateScheduler reducing learning rate to 0.00028254646093332347.\n",
      "Epoch 1560/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0394\n",
      "Epoch 01560: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0394 - val_loss: 0.0214\n",
      "\n",
      "Epoch 01561: LearningRateScheduler reducing learning rate to 0.000282524470729854.\n",
      "Epoch 1561/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0313\n",
      "Epoch 01561: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0313 - val_loss: 0.0219\n",
      "\n",
      "Epoch 01562: LearningRateScheduler reducing learning rate to 0.0002825024675481079.\n",
      "Epoch 1562/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0315\n",
      "Epoch 01562: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0315 - val_loss: 0.0228\n",
      "\n",
      "Epoch 01563: LearningRateScheduler reducing learning rate to 0.0002824804513902648.\n",
      "Epoch 1563/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0354\n",
      "Epoch 01563: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0354 - val_loss: 0.0225\n",
      "\n",
      "Epoch 01564: LearningRateScheduler reducing learning rate to 0.00028245842225850526.\n",
      "Epoch 1564/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0372\n",
      "Epoch 01564: val_loss did not improve from 0.01884\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0372 - val_loss: 0.0207\n",
      "\n",
      "Epoch 01565: LearningRateScheduler reducing learning rate to 0.0002824363801550115.\n",
      "Epoch 1565/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0318\n",
      "Epoch 01565: val_loss improved from 0.01884 to 0.01846, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.0318 - val_loss: 0.0185\n",
      "\n",
      "Epoch 01566: LearningRateScheduler reducing learning rate to 0.00028241432508196665.\n",
      "Epoch 1566/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0327\n",
      "Epoch 01566: val_loss improved from 0.01846 to 0.01835, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.0327 - val_loss: 0.0183\n",
      "\n",
      "Epoch 01567: LearningRateScheduler reducing learning rate to 0.00028239225704155543.\n",
      "Epoch 1567/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0280\n",
      "Epoch 01567: val_loss improved from 0.01835 to 0.01785, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 10s 3s/step - loss: 0.0280 - val_loss: 0.0179\n",
      "\n",
      "Epoch 01568: LearningRateScheduler reducing learning rate to 0.0002823701760359637.\n",
      "Epoch 1568/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0296\n",
      "Epoch 01568: val_loss did not improve from 0.01785\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0296 - val_loss: 0.0181\n",
      "\n",
      "Epoch 01569: LearningRateScheduler reducing learning rate to 0.0002823480820673787.\n",
      "Epoch 1569/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0386\n",
      "Epoch 01569: val_loss did not improve from 0.01785\n",
      "3/3 [==============================] - 1s 237ms/step - loss: 0.0386 - val_loss: 0.0201\n",
      "\n",
      "Epoch 01570: LearningRateScheduler reducing learning rate to 0.0002823259751379888.\n",
      "Epoch 1570/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0304\n",
      "Epoch 01570: val_loss did not improve from 0.01785\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0304 - val_loss: 0.0222\n",
      "\n",
      "Epoch 01571: LearningRateScheduler reducing learning rate to 0.00028230385524998376.\n",
      "Epoch 1571/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0390\n",
      "Epoch 01571: val_loss did not improve from 0.01785\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0390 - val_loss: 0.0239\n",
      "\n",
      "Epoch 01572: LearningRateScheduler reducing learning rate to 0.00028228172240555465.\n",
      "Epoch 1572/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0292\n",
      "Epoch 01572: val_loss did not improve from 0.01785\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0292 - val_loss: 0.0236\n",
      "\n",
      "Epoch 01573: LearningRateScheduler reducing learning rate to 0.0002822595766068937.\n",
      "Epoch 1573/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0256\n",
      "Epoch 01573: val_loss did not improve from 0.01785\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0256 - val_loss: 0.0226\n",
      "\n",
      "Epoch 01574: LearningRateScheduler reducing learning rate to 0.00028223741785619465.\n",
      "Epoch 1574/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0392\n",
      "Epoch 01574: val_loss did not improve from 0.01785\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0392 - val_loss: 0.0215\n",
      "\n",
      "Epoch 01575: LearningRateScheduler reducing learning rate to 0.0002822152461556522.\n",
      "Epoch 1575/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0330\n",
      "Epoch 01575: val_loss did not improve from 0.01785\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0330 - val_loss: 0.0233\n",
      "\n",
      "Epoch 01576: LearningRateScheduler reducing learning rate to 0.00028219306150746267.\n",
      "Epoch 1576/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0372\n",
      "Epoch 01576: val_loss did not improve from 0.01785\n",
      "3/3 [==============================] - 1s 238ms/step - loss: 0.0372 - val_loss: 0.0266\n",
      "\n",
      "Epoch 01577: LearningRateScheduler reducing learning rate to 0.0002821708639138234.\n",
      "Epoch 1577/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0327\n",
      "Epoch 01577: val_loss did not improve from 0.01785\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0327 - val_loss: 0.0281\n",
      "\n",
      "Epoch 01578: LearningRateScheduler reducing learning rate to 0.0002821486533769331.\n",
      "Epoch 1578/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0394\n",
      "Epoch 01578: val_loss did not improve from 0.01785\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0394 - val_loss: 0.0283\n",
      "\n",
      "Epoch 01579: LearningRateScheduler reducing learning rate to 0.00028212642989899184.\n",
      "Epoch 1579/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0259\n",
      "Epoch 01579: val_loss did not improve from 0.01785\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0259 - val_loss: 0.0275\n",
      "\n",
      "Epoch 01580: LearningRateScheduler reducing learning rate to 0.00028210419348220084.\n",
      "Epoch 1580/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0222\n",
      "Epoch 01580: val_loss did not improve from 0.01785\n",
      "3/3 [==============================] - 1s 237ms/step - loss: 0.0222 - val_loss: 0.0273\n",
      "\n",
      "Epoch 01581: LearningRateScheduler reducing learning rate to 0.00028208194412876274.\n",
      "Epoch 1581/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0340\n",
      "Epoch 01581: val_loss did not improve from 0.01785\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0340 - val_loss: 0.0257\n",
      "\n",
      "Epoch 01582: LearningRateScheduler reducing learning rate to 0.0002820596818408813.\n",
      "Epoch 1582/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0297\n",
      "Epoch 01582: val_loss did not improve from 0.01785\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0297 - val_loss: 0.0248\n",
      "\n",
      "Epoch 01583: LearningRateScheduler reducing learning rate to 0.00028203740662076176.\n",
      "Epoch 1583/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0404\n",
      "Epoch 01583: val_loss did not improve from 0.01785\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0404 - val_loss: 0.0237\n",
      "\n",
      "Epoch 01584: LearningRateScheduler reducing learning rate to 0.0002820151184706104.\n",
      "Epoch 1584/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0317\n",
      "Epoch 01584: val_loss did not improve from 0.01785\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0317 - val_loss: 0.0227\n",
      "\n",
      "Epoch 01585: LearningRateScheduler reducing learning rate to 0.0002819928173926351.\n",
      "Epoch 1585/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0277\n",
      "Epoch 01585: val_loss did not improve from 0.01785\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0277 - val_loss: 0.0234\n",
      "\n",
      "Epoch 01586: LearningRateScheduler reducing learning rate to 0.0002819705033890447.\n",
      "Epoch 1586/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0340\n",
      "Epoch 01586: val_loss did not improve from 0.01785\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0340 - val_loss: 0.0258\n",
      "\n",
      "Epoch 01587: LearningRateScheduler reducing learning rate to 0.0002819481764620494.\n",
      "Epoch 1587/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0332\n",
      "Epoch 01587: val_loss did not improve from 0.01785\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0332 - val_loss: 0.0304\n",
      "\n",
      "Epoch 01588: LearningRateScheduler reducing learning rate to 0.00028192583661386085.\n",
      "Epoch 1588/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0215\n",
      "Epoch 01588: val_loss did not improve from 0.01785\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0215 - val_loss: 0.0329\n",
      "\n",
      "Epoch 01589: LearningRateScheduler reducing learning rate to 0.00028190348384669184.\n",
      "Epoch 1589/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0313\n",
      "Epoch 01589: val_loss did not improve from 0.01785\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0313 - val_loss: 0.0344\n",
      "\n",
      "Epoch 01590: LearningRateScheduler reducing learning rate to 0.00028188111816275643.\n",
      "Epoch 1590/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0255\n",
      "Epoch 01590: val_loss did not improve from 0.01785\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0255 - val_loss: 0.0350\n",
      "\n",
      "Epoch 01591: LearningRateScheduler reducing learning rate to 0.00028185873956427.\n",
      "Epoch 1591/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0335\n",
      "Epoch 01591: val_loss did not improve from 0.01785\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0335 - val_loss: 0.0348\n",
      "\n",
      "Epoch 01592: LearningRateScheduler reducing learning rate to 0.0002818363480534492.\n",
      "Epoch 1592/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0316\n",
      "Epoch 01592: val_loss did not improve from 0.01785\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0316 - val_loss: 0.0349\n",
      "\n",
      "Epoch 01593: LearningRateScheduler reducing learning rate to 0.00028181394363251203.\n",
      "Epoch 1593/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0276\n",
      "Epoch 01593: val_loss did not improve from 0.01785\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0276 - val_loss: 0.0351\n",
      "\n",
      "Epoch 01594: LearningRateScheduler reducing learning rate to 0.00028179152630367765.\n",
      "Epoch 1594/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0300\n",
      "Epoch 01594: val_loss did not improve from 0.01785\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0300 - val_loss: 0.0330\n",
      "\n",
      "Epoch 01595: LearningRateScheduler reducing learning rate to 0.0002817690960691665.\n",
      "Epoch 1595/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0198\n",
      "Epoch 01595: val_loss did not improve from 0.01785\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0198 - val_loss: 0.0279\n",
      "\n",
      "Epoch 01596: LearningRateScheduler reducing learning rate to 0.00028174665293120035.\n",
      "Epoch 1596/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0565\n",
      "Epoch 01596: val_loss did not improve from 0.01785\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0565 - val_loss: 0.0317\n",
      "\n",
      "Epoch 01597: LearningRateScheduler reducing learning rate to 0.00028172419689200234.\n",
      "Epoch 1597/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0223\n",
      "Epoch 01597: val_loss did not improve from 0.01785\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0223 - val_loss: 0.0337\n",
      "\n",
      "Epoch 01598: LearningRateScheduler reducing learning rate to 0.0002817017279537967.\n",
      "Epoch 1598/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0358\n",
      "Epoch 01598: val_loss did not improve from 0.01785\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0358 - val_loss: 0.0315\n",
      "\n",
      "Epoch 01599: LearningRateScheduler reducing learning rate to 0.00028167924611880905.\n",
      "Epoch 1599/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0249\n",
      "Epoch 01599: val_loss did not improve from 0.01785\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0249 - val_loss: 0.0301\n",
      "\n",
      "Epoch 01600: LearningRateScheduler reducing learning rate to 0.0002816567513892663.\n",
      "Epoch 1600/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0260\n",
      "Epoch 01600: val_loss did not improve from 0.01785\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0260 - val_loss: 0.0280\n",
      "\n",
      "Epoch 01601: LearningRateScheduler reducing learning rate to 0.00028163424376739663.\n",
      "Epoch 1601/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0329\n",
      "Epoch 01601: val_loss did not improve from 0.01785\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0329 - val_loss: 0.0244\n",
      "\n",
      "Epoch 01602: LearningRateScheduler reducing learning rate to 0.00028161172325542937.\n",
      "Epoch 1602/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0212\n",
      "Epoch 01602: val_loss did not improve from 0.01785\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0212 - val_loss: 0.0249\n",
      "\n",
      "Epoch 01603: LearningRateScheduler reducing learning rate to 0.0002815891898555953.\n",
      "Epoch 1603/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0268\n",
      "Epoch 01603: val_loss did not improve from 0.01785\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0268 - val_loss: 0.0256\n",
      "\n",
      "Epoch 01604: LearningRateScheduler reducing learning rate to 0.0002815666435701264.\n",
      "Epoch 1604/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0266\n",
      "Epoch 01604: val_loss did not improve from 0.01785\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0266 - val_loss: 0.0282\n",
      "\n",
      "Epoch 01605: LearningRateScheduler reducing learning rate to 0.0002815440844012559.\n",
      "Epoch 1605/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0288\n",
      "Epoch 01605: val_loss did not improve from 0.01785\n",
      "3/3 [==============================] - 1s 238ms/step - loss: 0.0288 - val_loss: 0.0302\n",
      "\n",
      "Epoch 01606: LearningRateScheduler reducing learning rate to 0.00028152151235121843.\n",
      "Epoch 1606/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0287\n",
      "Epoch 01606: val_loss did not improve from 0.01785\n",
      "3/3 [==============================] - 1s 227ms/step - loss: 0.0287 - val_loss: 0.0324\n",
      "\n",
      "Epoch 01607: LearningRateScheduler reducing learning rate to 0.00028149892742224965.\n",
      "Epoch 1607/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0337\n",
      "Epoch 01607: val_loss did not improve from 0.01785\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0337 - val_loss: 0.0336\n",
      "\n",
      "Epoch 01608: LearningRateScheduler reducing learning rate to 0.0002814763296165868.\n",
      "Epoch 1608/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0368\n",
      "Epoch 01608: val_loss did not improve from 0.01785\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0368 - val_loss: 0.0342\n",
      "\n",
      "Epoch 01609: LearningRateScheduler reducing learning rate to 0.00028145371893646817.\n",
      "Epoch 1609/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0273\n",
      "Epoch 01609: val_loss did not improve from 0.01785\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0273 - val_loss: 0.0326\n",
      "\n",
      "Epoch 01610: LearningRateScheduler reducing learning rate to 0.0002814310953841334.\n",
      "Epoch 1610/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0236\n",
      "Epoch 01610: val_loss did not improve from 0.01785\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0236 - val_loss: 0.0336\n",
      "\n",
      "Epoch 01611: LearningRateScheduler reducing learning rate to 0.0002814084589618234.\n",
      "Epoch 1611/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0224\n",
      "Epoch 01611: val_loss did not improve from 0.01785\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0224 - val_loss: 0.0302\n",
      "\n",
      "Epoch 01612: LearningRateScheduler reducing learning rate to 0.0002813858096717804.\n",
      "Epoch 1612/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0325\n",
      "Epoch 01612: val_loss did not improve from 0.01785\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0325 - val_loss: 0.0261\n",
      "\n",
      "Epoch 01613: LearningRateScheduler reducing learning rate to 0.00028136314751624783.\n",
      "Epoch 1613/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0215\n",
      "Epoch 01613: val_loss did not improve from 0.01785\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0215 - val_loss: 0.0262\n",
      "\n",
      "Epoch 01614: LearningRateScheduler reducing learning rate to 0.00028134047249747044.\n",
      "Epoch 1614/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0212\n",
      "Epoch 01614: val_loss did not improve from 0.01785\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0212 - val_loss: 0.0283\n",
      "\n",
      "Epoch 01615: LearningRateScheduler reducing learning rate to 0.0002813177846176942.\n",
      "Epoch 1615/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0341\n",
      "Epoch 01615: val_loss did not improve from 0.01785\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0341 - val_loss: 0.0291\n",
      "\n",
      "Epoch 01616: LearningRateScheduler reducing learning rate to 0.0002812950838791665.\n",
      "Epoch 1616/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0249\n",
      "Epoch 01616: val_loss did not improve from 0.01785\n",
      "3/3 [==============================] - 1s 238ms/step - loss: 0.0249 - val_loss: 0.0269\n",
      "\n",
      "Epoch 01617: LearningRateScheduler reducing learning rate to 0.0002812723702841358.\n",
      "Epoch 1617/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0298\n",
      "Epoch 01617: val_loss did not improve from 0.01785\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0298 - val_loss: 0.0270\n",
      "\n",
      "Epoch 01618: LearningRateScheduler reducing learning rate to 0.00028124964383485204.\n",
      "Epoch 1618/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0333\n",
      "Epoch 01618: val_loss did not improve from 0.01785\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0333 - val_loss: 0.0285\n",
      "\n",
      "Epoch 01619: LearningRateScheduler reducing learning rate to 0.0002812269045335662.\n",
      "Epoch 1619/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0319\n",
      "Epoch 01619: val_loss did not improve from 0.01785\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0319 - val_loss: 0.0310\n",
      "\n",
      "Epoch 01620: LearningRateScheduler reducing learning rate to 0.0002812041523825307.\n",
      "Epoch 1620/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0272\n",
      "Epoch 01620: val_loss did not improve from 0.01785\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0272 - val_loss: 0.0326\n",
      "\n",
      "Epoch 01621: LearningRateScheduler reducing learning rate to 0.00028118138738399926.\n",
      "Epoch 1621/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0281\n",
      "Epoch 01621: val_loss did not improve from 0.01785\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0281 - val_loss: 0.0345\n",
      "\n",
      "Epoch 01622: LearningRateScheduler reducing learning rate to 0.00028115860954022677.\n",
      "Epoch 1622/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0385\n",
      "Epoch 01622: val_loss did not improve from 0.01785\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0385 - val_loss: 0.0333\n",
      "\n",
      "Epoch 01623: LearningRateScheduler reducing learning rate to 0.00028113581885346936.\n",
      "Epoch 1623/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0329\n",
      "Epoch 01623: val_loss did not improve from 0.01785\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0329 - val_loss: 0.0328\n",
      "\n",
      "Epoch 01624: LearningRateScheduler reducing learning rate to 0.0002811130153259846.\n",
      "Epoch 1624/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0368\n",
      "Epoch 01624: val_loss did not improve from 0.01785\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0368 - val_loss: 0.0309\n",
      "\n",
      "Epoch 01625: LearningRateScheduler reducing learning rate to 0.0002810901989600312.\n",
      "Epoch 1625/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0282\n",
      "Epoch 01625: val_loss did not improve from 0.01785\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0282 - val_loss: 0.0292\n",
      "\n",
      "Epoch 01626: LearningRateScheduler reducing learning rate to 0.0002810673697578691.\n",
      "Epoch 1626/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0185\n",
      "Epoch 01626: val_loss did not improve from 0.01785\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0185 - val_loss: 0.0276\n",
      "\n",
      "Epoch 01627: LearningRateScheduler reducing learning rate to 0.0002810445277217597.\n",
      "Epoch 1627/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0303\n",
      "Epoch 01627: val_loss did not improve from 0.01785\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0303 - val_loss: 0.0299\n",
      "\n",
      "Epoch 01628: LearningRateScheduler reducing learning rate to 0.00028102167285396553.\n",
      "Epoch 1628/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0388\n",
      "Epoch 01628: val_loss did not improve from 0.01785\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0388 - val_loss: 0.0315\n",
      "\n",
      "Epoch 01629: LearningRateScheduler reducing learning rate to 0.0002809988051567504.\n",
      "Epoch 1629/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0249\n",
      "Epoch 01629: val_loss did not improve from 0.01785\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0249 - val_loss: 0.0293\n",
      "\n",
      "Epoch 01630: LearningRateScheduler reducing learning rate to 0.00028097592463237933.\n",
      "Epoch 1630/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0318\n",
      "Epoch 01630: val_loss did not improve from 0.01785\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0318 - val_loss: 0.0263\n",
      "\n",
      "Epoch 01631: LearningRateScheduler reducing learning rate to 0.00028095303128311875.\n",
      "Epoch 1631/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0243\n",
      "Epoch 01631: val_loss did not improve from 0.01785\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0243 - val_loss: 0.0260\n",
      "\n",
      "Epoch 01632: LearningRateScheduler reducing learning rate to 0.0002809301251112364.\n",
      "Epoch 1632/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0448\n",
      "Epoch 01632: val_loss did not improve from 0.01785\n",
      "3/3 [==============================] - 1s 228ms/step - loss: 0.0448 - val_loss: 0.0307\n",
      "\n",
      "Epoch 01633: LearningRateScheduler reducing learning rate to 0.00028090720611900104.\n",
      "Epoch 1633/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0204\n",
      "Epoch 01633: val_loss did not improve from 0.01785\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0204 - val_loss: 0.0323\n",
      "\n",
      "Epoch 01634: LearningRateScheduler reducing learning rate to 0.0002808842743086829.\n",
      "Epoch 1634/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0310\n",
      "Epoch 01634: val_loss did not improve from 0.01785\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0310 - val_loss: 0.0300\n",
      "\n",
      "Epoch 01635: LearningRateScheduler reducing learning rate to 0.0002808613296825534.\n",
      "Epoch 1635/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0407\n",
      "Epoch 01635: val_loss did not improve from 0.01785\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0407 - val_loss: 0.0275\n",
      "\n",
      "Epoch 01636: LearningRateScheduler reducing learning rate to 0.0002808383722428854.\n",
      "Epoch 1636/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0269\n",
      "Epoch 01636: val_loss did not improve from 0.01785\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0269 - val_loss: 0.0241\n",
      "\n",
      "Epoch 01637: LearningRateScheduler reducing learning rate to 0.00028081540199195276.\n",
      "Epoch 1637/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0289\n",
      "Epoch 01637: val_loss did not improve from 0.01785\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0289 - val_loss: 0.0228\n",
      "\n",
      "Epoch 01638: LearningRateScheduler reducing learning rate to 0.00028079241893203075.\n",
      "Epoch 1638/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0214\n",
      "Epoch 01638: val_loss did not improve from 0.01785\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0214 - val_loss: 0.0250\n",
      "\n",
      "Epoch 01639: LearningRateScheduler reducing learning rate to 0.00028076942306539586.\n",
      "Epoch 1639/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0244\n",
      "Epoch 01639: val_loss did not improve from 0.01785\n",
      "3/3 [==============================] - 1s 225ms/step - loss: 0.0244 - val_loss: 0.0260\n",
      "\n",
      "Epoch 01640: LearningRateScheduler reducing learning rate to 0.00028074641439432594.\n",
      "Epoch 1640/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0329\n",
      "Epoch 01640: val_loss did not improve from 0.01785\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0329 - val_loss: 0.0258\n",
      "\n",
      "Epoch 01641: LearningRateScheduler reducing learning rate to 0.0002807233929211001.\n",
      "Epoch 1641/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0232\n",
      "Epoch 01641: val_loss did not improve from 0.01785\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0232 - val_loss: 0.0246\n",
      "\n",
      "Epoch 01642: LearningRateScheduler reducing learning rate to 0.0002807003586479985.\n",
      "Epoch 1642/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0254\n",
      "Epoch 01642: val_loss did not improve from 0.01785\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0254 - val_loss: 0.0236\n",
      "\n",
      "Epoch 01643: LearningRateScheduler reducing learning rate to 0.0002806773115773029.\n",
      "Epoch 1643/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0225\n",
      "Epoch 01643: val_loss did not improve from 0.01785\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0225 - val_loss: 0.0227\n",
      "\n",
      "Epoch 01644: LearningRateScheduler reducing learning rate to 0.00028065425171129615.\n",
      "Epoch 1644/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0249\n",
      "Epoch 01644: val_loss did not improve from 0.01785\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0249 - val_loss: 0.0216\n",
      "\n",
      "Epoch 01645: LearningRateScheduler reducing learning rate to 0.0002806311790522623.\n",
      "Epoch 1645/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0220\n",
      "Epoch 01645: val_loss did not improve from 0.01785\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0220 - val_loss: 0.0209\n",
      "\n",
      "Epoch 01646: LearningRateScheduler reducing learning rate to 0.0002806080936024868.\n",
      "Epoch 1646/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0263\n",
      "Epoch 01646: val_loss did not improve from 0.01785\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0263 - val_loss: 0.0221\n",
      "\n",
      "Epoch 01647: LearningRateScheduler reducing learning rate to 0.00028058499536425636.\n",
      "Epoch 1647/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0299\n",
      "Epoch 01647: val_loss did not improve from 0.01785\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0299 - val_loss: 0.0249\n",
      "\n",
      "Epoch 01648: LearningRateScheduler reducing learning rate to 0.00028056188433985886.\n",
      "Epoch 1648/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0278\n",
      "Epoch 01648: val_loss did not improve from 0.01785\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0278 - val_loss: 0.0250\n",
      "\n",
      "Epoch 01649: LearningRateScheduler reducing learning rate to 0.00028053876053158345.\n",
      "Epoch 1649/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0346\n",
      "Epoch 01649: val_loss did not improve from 0.01785\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0346 - val_loss: 0.0244\n",
      "\n",
      "Epoch 01650: LearningRateScheduler reducing learning rate to 0.00028051562394172067.\n",
      "Epoch 1650/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0323\n",
      "Epoch 01650: val_loss did not improve from 0.01785\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0323 - val_loss: 0.0246\n",
      "\n",
      "Epoch 01651: LearningRateScheduler reducing learning rate to 0.00028049247457256226.\n",
      "Epoch 1651/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0224\n",
      "Epoch 01651: val_loss did not improve from 0.01785\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0224 - val_loss: 0.0244\n",
      "\n",
      "Epoch 01652: LearningRateScheduler reducing learning rate to 0.0002804693124264012.\n",
      "Epoch 1652/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0223\n",
      "Epoch 01652: val_loss did not improve from 0.01785\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0223 - val_loss: 0.0251\n",
      "\n",
      "Epoch 01653: LearningRateScheduler reducing learning rate to 0.00028044613750553176.\n",
      "Epoch 1653/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0298\n",
      "Epoch 01653: val_loss did not improve from 0.01785\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0298 - val_loss: 0.0253\n",
      "\n",
      "Epoch 01654: LearningRateScheduler reducing learning rate to 0.0002804229498122494.\n",
      "Epoch 1654/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0271\n",
      "Epoch 01654: val_loss did not improve from 0.01785\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0271 - val_loss: 0.0238\n",
      "\n",
      "Epoch 01655: LearningRateScheduler reducing learning rate to 0.000280399749348851.\n",
      "Epoch 1655/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0243\n",
      "Epoch 01655: val_loss did not improve from 0.01785\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0243 - val_loss: 0.0261\n",
      "\n",
      "Epoch 01656: LearningRateScheduler reducing learning rate to 0.00028037653611763465.\n",
      "Epoch 1656/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0386\n",
      "Epoch 01656: val_loss did not improve from 0.01785\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0386 - val_loss: 0.0259\n",
      "\n",
      "Epoch 01657: LearningRateScheduler reducing learning rate to 0.0002803533101208996.\n",
      "Epoch 1657/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0340\n",
      "Epoch 01657: val_loss did not improve from 0.01785\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0340 - val_loss: 0.0242\n",
      "\n",
      "Epoch 01658: LearningRateScheduler reducing learning rate to 0.00028033007136094636.\n",
      "Epoch 1658/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0235\n",
      "Epoch 01658: val_loss did not improve from 0.01785\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0235 - val_loss: 0.0211\n",
      "\n",
      "Epoch 01659: LearningRateScheduler reducing learning rate to 0.000280306819840077.\n",
      "Epoch 1659/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0293\n",
      "Epoch 01659: val_loss did not improve from 0.01785\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0293 - val_loss: 0.0191\n",
      "\n",
      "Epoch 01660: LearningRateScheduler reducing learning rate to 0.0002802835555605945.\n",
      "Epoch 1660/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0360\n",
      "Epoch 01660: val_loss improved from 0.01785 to 0.01657, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 10s 3s/step - loss: 0.0360 - val_loss: 0.0166\n",
      "\n",
      "Epoch 01661: LearningRateScheduler reducing learning rate to 0.00028026027852480316.\n",
      "Epoch 1661/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0238\n",
      "Epoch 01661: val_loss improved from 0.01657 to 0.01573, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.0238 - val_loss: 0.0157\n",
      "\n",
      "Epoch 01662: LearningRateScheduler reducing learning rate to 0.0002802369887350088.\n",
      "Epoch 1662/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0280\n",
      "Epoch 01662: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0280 - val_loss: 0.0170\n",
      "\n",
      "Epoch 01663: LearningRateScheduler reducing learning rate to 0.0002802136861935182.\n",
      "Epoch 1663/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0197\n",
      "Epoch 01663: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 240ms/step - loss: 0.0197 - val_loss: 0.0192\n",
      "\n",
      "Epoch 01664: LearningRateScheduler reducing learning rate to 0.00028019037090263964.\n",
      "Epoch 1664/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0339\n",
      "Epoch 01664: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0339 - val_loss: 0.0195\n",
      "\n",
      "Epoch 01665: LearningRateScheduler reducing learning rate to 0.00028016704286468246.\n",
      "Epoch 1665/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0269\n",
      "Epoch 01665: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0269 - val_loss: 0.0200\n",
      "\n",
      "Epoch 01666: LearningRateScheduler reducing learning rate to 0.00028014370208195735.\n",
      "Epoch 1666/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0261\n",
      "Epoch 01666: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 238ms/step - loss: 0.0261 - val_loss: 0.0202\n",
      "\n",
      "Epoch 01667: LearningRateScheduler reducing learning rate to 0.00028012034855677635.\n",
      "Epoch 1667/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0234\n",
      "Epoch 01667: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 225ms/step - loss: 0.0234 - val_loss: 0.0196\n",
      "\n",
      "Epoch 01668: LearningRateScheduler reducing learning rate to 0.0002800969822914527.\n",
      "Epoch 1668/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0279\n",
      "Epoch 01668: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0279 - val_loss: 0.0209\n",
      "\n",
      "Epoch 01669: LearningRateScheduler reducing learning rate to 0.0002800736032883007.\n",
      "Epoch 1669/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0338\n",
      "Epoch 01669: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0338 - val_loss: 0.0228\n",
      "\n",
      "Epoch 01670: LearningRateScheduler reducing learning rate to 0.00028005021154963627.\n",
      "Epoch 1670/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0268\n",
      "Epoch 01670: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0268 - val_loss: 0.0226\n",
      "\n",
      "Epoch 01671: LearningRateScheduler reducing learning rate to 0.00028002680707777636.\n",
      "Epoch 1671/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0186\n",
      "Epoch 01671: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0186 - val_loss: 0.0207\n",
      "\n",
      "Epoch 01672: LearningRateScheduler reducing learning rate to 0.00028000338987503923.\n",
      "Epoch 1672/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0334\n",
      "Epoch 01672: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0334 - val_loss: 0.0220\n",
      "\n",
      "Epoch 01673: LearningRateScheduler reducing learning rate to 0.00027997995994374436.\n",
      "Epoch 1673/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0262\n",
      "Epoch 01673: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0262 - val_loss: 0.0246\n",
      "\n",
      "Epoch 01674: LearningRateScheduler reducing learning rate to 0.0002799565172862127.\n",
      "Epoch 1674/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0338\n",
      "Epoch 01674: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0338 - val_loss: 0.0249\n",
      "\n",
      "Epoch 01675: LearningRateScheduler reducing learning rate to 0.00027993306190476615.\n",
      "Epoch 1675/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0265\n",
      "Epoch 01675: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 228ms/step - loss: 0.0265 - val_loss: 0.0216\n",
      "\n",
      "Epoch 01676: LearningRateScheduler reducing learning rate to 0.00027990959380172806.\n",
      "Epoch 1676/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0244\n",
      "Epoch 01676: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0244 - val_loss: 0.0195\n",
      "\n",
      "Epoch 01677: LearningRateScheduler reducing learning rate to 0.000279886112979423.\n",
      "Epoch 1677/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0212\n",
      "Epoch 01677: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0212 - val_loss: 0.0193\n",
      "\n",
      "Epoch 01678: LearningRateScheduler reducing learning rate to 0.0002798626194401768.\n",
      "Epoch 1678/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0513\n",
      "Epoch 01678: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0513 - val_loss: 0.0207\n",
      "\n",
      "Epoch 01679: LearningRateScheduler reducing learning rate to 0.00027983911318631654.\n",
      "Epoch 1679/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0287\n",
      "Epoch 01679: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0287 - val_loss: 0.0213\n",
      "\n",
      "Epoch 01680: LearningRateScheduler reducing learning rate to 0.0002798155942201706.\n",
      "Epoch 1680/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0291\n",
      "Epoch 01680: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 241ms/step - loss: 0.0291 - val_loss: 0.0228\n",
      "\n",
      "Epoch 01681: LearningRateScheduler reducing learning rate to 0.0002797920625440685.\n",
      "Epoch 1681/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0317\n",
      "Epoch 01681: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0317 - val_loss: 0.0269\n",
      "\n",
      "Epoch 01682: LearningRateScheduler reducing learning rate to 0.0002797685181603413.\n",
      "Epoch 1682/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0237\n",
      "Epoch 01682: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0237 - val_loss: 0.0280\n",
      "\n",
      "Epoch 01683: LearningRateScheduler reducing learning rate to 0.00027974496107132095.\n",
      "Epoch 1683/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0365\n",
      "Epoch 01683: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 240ms/step - loss: 0.0365 - val_loss: 0.0286\n",
      "\n",
      "Epoch 01684: LearningRateScheduler reducing learning rate to 0.0002797213912793408.\n",
      "Epoch 1684/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0247\n",
      "Epoch 01684: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0247 - val_loss: 0.0282\n",
      "\n",
      "Epoch 01685: LearningRateScheduler reducing learning rate to 0.00027969780878673575.\n",
      "Epoch 1685/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0374\n",
      "Epoch 01685: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 238ms/step - loss: 0.0374 - val_loss: 0.0274\n",
      "\n",
      "Epoch 01686: LearningRateScheduler reducing learning rate to 0.00027967421359584136.\n",
      "Epoch 1686/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0288\n",
      "Epoch 01686: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0288 - val_loss: 0.0236\n",
      "\n",
      "Epoch 01687: LearningRateScheduler reducing learning rate to 0.00027965060570899505.\n",
      "Epoch 1687/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0193\n",
      "Epoch 01687: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0193 - val_loss: 0.0207\n",
      "\n",
      "Epoch 01688: LearningRateScheduler reducing learning rate to 0.0002796269851285351.\n",
      "Epoch 1688/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0366\n",
      "Epoch 01688: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0366 - val_loss: 0.0206\n",
      "\n",
      "Epoch 01689: LearningRateScheduler reducing learning rate to 0.00027960335185680125.\n",
      "Epoch 1689/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0328\n",
      "Epoch 01689: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0328 - val_loss: 0.0211\n",
      "\n",
      "Epoch 01690: LearningRateScheduler reducing learning rate to 0.00027957970589613437.\n",
      "Epoch 1690/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0280\n",
      "Epoch 01690: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 228ms/step - loss: 0.0280 - val_loss: 0.0224\n",
      "\n",
      "Epoch 01691: LearningRateScheduler reducing learning rate to 0.00027955604724887677.\n",
      "Epoch 1691/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0206\n",
      "Epoch 01691: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 240ms/step - loss: 0.0206 - val_loss: 0.0246\n",
      "\n",
      "Epoch 01692: LearningRateScheduler reducing learning rate to 0.0002795323759173717.\n",
      "Epoch 1692/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0292\n",
      "Epoch 01692: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0292 - val_loss: 0.0229\n",
      "\n",
      "Epoch 01693: LearningRateScheduler reducing learning rate to 0.000279508691903964.\n",
      "Epoch 1693/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0308\n",
      "Epoch 01693: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 247ms/step - loss: 0.0308 - val_loss: 0.0217\n",
      "\n",
      "Epoch 01694: LearningRateScheduler reducing learning rate to 0.00027948499521099964.\n",
      "Epoch 1694/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0239\n",
      "Epoch 01694: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0239 - val_loss: 0.0231\n",
      "\n",
      "Epoch 01695: LearningRateScheduler reducing learning rate to 0.0002794612858408257.\n",
      "Epoch 1695/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0242\n",
      "Epoch 01695: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0242 - val_loss: 0.0219\n",
      "\n",
      "Epoch 01696: LearningRateScheduler reducing learning rate to 0.00027943756379579076.\n",
      "Epoch 1696/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0357\n",
      "Epoch 01696: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0357 - val_loss: 0.0206\n",
      "\n",
      "Epoch 01697: LearningRateScheduler reducing learning rate to 0.00027941382907824454.\n",
      "Epoch 1697/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0259\n",
      "Epoch 01697: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0259 - val_loss: 0.0216\n",
      "\n",
      "Epoch 01698: LearningRateScheduler reducing learning rate to 0.00027939008169053803.\n",
      "Epoch 1698/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0260\n",
      "Epoch 01698: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 237ms/step - loss: 0.0260 - val_loss: 0.0240\n",
      "\n",
      "Epoch 01699: LearningRateScheduler reducing learning rate to 0.0002793663216350234.\n",
      "Epoch 1699/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0184\n",
      "Epoch 01699: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0184 - val_loss: 0.0248\n",
      "\n",
      "Epoch 01700: LearningRateScheduler reducing learning rate to 0.0002793425489140541.\n",
      "Epoch 1700/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0314\n",
      "Epoch 01700: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0314 - val_loss: 0.0231\n",
      "\n",
      "Epoch 01701: LearningRateScheduler reducing learning rate to 0.000279318763529985.\n",
      "Epoch 1701/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0317\n",
      "Epoch 01701: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0317 - val_loss: 0.0230\n",
      "\n",
      "Epoch 01702: LearningRateScheduler reducing learning rate to 0.00027929496548517203.\n",
      "Epoch 1702/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0201\n",
      "Epoch 01702: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 228ms/step - loss: 0.0201 - val_loss: 0.0229\n",
      "\n",
      "Epoch 01703: LearningRateScheduler reducing learning rate to 0.00027927115478197244.\n",
      "Epoch 1703/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0181\n",
      "Epoch 01703: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0181 - val_loss: 0.0238\n",
      "\n",
      "Epoch 01704: LearningRateScheduler reducing learning rate to 0.00027924733142274476.\n",
      "Epoch 1704/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0178\n",
      "Epoch 01704: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0178 - val_loss: 0.0238\n",
      "\n",
      "Epoch 01705: LearningRateScheduler reducing learning rate to 0.0002792234954098487.\n",
      "Epoch 1705/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0204\n",
      "Epoch 01705: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0204 - val_loss: 0.0236\n",
      "\n",
      "Epoch 01706: LearningRateScheduler reducing learning rate to 0.0002791996467456453.\n",
      "Epoch 1706/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0262\n",
      "Epoch 01706: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0262 - val_loss: 0.0261\n",
      "\n",
      "Epoch 01707: LearningRateScheduler reducing learning rate to 0.00027917578543249687.\n",
      "Epoch 1707/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0181\n",
      "Epoch 01707: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0181 - val_loss: 0.0301\n",
      "\n",
      "Epoch 01708: LearningRateScheduler reducing learning rate to 0.0002791519114727669.\n",
      "Epoch 1708/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0238\n",
      "Epoch 01708: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0238 - val_loss: 0.0296\n",
      "\n",
      "Epoch 01709: LearningRateScheduler reducing learning rate to 0.0002791280248688201.\n",
      "Epoch 1709/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0231\n",
      "Epoch 01709: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0231 - val_loss: 0.0259\n",
      "\n",
      "Epoch 01710: LearningRateScheduler reducing learning rate to 0.0002791041256230226.\n",
      "Epoch 1710/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0339\n",
      "Epoch 01710: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0339 - val_loss: 0.0239\n",
      "\n",
      "Epoch 01711: LearningRateScheduler reducing learning rate to 0.0002790802137377415.\n",
      "Epoch 1711/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0254\n",
      "Epoch 01711: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0254 - val_loss: 0.0244\n",
      "\n",
      "Epoch 01712: LearningRateScheduler reducing learning rate to 0.00027905628921534554.\n",
      "Epoch 1712/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0170\n",
      "Epoch 01712: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0170 - val_loss: 0.0267\n",
      "\n",
      "Epoch 01713: LearningRateScheduler reducing learning rate to 0.0002790323520582044.\n",
      "Epoch 1713/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0382\n",
      "Epoch 01713: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0382 - val_loss: 0.0282\n",
      "\n",
      "Epoch 01714: LearningRateScheduler reducing learning rate to 0.0002790084022686891.\n",
      "Epoch 1714/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0340\n",
      "Epoch 01714: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 228ms/step - loss: 0.0340 - val_loss: 0.0267\n",
      "\n",
      "Epoch 01715: LearningRateScheduler reducing learning rate to 0.00027898443984917187.\n",
      "Epoch 1715/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0337\n",
      "Epoch 01715: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0337 - val_loss: 0.0248\n",
      "\n",
      "Epoch 01716: LearningRateScheduler reducing learning rate to 0.0002789604648020264.\n",
      "Epoch 1716/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0241\n",
      "Epoch 01716: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0241 - val_loss: 0.0226\n",
      "\n",
      "Epoch 01717: LearningRateScheduler reducing learning rate to 0.00027893647712962734.\n",
      "Epoch 1717/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0469\n",
      "Epoch 01717: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 239ms/step - loss: 0.0469 - val_loss: 0.0210\n",
      "\n",
      "Epoch 01718: LearningRateScheduler reducing learning rate to 0.0002789124768343508.\n",
      "Epoch 1718/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0292\n",
      "Epoch 01718: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0292 - val_loss: 0.0213\n",
      "\n",
      "Epoch 01719: LearningRateScheduler reducing learning rate to 0.000278888463918574.\n",
      "Epoch 1719/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0224\n",
      "Epoch 01719: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0224 - val_loss: 0.0221\n",
      "\n",
      "Epoch 01720: LearningRateScheduler reducing learning rate to 0.00027886443838467554.\n",
      "Epoch 1720/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0375\n",
      "Epoch 01720: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0375 - val_loss: 0.0207\n",
      "\n",
      "Epoch 01721: LearningRateScheduler reducing learning rate to 0.0002788404002350352.\n",
      "Epoch 1721/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0167\n",
      "Epoch 01721: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0167 - val_loss: 0.0196\n",
      "\n",
      "Epoch 01722: LearningRateScheduler reducing learning rate to 0.0002788163494720339.\n",
      "Epoch 1722/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0391\n",
      "Epoch 01722: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 228ms/step - loss: 0.0391 - val_loss: 0.0218\n",
      "\n",
      "Epoch 01723: LearningRateScheduler reducing learning rate to 0.00027879228609805403.\n",
      "Epoch 1723/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0206\n",
      "Epoch 01723: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 237ms/step - loss: 0.0206 - val_loss: 0.0220\n",
      "\n",
      "Epoch 01724: LearningRateScheduler reducing learning rate to 0.00027876821011547915.\n",
      "Epoch 1724/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0163\n",
      "Epoch 01724: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0163 - val_loss: 0.0222\n",
      "\n",
      "Epoch 01725: LearningRateScheduler reducing learning rate to 0.00027874412152669397.\n",
      "Epoch 1725/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0283\n",
      "Epoch 01725: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0283 - val_loss: 0.0215\n",
      "\n",
      "Epoch 01726: LearningRateScheduler reducing learning rate to 0.0002787200203340845.\n",
      "Epoch 1726/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0469\n",
      "Epoch 01726: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 238ms/step - loss: 0.0469 - val_loss: 0.0220\n",
      "\n",
      "Epoch 01727: LearningRateScheduler reducing learning rate to 0.00027869590654003816.\n",
      "Epoch 1727/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0150\n",
      "Epoch 01727: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 240ms/step - loss: 0.0150 - val_loss: 0.0212\n",
      "\n",
      "Epoch 01728: LearningRateScheduler reducing learning rate to 0.0002786717801469433.\n",
      "Epoch 1728/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0242\n",
      "Epoch 01728: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0242 - val_loss: 0.0204\n",
      "\n",
      "Epoch 01729: LearningRateScheduler reducing learning rate to 0.0002786476411571898.\n",
      "Epoch 1729/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0174\n",
      "Epoch 01729: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0174 - val_loss: 0.0207\n",
      "\n",
      "Epoch 01730: LearningRateScheduler reducing learning rate to 0.00027862348957316866.\n",
      "Epoch 1730/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0162\n",
      "Epoch 01730: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0162 - val_loss: 0.0211\n",
      "\n",
      "Epoch 01731: LearningRateScheduler reducing learning rate to 0.0002785993253972721.\n",
      "Epoch 1731/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0176\n",
      "Epoch 01731: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0176 - val_loss: 0.0213\n",
      "\n",
      "Epoch 01732: LearningRateScheduler reducing learning rate to 0.0002785751486318937.\n",
      "Epoch 1732/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0293\n",
      "Epoch 01732: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0293 - val_loss: 0.0223\n",
      "\n",
      "Epoch 01733: LearningRateScheduler reducing learning rate to 0.00027855095927942827.\n",
      "Epoch 1733/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0318\n",
      "Epoch 01733: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0318 - val_loss: 0.0241\n",
      "\n",
      "Epoch 01734: LearningRateScheduler reducing learning rate to 0.00027852675734227176.\n",
      "Epoch 1734/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0252\n",
      "Epoch 01734: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0252 - val_loss: 0.0248\n",
      "\n",
      "Epoch 01735: LearningRateScheduler reducing learning rate to 0.0002785025428228214.\n",
      "Epoch 1735/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0166\n",
      "Epoch 01735: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0166 - val_loss: 0.0271\n",
      "\n",
      "Epoch 01736: LearningRateScheduler reducing learning rate to 0.0002784783157234757.\n",
      "Epoch 1736/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0303\n",
      "Epoch 01736: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0303 - val_loss: 0.0269\n",
      "\n",
      "Epoch 01737: LearningRateScheduler reducing learning rate to 0.00027845407604663446.\n",
      "Epoch 1737/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0209\n",
      "Epoch 01737: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 242ms/step - loss: 0.0209 - val_loss: 0.0264\n",
      "\n",
      "Epoch 01738: LearningRateScheduler reducing learning rate to 0.00027842982379469865.\n",
      "Epoch 1738/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0259\n",
      "Epoch 01738: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0259 - val_loss: 0.0251\n",
      "\n",
      "Epoch 01739: LearningRateScheduler reducing learning rate to 0.0002784055589700705.\n",
      "Epoch 1739/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0340\n",
      "Epoch 01739: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0340 - val_loss: 0.0263\n",
      "\n",
      "Epoch 01740: LearningRateScheduler reducing learning rate to 0.0002783812815751535.\n",
      "Epoch 1740/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0286\n",
      "Epoch 01740: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0286 - val_loss: 0.0285\n",
      "\n",
      "Epoch 01741: LearningRateScheduler reducing learning rate to 0.00027835699161235245.\n",
      "Epoch 1741/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0314\n",
      "Epoch 01741: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0314 - val_loss: 0.0308\n",
      "\n",
      "Epoch 01742: LearningRateScheduler reducing learning rate to 0.0002783326890840732.\n",
      "Epoch 1742/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0304\n",
      "Epoch 01742: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0304 - val_loss: 0.0283\n",
      "\n",
      "Epoch 01743: LearningRateScheduler reducing learning rate to 0.000278308373992723.\n",
      "Epoch 1743/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0314\n",
      "Epoch 01743: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 227ms/step - loss: 0.0314 - val_loss: 0.0250\n",
      "\n",
      "Epoch 01744: LearningRateScheduler reducing learning rate to 0.00027828404634071046.\n",
      "Epoch 1744/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0337\n",
      "Epoch 01744: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0337 - val_loss: 0.0259\n",
      "\n",
      "Epoch 01745: LearningRateScheduler reducing learning rate to 0.0002782597061304452.\n",
      "Epoch 1745/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0269\n",
      "Epoch 01745: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0269 - val_loss: 0.0282\n",
      "\n",
      "Epoch 01746: LearningRateScheduler reducing learning rate to 0.00027823535336433806.\n",
      "Epoch 1746/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0246\n",
      "Epoch 01746: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0246 - val_loss: 0.0295\n",
      "\n",
      "Epoch 01747: LearningRateScheduler reducing learning rate to 0.0002782109880448014.\n",
      "Epoch 1747/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0249\n",
      "Epoch 01747: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 237ms/step - loss: 0.0249 - val_loss: 0.0292\n",
      "\n",
      "Epoch 01748: LearningRateScheduler reducing learning rate to 0.0002781866101742485.\n",
      "Epoch 1748/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0185\n",
      "Epoch 01748: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0185 - val_loss: 0.0287\n",
      "\n",
      "Epoch 01749: LearningRateScheduler reducing learning rate to 0.00027816221975509426.\n",
      "Epoch 1749/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0300\n",
      "Epoch 01749: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0300 - val_loss: 0.0256\n",
      "\n",
      "Epoch 01750: LearningRateScheduler reducing learning rate to 0.00027813781678975445.\n",
      "Epoch 1750/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0291\n",
      "Epoch 01750: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0291 - val_loss: 0.0215\n",
      "\n",
      "Epoch 01751: LearningRateScheduler reducing learning rate to 0.00027811340128064627.\n",
      "Epoch 1751/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0240\n",
      "Epoch 01751: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0240 - val_loss: 0.0179\n",
      "\n",
      "Epoch 01752: LearningRateScheduler reducing learning rate to 0.00027808897323018813.\n",
      "Epoch 1752/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0258\n",
      "Epoch 01752: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0258 - val_loss: 0.0196\n",
      "\n",
      "Epoch 01753: LearningRateScheduler reducing learning rate to 0.00027806453264079975.\n",
      "Epoch 1753/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0352\n",
      "Epoch 01753: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0352 - val_loss: 0.0225\n",
      "\n",
      "Epoch 01754: LearningRateScheduler reducing learning rate to 0.00027804007951490195.\n",
      "Epoch 1754/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0305\n",
      "Epoch 01754: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0305 - val_loss: 0.0212\n",
      "\n",
      "Epoch 01755: LearningRateScheduler reducing learning rate to 0.0002780156138549168.\n",
      "Epoch 1755/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0262\n",
      "Epoch 01755: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 228ms/step - loss: 0.0262 - val_loss: 0.0208\n",
      "\n",
      "Epoch 01756: LearningRateScheduler reducing learning rate to 0.0002779911356632679.\n",
      "Epoch 1756/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0235\n",
      "Epoch 01756: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0235 - val_loss: 0.0207\n",
      "\n",
      "Epoch 01757: LearningRateScheduler reducing learning rate to 0.0002779666449423797.\n",
      "Epoch 1757/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0290\n",
      "Epoch 01757: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0290 - val_loss: 0.0225\n",
      "\n",
      "Epoch 01758: LearningRateScheduler reducing learning rate to 0.0002779421416946781.\n",
      "Epoch 1758/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0263\n",
      "Epoch 01758: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 237ms/step - loss: 0.0263 - val_loss: 0.0247\n",
      "\n",
      "Epoch 01759: LearningRateScheduler reducing learning rate to 0.0002779176259225902.\n",
      "Epoch 1759/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0219\n",
      "Epoch 01759: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0219 - val_loss: 0.0274\n",
      "\n",
      "Epoch 01760: LearningRateScheduler reducing learning rate to 0.0002778930976285444.\n",
      "Epoch 1760/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0270\n",
      "Epoch 01760: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 237ms/step - loss: 0.0270 - val_loss: 0.0286\n",
      "\n",
      "Epoch 01761: LearningRateScheduler reducing learning rate to 0.0002778685568149702.\n",
      "Epoch 1761/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0250\n",
      "Epoch 01761: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0250 - val_loss: 0.0278\n",
      "\n",
      "Epoch 01762: LearningRateScheduler reducing learning rate to 0.0002778440034842985.\n",
      "Epoch 1762/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0223\n",
      "Epoch 01762: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0223 - val_loss: 0.0253\n",
      "\n",
      "Epoch 01763: LearningRateScheduler reducing learning rate to 0.00027781943763896126.\n",
      "Epoch 1763/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0169\n",
      "Epoch 01763: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0169 - val_loss: 0.0221\n",
      "\n",
      "Epoch 01764: LearningRateScheduler reducing learning rate to 0.0002777948592813919.\n",
      "Epoch 1764/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0302\n",
      "Epoch 01764: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0302 - val_loss: 0.0234\n",
      "\n",
      "Epoch 01765: LearningRateScheduler reducing learning rate to 0.0002777702684140249.\n",
      "Epoch 1765/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0246\n",
      "Epoch 01765: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0246 - val_loss: 0.0264\n",
      "\n",
      "Epoch 01766: LearningRateScheduler reducing learning rate to 0.000277745665039296.\n",
      "Epoch 1766/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0388\n",
      "Epoch 01766: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0388 - val_loss: 0.0255\n",
      "\n",
      "Epoch 01767: LearningRateScheduler reducing learning rate to 0.0002777210491596424.\n",
      "Epoch 1767/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0250\n",
      "Epoch 01767: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0250 - val_loss: 0.0233\n",
      "\n",
      "Epoch 01768: LearningRateScheduler reducing learning rate to 0.0002776964207775021.\n",
      "Epoch 1768/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0353\n",
      "Epoch 01768: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0353 - val_loss: 0.0229\n",
      "\n",
      "Epoch 01769: LearningRateScheduler reducing learning rate to 0.0002776717798953148.\n",
      "Epoch 1769/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0284\n",
      "Epoch 01769: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 237ms/step - loss: 0.0284 - val_loss: 0.0277\n",
      "\n",
      "Epoch 01770: LearningRateScheduler reducing learning rate to 0.00027764712651552116.\n",
      "Epoch 1770/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0241\n",
      "Epoch 01770: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0241 - val_loss: 0.0554\n",
      "\n",
      "Epoch 01771: LearningRateScheduler reducing learning rate to 0.0002776224606405632.\n",
      "Epoch 1771/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0281\n",
      "Epoch 01771: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0281 - val_loss: 0.1061\n",
      "\n",
      "Epoch 01772: LearningRateScheduler reducing learning rate to 0.0002775977822728841.\n",
      "Epoch 1772/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0234\n",
      "Epoch 01772: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0234 - val_loss: 0.1457\n",
      "\n",
      "Epoch 01773: LearningRateScheduler reducing learning rate to 0.00027757309141492823.\n",
      "Epoch 1773/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0202\n",
      "Epoch 01773: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0202 - val_loss: 0.1182\n",
      "\n",
      "Epoch 01774: LearningRateScheduler reducing learning rate to 0.0002775483880691414.\n",
      "Epoch 1774/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0367\n",
      "Epoch 01774: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0367 - val_loss: 0.0663\n",
      "\n",
      "Epoch 01775: LearningRateScheduler reducing learning rate to 0.0002775236722379705.\n",
      "Epoch 1775/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0262\n",
      "Epoch 01775: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0262 - val_loss: 0.0401\n",
      "\n",
      "Epoch 01776: LearningRateScheduler reducing learning rate to 0.00027749894392386365.\n",
      "Epoch 1776/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0338\n",
      "Epoch 01776: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0338 - val_loss: 0.0307\n",
      "\n",
      "Epoch 01777: LearningRateScheduler reducing learning rate to 0.0002774742031292703.\n",
      "Epoch 1777/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0193\n",
      "Epoch 01777: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0193 - val_loss: 0.0256\n",
      "\n",
      "Epoch 01778: LearningRateScheduler reducing learning rate to 0.000277449449856641.\n",
      "Epoch 1778/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0281\n",
      "Epoch 01778: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0281 - val_loss: 0.0232\n",
      "\n",
      "Epoch 01779: LearningRateScheduler reducing learning rate to 0.00027742468410842775.\n",
      "Epoch 1779/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0291\n",
      "Epoch 01779: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0291 - val_loss: 0.0223\n",
      "\n",
      "Epoch 01780: LearningRateScheduler reducing learning rate to 0.00027739990588708356.\n",
      "Epoch 1780/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0212\n",
      "Epoch 01780: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0212 - val_loss: 0.0228\n",
      "\n",
      "Epoch 01781: LearningRateScheduler reducing learning rate to 0.00027737511519506275.\n",
      "Epoch 1781/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0226\n",
      "Epoch 01781: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0226 - val_loss: 0.0249\n",
      "\n",
      "Epoch 01782: LearningRateScheduler reducing learning rate to 0.000277350312034821.\n",
      "Epoch 1782/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0182\n",
      "Epoch 01782: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0182 - val_loss: 0.0260\n",
      "\n",
      "Epoch 01783: LearningRateScheduler reducing learning rate to 0.000277325496408815.\n",
      "Epoch 1783/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0333\n",
      "Epoch 01783: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0333 - val_loss: 0.0259\n",
      "\n",
      "Epoch 01784: LearningRateScheduler reducing learning rate to 0.00027730066831950284.\n",
      "Epoch 1784/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0207\n",
      "Epoch 01784: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 227ms/step - loss: 0.0207 - val_loss: 0.0238\n",
      "\n",
      "Epoch 01785: LearningRateScheduler reducing learning rate to 0.00027727582776934385.\n",
      "Epoch 1785/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0182\n",
      "Epoch 01785: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0182 - val_loss: 0.0222\n",
      "\n",
      "Epoch 01786: LearningRateScheduler reducing learning rate to 0.0002772509747607985.\n",
      "Epoch 1786/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0229\n",
      "Epoch 01786: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0229 - val_loss: 0.0227\n",
      "\n",
      "Epoch 01787: LearningRateScheduler reducing learning rate to 0.00027722610929632854.\n",
      "Epoch 1787/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0295\n",
      "Epoch 01787: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0295 - val_loss: 0.0234\n",
      "\n",
      "Epoch 01788: LearningRateScheduler reducing learning rate to 0.00027720123137839694.\n",
      "Epoch 1788/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0291\n",
      "Epoch 01788: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0291 - val_loss: 0.0259\n",
      "\n",
      "Epoch 01789: LearningRateScheduler reducing learning rate to 0.0002771763410094679.\n",
      "Epoch 1789/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0392\n",
      "Epoch 01789: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0392 - val_loss: 0.0300\n",
      "\n",
      "Epoch 01790: LearningRateScheduler reducing learning rate to 0.00027715143819200693.\n",
      "Epoch 1790/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0329\n",
      "Epoch 01790: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0329 - val_loss: 0.0332\n",
      "\n",
      "Epoch 01791: LearningRateScheduler reducing learning rate to 0.00027712652292848067.\n",
      "Epoch 1791/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0320\n",
      "Epoch 01791: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0320 - val_loss: 0.0342\n",
      "\n",
      "Epoch 01792: LearningRateScheduler reducing learning rate to 0.00027710159522135704.\n",
      "Epoch 1792/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0236\n",
      "Epoch 01792: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0236 - val_loss: 0.0340\n",
      "\n",
      "Epoch 01793: LearningRateScheduler reducing learning rate to 0.0002770766550731052.\n",
      "Epoch 1793/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0355\n",
      "Epoch 01793: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 237ms/step - loss: 0.0355 - val_loss: 0.0339\n",
      "\n",
      "Epoch 01794: LearningRateScheduler reducing learning rate to 0.00027705170248619557.\n",
      "Epoch 1794/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0275\n",
      "Epoch 01794: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0275 - val_loss: 0.0321\n",
      "\n",
      "Epoch 01795: LearningRateScheduler reducing learning rate to 0.0002770267374630997.\n",
      "Epoch 1795/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0402\n",
      "Epoch 01795: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0402 - val_loss: 0.0305\n",
      "\n",
      "Epoch 01796: LearningRateScheduler reducing learning rate to 0.00027700176000629035.\n",
      "Epoch 1796/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0356\n",
      "Epoch 01796: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0356 - val_loss: 0.0291\n",
      "\n",
      "Epoch 01797: LearningRateScheduler reducing learning rate to 0.00027697677011824176.\n",
      "Epoch 1797/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0258\n",
      "Epoch 01797: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0258 - val_loss: 0.0285\n",
      "\n",
      "Epoch 01798: LearningRateScheduler reducing learning rate to 0.00027695176780142915.\n",
      "Epoch 1798/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0308\n",
      "Epoch 01798: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0308 - val_loss: 0.0263\n",
      "\n",
      "Epoch 01799: LearningRateScheduler reducing learning rate to 0.0002769267530583291.\n",
      "Epoch 1799/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0253\n",
      "Epoch 01799: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0253 - val_loss: 0.0246\n",
      "\n",
      "Epoch 01800: LearningRateScheduler reducing learning rate to 0.0002769017258914193.\n",
      "Epoch 1800/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0329\n",
      "Epoch 01800: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0329 - val_loss: 0.0233\n",
      "\n",
      "Epoch 01801: LearningRateScheduler reducing learning rate to 0.0002768766863031788.\n",
      "Epoch 1801/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0243\n",
      "Epoch 01801: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0243 - val_loss: 0.0243\n",
      "\n",
      "Epoch 01802: LearningRateScheduler reducing learning rate to 0.00027685163429608787.\n",
      "Epoch 1802/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0209\n",
      "Epoch 01802: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0209 - val_loss: 0.0268\n",
      "\n",
      "Epoch 01803: LearningRateScheduler reducing learning rate to 0.0002768265698726279.\n",
      "Epoch 1803/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0334\n",
      "Epoch 01803: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0334 - val_loss: 0.0286\n",
      "\n",
      "Epoch 01804: LearningRateScheduler reducing learning rate to 0.0002768014930352816.\n",
      "Epoch 1804/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0244\n",
      "Epoch 01804: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0244 - val_loss: 0.0252\n",
      "\n",
      "Epoch 01805: LearningRateScheduler reducing learning rate to 0.0002767764037865329.\n",
      "Epoch 1805/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0195\n",
      "Epoch 01805: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0195 - val_loss: 0.0216\n",
      "\n",
      "Epoch 01806: LearningRateScheduler reducing learning rate to 0.000276751302128867.\n",
      "Epoch 1806/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0321\n",
      "Epoch 01806: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0321 - val_loss: 0.0203\n",
      "\n",
      "Epoch 01807: LearningRateScheduler reducing learning rate to 0.00027672618806477017.\n",
      "Epoch 1807/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0341\n",
      "Epoch 01807: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0341 - val_loss: 0.0204\n",
      "\n",
      "Epoch 01808: LearningRateScheduler reducing learning rate to 0.00027670106159673.\n",
      "Epoch 1808/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0216\n",
      "Epoch 01808: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 242ms/step - loss: 0.0216 - val_loss: 0.0189\n",
      "\n",
      "Epoch 01809: LearningRateScheduler reducing learning rate to 0.00027667592272723554.\n",
      "Epoch 1809/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0165\n",
      "Epoch 01809: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0165 - val_loss: 0.0212\n",
      "\n",
      "Epoch 01810: LearningRateScheduler reducing learning rate to 0.0002766507714587766.\n",
      "Epoch 1810/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0258\n",
      "Epoch 01810: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0258 - val_loss: 0.0246\n",
      "\n",
      "Epoch 01811: LearningRateScheduler reducing learning rate to 0.0002766256077938446.\n",
      "Epoch 1811/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0315\n",
      "Epoch 01811: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0315 - val_loss: 0.0273\n",
      "\n",
      "Epoch 01812: LearningRateScheduler reducing learning rate to 0.00027660043173493204.\n",
      "Epoch 1812/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0310\n",
      "Epoch 01812: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 240ms/step - loss: 0.0310 - val_loss: 0.0257\n",
      "\n",
      "Epoch 01813: LearningRateScheduler reducing learning rate to 0.0002765752432845327.\n",
      "Epoch 1813/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0153\n",
      "Epoch 01813: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 239ms/step - loss: 0.0153 - val_loss: 0.0215\n",
      "\n",
      "Epoch 01814: LearningRateScheduler reducing learning rate to 0.00027655004244514145.\n",
      "Epoch 1814/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0233\n",
      "Epoch 01814: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0233 - val_loss: 0.0207\n",
      "\n",
      "Epoch 01815: LearningRateScheduler reducing learning rate to 0.00027652482921925456.\n",
      "Epoch 1815/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0257\n",
      "Epoch 01815: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0257 - val_loss: 0.0212\n",
      "\n",
      "Epoch 01816: LearningRateScheduler reducing learning rate to 0.00027649960360936945.\n",
      "Epoch 1816/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0343\n",
      "Epoch 01816: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0343 - val_loss: 0.0217\n",
      "\n",
      "Epoch 01817: LearningRateScheduler reducing learning rate to 0.00027647436561798486.\n",
      "Epoch 1817/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0276\n",
      "Epoch 01817: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0276 - val_loss: 0.0215\n",
      "\n",
      "Epoch 01818: LearningRateScheduler reducing learning rate to 0.00027644911524760053.\n",
      "Epoch 1818/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0404\n",
      "Epoch 01818: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0404 - val_loss: 0.0229\n",
      "\n",
      "Epoch 01819: LearningRateScheduler reducing learning rate to 0.0002764238525007176.\n",
      "Epoch 1819/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0185\n",
      "Epoch 01819: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0185 - val_loss: 0.0212\n",
      "\n",
      "Epoch 01820: LearningRateScheduler reducing learning rate to 0.0002763985773798385.\n",
      "Epoch 1820/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0304\n",
      "Epoch 01820: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0304 - val_loss: 0.0214\n",
      "\n",
      "Epoch 01821: LearningRateScheduler reducing learning rate to 0.0002763732898874667.\n",
      "Epoch 1821/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0358\n",
      "Epoch 01821: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0358 - val_loss: 0.0228\n",
      "\n",
      "Epoch 01822: LearningRateScheduler reducing learning rate to 0.00027634799002610697.\n",
      "Epoch 1822/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0268\n",
      "Epoch 01822: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0268 - val_loss: 0.0232\n",
      "\n",
      "Epoch 01823: LearningRateScheduler reducing learning rate to 0.0002763226777982654.\n",
      "Epoch 1823/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0362\n",
      "Epoch 01823: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0362 - val_loss: 0.0216\n",
      "\n",
      "Epoch 01824: LearningRateScheduler reducing learning rate to 0.0002762973532064491.\n",
      "Epoch 1824/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0372\n",
      "Epoch 01824: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 226ms/step - loss: 0.0372 - val_loss: 0.0234\n",
      "\n",
      "Epoch 01825: LearningRateScheduler reducing learning rate to 0.0002762720162531667.\n",
      "Epoch 1825/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0194\n",
      "Epoch 01825: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0194 - val_loss: 0.0260\n",
      "\n",
      "Epoch 01826: LearningRateScheduler reducing learning rate to 0.0002762466669409278.\n",
      "Epoch 1826/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0204\n",
      "Epoch 01826: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 240ms/step - loss: 0.0204 - val_loss: 0.0259\n",
      "\n",
      "Epoch 01827: LearningRateScheduler reducing learning rate to 0.0002762213052722433.\n",
      "Epoch 1827/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0274\n",
      "Epoch 01827: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0274 - val_loss: 0.0232\n",
      "\n",
      "Epoch 01828: LearningRateScheduler reducing learning rate to 0.0002761959312496253.\n",
      "Epoch 1828/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0287\n",
      "Epoch 01828: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0287 - val_loss: 0.0228\n",
      "\n",
      "Epoch 01829: LearningRateScheduler reducing learning rate to 0.0002761705448755872.\n",
      "Epoch 1829/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0372\n",
      "Epoch 01829: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0372 - val_loss: 0.0229\n",
      "\n",
      "Epoch 01830: LearningRateScheduler reducing learning rate to 0.00027614514615264353.\n",
      "Epoch 1830/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0157\n",
      "Epoch 01830: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0157 - val_loss: 0.0232\n",
      "\n",
      "Epoch 01831: LearningRateScheduler reducing learning rate to 0.00027611973508331016.\n",
      "Epoch 1831/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0164\n",
      "Epoch 01831: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0164 - val_loss: 0.0274\n",
      "\n",
      "Epoch 01832: LearningRateScheduler reducing learning rate to 0.00027609431167010406.\n",
      "Epoch 1832/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0309\n",
      "Epoch 01832: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0309 - val_loss: 0.0327\n",
      "\n",
      "Epoch 01833: LearningRateScheduler reducing learning rate to 0.0002760688759155436.\n",
      "Epoch 1833/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0363\n",
      "Epoch 01833: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 239ms/step - loss: 0.0363 - val_loss: 0.0355\n",
      "\n",
      "Epoch 01834: LearningRateScheduler reducing learning rate to 0.00027604342782214806.\n",
      "Epoch 1834/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0220\n",
      "Epoch 01834: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 241ms/step - loss: 0.0220 - val_loss: 0.0323\n",
      "\n",
      "Epoch 01835: LearningRateScheduler reducing learning rate to 0.0002760179673924382.\n",
      "Epoch 1835/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0292\n",
      "Epoch 01835: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0292 - val_loss: 0.0258\n",
      "\n",
      "Epoch 01836: LearningRateScheduler reducing learning rate to 0.00027599249462893603.\n",
      "Epoch 1836/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0227\n",
      "Epoch 01836: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 238ms/step - loss: 0.0227 - val_loss: 0.0235\n",
      "\n",
      "Epoch 01837: LearningRateScheduler reducing learning rate to 0.0002759670095341646.\n",
      "Epoch 1837/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0338\n",
      "Epoch 01837: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0338 - val_loss: 0.0215\n",
      "\n",
      "Epoch 01838: LearningRateScheduler reducing learning rate to 0.00027594151211064825.\n",
      "Epoch 1838/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0170\n",
      "Epoch 01838: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0170 - val_loss: 0.0196\n",
      "\n",
      "Epoch 01839: LearningRateScheduler reducing learning rate to 0.00027591600236091265.\n",
      "Epoch 1839/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0195\n",
      "Epoch 01839: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0195 - val_loss: 0.0213\n",
      "\n",
      "Epoch 01840: LearningRateScheduler reducing learning rate to 0.0002758904802874845.\n",
      "Epoch 1840/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0403\n",
      "Epoch 01840: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0403 - val_loss: 0.0243\n",
      "\n",
      "Epoch 01841: LearningRateScheduler reducing learning rate to 0.0002758649458928919.\n",
      "Epoch 1841/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0353\n",
      "Epoch 01841: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0353 - val_loss: 0.0261\n",
      "\n",
      "Epoch 01842: LearningRateScheduler reducing learning rate to 0.00027583939917966404.\n",
      "Epoch 1842/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0420\n",
      "Epoch 01842: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0420 - val_loss: 0.0251\n",
      "\n",
      "Epoch 01843: LearningRateScheduler reducing learning rate to 0.0002758138401503314.\n",
      "Epoch 1843/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0276\n",
      "Epoch 01843: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0276 - val_loss: 0.0248\n",
      "\n",
      "Epoch 01844: LearningRateScheduler reducing learning rate to 0.0002757882688074256.\n",
      "Epoch 1844/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0256\n",
      "Epoch 01844: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0256 - val_loss: 0.0244\n",
      "\n",
      "Epoch 01845: LearningRateScheduler reducing learning rate to 0.00027576268515347967.\n",
      "Epoch 1845/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0218\n",
      "Epoch 01845: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0218 - val_loss: 0.0225\n",
      "\n",
      "Epoch 01846: LearningRateScheduler reducing learning rate to 0.00027573708919102763.\n",
      "Epoch 1846/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0202\n",
      "Epoch 01846: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 237ms/step - loss: 0.0202 - val_loss: 0.0218\n",
      "\n",
      "Epoch 01847: LearningRateScheduler reducing learning rate to 0.0002757114809226048.\n",
      "Epoch 1847/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0284\n",
      "Epoch 01847: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0284 - val_loss: 0.0235\n",
      "\n",
      "Epoch 01848: LearningRateScheduler reducing learning rate to 0.00027568586035074785.\n",
      "Epoch 1848/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0274\n",
      "Epoch 01848: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0274 - val_loss: 0.0276\n",
      "\n",
      "Epoch 01849: LearningRateScheduler reducing learning rate to 0.00027566022747799445.\n",
      "Epoch 1849/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0300\n",
      "Epoch 01849: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 237ms/step - loss: 0.0300 - val_loss: 0.0307\n",
      "\n",
      "Epoch 01850: LearningRateScheduler reducing learning rate to 0.00027563458230688367.\n",
      "Epoch 1850/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0220\n",
      "Epoch 01850: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 227ms/step - loss: 0.0220 - val_loss: 0.0298\n",
      "\n",
      "Epoch 01851: LearningRateScheduler reducing learning rate to 0.0002756089248399557.\n",
      "Epoch 1851/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0286\n",
      "Epoch 01851: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0286 - val_loss: 0.0272\n",
      "\n",
      "Epoch 01852: LearningRateScheduler reducing learning rate to 0.00027558325507975184.\n",
      "Epoch 1852/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0236\n",
      "Epoch 01852: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0236 - val_loss: 0.0245\n",
      "\n",
      "Epoch 01853: LearningRateScheduler reducing learning rate to 0.000275557573028815.\n",
      "Epoch 1853/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0235\n",
      "Epoch 01853: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0235 - val_loss: 0.0232\n",
      "\n",
      "Epoch 01854: LearningRateScheduler reducing learning rate to 0.0002755318786896888.\n",
      "Epoch 1854/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0329\n",
      "Epoch 01854: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0329 - val_loss: 0.0242\n",
      "\n",
      "Epoch 01855: LearningRateScheduler reducing learning rate to 0.00027550617206491853.\n",
      "Epoch 1855/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0255\n",
      "Epoch 01855: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0255 - val_loss: 0.0260\n",
      "\n",
      "Epoch 01856: LearningRateScheduler reducing learning rate to 0.00027548045315705033.\n",
      "Epoch 1856/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0277\n",
      "Epoch 01856: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 237ms/step - loss: 0.0277 - val_loss: 0.0264\n",
      "\n",
      "Epoch 01857: LearningRateScheduler reducing learning rate to 0.0002754547219686318.\n",
      "Epoch 1857/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0223\n",
      "Epoch 01857: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0223 - val_loss: 0.0276\n",
      "\n",
      "Epoch 01858: LearningRateScheduler reducing learning rate to 0.00027542897850221164.\n",
      "Epoch 1858/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0315\n",
      "Epoch 01858: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 239ms/step - loss: 0.0315 - val_loss: 0.0278\n",
      "\n",
      "Epoch 01859: LearningRateScheduler reducing learning rate to 0.0002754032227603398.\n",
      "Epoch 1859/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0181\n",
      "Epoch 01859: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0181 - val_loss: 0.0256\n",
      "\n",
      "Epoch 01860: LearningRateScheduler reducing learning rate to 0.0002753774547455675.\n",
      "Epoch 1860/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0316\n",
      "Epoch 01860: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0316 - val_loss: 0.0229\n",
      "\n",
      "Epoch 01861: LearningRateScheduler reducing learning rate to 0.00027535167446044703.\n",
      "Epoch 1861/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0279\n",
      "Epoch 01861: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0279 - val_loss: 0.0215\n",
      "\n",
      "Epoch 01862: LearningRateScheduler reducing learning rate to 0.00027532588190753207.\n",
      "Epoch 1862/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0258\n",
      "Epoch 01862: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0258 - val_loss: 0.0211\n",
      "\n",
      "Epoch 01863: LearningRateScheduler reducing learning rate to 0.0002753000770893774.\n",
      "Epoch 1863/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0131\n",
      "Epoch 01863: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 238ms/step - loss: 0.0131 - val_loss: 0.0217\n",
      "\n",
      "Epoch 01864: LearningRateScheduler reducing learning rate to 0.000275274260008539.\n",
      "Epoch 1864/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0170\n",
      "Epoch 01864: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0170 - val_loss: 0.0242\n",
      "\n",
      "Epoch 01865: LearningRateScheduler reducing learning rate to 0.00027524843066757416.\n",
      "Epoch 1865/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0305\n",
      "Epoch 01865: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0305 - val_loss: 0.0273\n",
      "\n",
      "Epoch 01866: LearningRateScheduler reducing learning rate to 0.0002752225890690413.\n",
      "Epoch 1866/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0289\n",
      "Epoch 01866: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0289 - val_loss: 0.0287\n",
      "\n",
      "Epoch 01867: LearningRateScheduler reducing learning rate to 0.0002751967352155002.\n",
      "Epoch 1867/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0216\n",
      "Epoch 01867: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0216 - val_loss: 0.0273\n",
      "\n",
      "Epoch 01868: LearningRateScheduler reducing learning rate to 0.00027517086910951166.\n",
      "Epoch 1868/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0261\n",
      "Epoch 01868: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 242ms/step - loss: 0.0261 - val_loss: 0.0250\n",
      "\n",
      "Epoch 01869: LearningRateScheduler reducing learning rate to 0.0002751449907536378.\n",
      "Epoch 1869/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0384\n",
      "Epoch 01869: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 240ms/step - loss: 0.0384 - val_loss: 0.0240\n",
      "\n",
      "Epoch 01870: LearningRateScheduler reducing learning rate to 0.0002751191001504418.\n",
      "Epoch 1870/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0250\n",
      "Epoch 01870: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0250 - val_loss: 0.0247\n",
      "\n",
      "Epoch 01871: LearningRateScheduler reducing learning rate to 0.0002750931973024884.\n",
      "Epoch 1871/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0220\n",
      "Epoch 01871: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0220 - val_loss: 0.0259\n",
      "\n",
      "Epoch 01872: LearningRateScheduler reducing learning rate to 0.0002750672822123432.\n",
      "Epoch 1872/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0319\n",
      "Epoch 01872: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0319 - val_loss: 0.0258\n",
      "\n",
      "Epoch 01873: LearningRateScheduler reducing learning rate to 0.00027504135488257316.\n",
      "Epoch 1873/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0163\n",
      "Epoch 01873: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0163 - val_loss: 0.0242\n",
      "\n",
      "Epoch 01874: LearningRateScheduler reducing learning rate to 0.0002750154153157465.\n",
      "Epoch 1874/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0298\n",
      "Epoch 01874: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0298 - val_loss: 0.0240\n",
      "\n",
      "Epoch 01875: LearningRateScheduler reducing learning rate to 0.0002749894635144326.\n",
      "Epoch 1875/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0331\n",
      "Epoch 01875: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0331 - val_loss: 0.0237\n",
      "\n",
      "Epoch 01876: LearningRateScheduler reducing learning rate to 0.0002749634994812019.\n",
      "Epoch 1876/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0315\n",
      "Epoch 01876: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0315 - val_loss: 0.0202\n",
      "\n",
      "Epoch 01877: LearningRateScheduler reducing learning rate to 0.0002749375232186264.\n",
      "Epoch 1877/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0282\n",
      "Epoch 01877: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 238ms/step - loss: 0.0282 - val_loss: 0.0190\n",
      "\n",
      "Epoch 01878: LearningRateScheduler reducing learning rate to 0.000274911534729279.\n",
      "Epoch 1878/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0410\n",
      "Epoch 01878: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0410 - val_loss: 0.0207\n",
      "\n",
      "Epoch 01879: LearningRateScheduler reducing learning rate to 0.00027488553401573386.\n",
      "Epoch 1879/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0246\n",
      "Epoch 01879: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 227ms/step - loss: 0.0246 - val_loss: 0.0217\n",
      "\n",
      "Epoch 01880: LearningRateScheduler reducing learning rate to 0.00027485952108056657.\n",
      "Epoch 1880/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0255\n",
      "Epoch 01880: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0255 - val_loss: 0.0222\n",
      "\n",
      "Epoch 01881: LearningRateScheduler reducing learning rate to 0.00027483349592635363.\n",
      "Epoch 1881/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0154\n",
      "Epoch 01881: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0154 - val_loss: 0.0226\n",
      "\n",
      "Epoch 01882: LearningRateScheduler reducing learning rate to 0.000274807458555673.\n",
      "Epoch 1882/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0206\n",
      "Epoch 01882: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0206 - val_loss: 0.0222\n",
      "\n",
      "Epoch 01883: LearningRateScheduler reducing learning rate to 0.0002747814089711036.\n",
      "Epoch 1883/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0307\n",
      "Epoch 01883: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0307 - val_loss: 0.0211\n",
      "\n",
      "Epoch 01884: LearningRateScheduler reducing learning rate to 0.00027475534717522586.\n",
      "Epoch 1884/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0211\n",
      "Epoch 01884: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0211 - val_loss: 0.0208\n",
      "\n",
      "Epoch 01885: LearningRateScheduler reducing learning rate to 0.00027472927317062114.\n",
      "Epoch 1885/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0204\n",
      "Epoch 01885: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0204 - val_loss: 0.0201\n",
      "\n",
      "Epoch 01886: LearningRateScheduler reducing learning rate to 0.00027470318695987223.\n",
      "Epoch 1886/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0197\n",
      "Epoch 01886: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0197 - val_loss: 0.0187\n",
      "\n",
      "Epoch 01887: LearningRateScheduler reducing learning rate to 0.00027467708854556294.\n",
      "Epoch 1887/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0205\n",
      "Epoch 01887: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 241ms/step - loss: 0.0205 - val_loss: 0.0183\n",
      "\n",
      "Epoch 01888: LearningRateScheduler reducing learning rate to 0.00027465097793027846.\n",
      "Epoch 1888/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0393\n",
      "Epoch 01888: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0393 - val_loss: 0.0188\n",
      "\n",
      "Epoch 01889: LearningRateScheduler reducing learning rate to 0.0002746248551166051.\n",
      "Epoch 1889/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0191\n",
      "Epoch 01889: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0191 - val_loss: 0.0189\n",
      "\n",
      "Epoch 01890: LearningRateScheduler reducing learning rate to 0.00027459872010713026.\n",
      "Epoch 1890/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0210\n",
      "Epoch 01890: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0210 - val_loss: 0.0175\n",
      "\n",
      "Epoch 01891: LearningRateScheduler reducing learning rate to 0.0002745725729044428.\n",
      "Epoch 1891/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0290\n",
      "Epoch 01891: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0290 - val_loss: 0.0163\n",
      "\n",
      "Epoch 01892: LearningRateScheduler reducing learning rate to 0.0002745464135111326.\n",
      "Epoch 1892/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0220\n",
      "Epoch 01892: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0220 - val_loss: 0.0166\n",
      "\n",
      "Epoch 01893: LearningRateScheduler reducing learning rate to 0.00027452024192979086.\n",
      "Epoch 1893/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0187\n",
      "Epoch 01893: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0187 - val_loss: 0.0158\n",
      "\n",
      "Epoch 01894: LearningRateScheduler reducing learning rate to 0.0002744940581630099.\n",
      "Epoch 1894/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0216\n",
      "Epoch 01894: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0216 - val_loss: 0.0162\n",
      "\n",
      "Epoch 01895: LearningRateScheduler reducing learning rate to 0.0002744678622133833.\n",
      "Epoch 1895/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0308\n",
      "Epoch 01895: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0308 - val_loss: 0.0186\n",
      "\n",
      "Epoch 01896: LearningRateScheduler reducing learning rate to 0.00027444165408350583.\n",
      "Epoch 1896/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0210\n",
      "Epoch 01896: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0210 - val_loss: 0.0214\n",
      "\n",
      "Epoch 01897: LearningRateScheduler reducing learning rate to 0.00027441543377597344.\n",
      "Epoch 1897/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0263\n",
      "Epoch 01897: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0263 - val_loss: 0.0222\n",
      "\n",
      "Epoch 01898: LearningRateScheduler reducing learning rate to 0.0002743892012933833.\n",
      "Epoch 1898/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0303\n",
      "Epoch 01898: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0303 - val_loss: 0.0207\n",
      "\n",
      "Epoch 01899: LearningRateScheduler reducing learning rate to 0.00027436295663833387.\n",
      "Epoch 1899/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0265\n",
      "Epoch 01899: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0265 - val_loss: 0.0191\n",
      "\n",
      "Epoch 01900: LearningRateScheduler reducing learning rate to 0.0002743366998134247.\n",
      "Epoch 1900/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0206\n",
      "Epoch 01900: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0206 - val_loss: 0.0179\n",
      "\n",
      "Epoch 01901: LearningRateScheduler reducing learning rate to 0.00027431043082125654.\n",
      "Epoch 1901/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0329\n",
      "Epoch 01901: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0329 - val_loss: 0.0177\n",
      "\n",
      "Epoch 01902: LearningRateScheduler reducing learning rate to 0.0002742841496644315.\n",
      "Epoch 1902/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0202\n",
      "Epoch 01902: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 244ms/step - loss: 0.0202 - val_loss: 0.0177\n",
      "\n",
      "Epoch 01903: LearningRateScheduler reducing learning rate to 0.0002742578563455527.\n",
      "Epoch 1903/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0263\n",
      "Epoch 01903: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0263 - val_loss: 0.0179\n",
      "\n",
      "Epoch 01904: LearningRateScheduler reducing learning rate to 0.00027423155086722455.\n",
      "Epoch 1904/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0208\n",
      "Epoch 01904: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0208 - val_loss: 0.0199\n",
      "\n",
      "Epoch 01905: LearningRateScheduler reducing learning rate to 0.0002742052332320528.\n",
      "Epoch 1905/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0329\n",
      "Epoch 01905: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0329 - val_loss: 0.0220\n",
      "\n",
      "Epoch 01906: LearningRateScheduler reducing learning rate to 0.0002741789034426441.\n",
      "Epoch 1906/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0246\n",
      "Epoch 01906: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0246 - val_loss: 0.0221\n",
      "\n",
      "Epoch 01907: LearningRateScheduler reducing learning rate to 0.0002741525615016066.\n",
      "Epoch 1907/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0200\n",
      "Epoch 01907: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0200 - val_loss: 0.0206\n",
      "\n",
      "Epoch 01908: LearningRateScheduler reducing learning rate to 0.0002741262074115495.\n",
      "Epoch 1908/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0321\n",
      "Epoch 01908: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0321 - val_loss: 0.0193\n",
      "\n",
      "Epoch 01909: LearningRateScheduler reducing learning rate to 0.0002740998411750831.\n",
      "Epoch 1909/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0298\n",
      "Epoch 01909: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0298 - val_loss: 0.0193\n",
      "\n",
      "Epoch 01910: LearningRateScheduler reducing learning rate to 0.0002740734627948193.\n",
      "Epoch 1910/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0284\n",
      "Epoch 01910: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0284 - val_loss: 0.0204\n",
      "\n",
      "Epoch 01911: LearningRateScheduler reducing learning rate to 0.0002740470722733707.\n",
      "Epoch 1911/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0212\n",
      "Epoch 01911: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0212 - val_loss: 0.0214\n",
      "\n",
      "Epoch 01912: LearningRateScheduler reducing learning rate to 0.00027402066961335146.\n",
      "Epoch 1912/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0170\n",
      "Epoch 01912: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0170 - val_loss: 0.0223\n",
      "\n",
      "Epoch 01913: LearningRateScheduler reducing learning rate to 0.0002739942548173768.\n",
      "Epoch 1913/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0258\n",
      "Epoch 01913: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0258 - val_loss: 0.0222\n",
      "\n",
      "Epoch 01914: LearningRateScheduler reducing learning rate to 0.0002739678278880632.\n",
      "Epoch 1914/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0206\n",
      "Epoch 01914: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 239ms/step - loss: 0.0206 - val_loss: 0.0215\n",
      "\n",
      "Epoch 01915: LearningRateScheduler reducing learning rate to 0.0002739413888280282.\n",
      "Epoch 1915/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0186\n",
      "Epoch 01915: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0186 - val_loss: 0.0185\n",
      "\n",
      "Epoch 01916: LearningRateScheduler reducing learning rate to 0.0002739149376398908.\n",
      "Epoch 1916/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0267\n",
      "Epoch 01916: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0267 - val_loss: 0.0173\n",
      "\n",
      "Epoch 01917: LearningRateScheduler reducing learning rate to 0.000273888474326271.\n",
      "Epoch 1917/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0332\n",
      "Epoch 01917: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0332 - val_loss: 0.0171\n",
      "\n",
      "Epoch 01918: LearningRateScheduler reducing learning rate to 0.0002738619988897899.\n",
      "Epoch 1918/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0172\n",
      "Epoch 01918: val_loss did not improve from 0.01573\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0172 - val_loss: 0.0165\n",
      "\n",
      "Epoch 01919: LearningRateScheduler reducing learning rate to 0.00027383551133307023.\n",
      "Epoch 1919/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0195\n",
      "Epoch 01919: val_loss improved from 0.01573 to 0.01522, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 10s 3s/step - loss: 0.0195 - val_loss: 0.0152\n",
      "\n",
      "Epoch 01920: LearningRateScheduler reducing learning rate to 0.0002738090116587354.\n",
      "Epoch 1920/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0209\n",
      "Epoch 01920: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0209 - val_loss: 0.0157\n",
      "\n",
      "Epoch 01921: LearningRateScheduler reducing learning rate to 0.0002737824998694104.\n",
      "Epoch 1921/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0168\n",
      "Epoch 01921: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0168 - val_loss: 0.0216\n",
      "\n",
      "Epoch 01922: LearningRateScheduler reducing learning rate to 0.0002737559759677212.\n",
      "Epoch 1922/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0228\n",
      "Epoch 01922: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0228 - val_loss: 0.0232\n",
      "\n",
      "Epoch 01923: LearningRateScheduler reducing learning rate to 0.00027372943995629514.\n",
      "Epoch 1923/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0244\n",
      "Epoch 01923: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0244 - val_loss: 0.0242\n",
      "\n",
      "Epoch 01924: LearningRateScheduler reducing learning rate to 0.00027370289183776067.\n",
      "Epoch 1924/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0262\n",
      "Epoch 01924: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0262 - val_loss: 0.0250\n",
      "\n",
      "Epoch 01925: LearningRateScheduler reducing learning rate to 0.0002736763316147473.\n",
      "Epoch 1925/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0294\n",
      "Epoch 01925: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0294 - val_loss: 0.0239\n",
      "\n",
      "Epoch 01926: LearningRateScheduler reducing learning rate to 0.00027364975928988606.\n",
      "Epoch 1926/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0238\n",
      "Epoch 01926: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 228ms/step - loss: 0.0238 - val_loss: 0.0216\n",
      "\n",
      "Epoch 01927: LearningRateScheduler reducing learning rate to 0.0002736231748658089.\n",
      "Epoch 1927/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0158\n",
      "Epoch 01927: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 238ms/step - loss: 0.0158 - val_loss: 0.0203\n",
      "\n",
      "Epoch 01928: LearningRateScheduler reducing learning rate to 0.0002735965783451491.\n",
      "Epoch 1928/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0214\n",
      "Epoch 01928: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0214 - val_loss: 0.0207\n",
      "\n",
      "Epoch 01929: LearningRateScheduler reducing learning rate to 0.0002735699697305411.\n",
      "Epoch 1929/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0236\n",
      "Epoch 01929: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0236 - val_loss: 0.0207\n",
      "\n",
      "Epoch 01930: LearningRateScheduler reducing learning rate to 0.0002735433490246205.\n",
      "Epoch 1930/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0240\n",
      "Epoch 01930: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0240 - val_loss: 0.0193\n",
      "\n",
      "Epoch 01931: LearningRateScheduler reducing learning rate to 0.0002735167162300243.\n",
      "Epoch 1931/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0171\n",
      "Epoch 01931: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0171 - val_loss: 0.0177\n",
      "\n",
      "Epoch 01932: LearningRateScheduler reducing learning rate to 0.0002734900713493903.\n",
      "Epoch 1932/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0230\n",
      "Epoch 01932: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0230 - val_loss: 0.0180\n",
      "\n",
      "Epoch 01933: LearningRateScheduler reducing learning rate to 0.00027346341438535794.\n",
      "Epoch 1933/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0211\n",
      "Epoch 01933: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0211 - val_loss: 0.0204\n",
      "\n",
      "Epoch 01934: LearningRateScheduler reducing learning rate to 0.0002734367453405676.\n",
      "Epoch 1934/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0195\n",
      "Epoch 01934: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0195 - val_loss: 0.0216\n",
      "\n",
      "Epoch 01935: LearningRateScheduler reducing learning rate to 0.0002734100642176608.\n",
      "Epoch 1935/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0299\n",
      "Epoch 01935: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0299 - val_loss: 0.0211\n",
      "\n",
      "Epoch 01936: LearningRateScheduler reducing learning rate to 0.00027338337101928055.\n",
      "Epoch 1936/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0139\n",
      "Epoch 01936: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0139 - val_loss: 0.0202\n",
      "\n",
      "Epoch 01937: LearningRateScheduler reducing learning rate to 0.0002733566657480707.\n",
      "Epoch 1937/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0240\n",
      "Epoch 01937: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0240 - val_loss: 0.0187\n",
      "\n",
      "Epoch 01938: LearningRateScheduler reducing learning rate to 0.0002733299484066766.\n",
      "Epoch 1938/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0326\n",
      "Epoch 01938: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 227ms/step - loss: 0.0326 - val_loss: 0.0180\n",
      "\n",
      "Epoch 01939: LearningRateScheduler reducing learning rate to 0.0002733032189977446.\n",
      "Epoch 1939/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0216\n",
      "Epoch 01939: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0216 - val_loss: 0.0198\n",
      "\n",
      "Epoch 01940: LearningRateScheduler reducing learning rate to 0.0002732764775239223.\n",
      "Epoch 1940/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0135\n",
      "Epoch 01940: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0135 - val_loss: 0.0223\n",
      "\n",
      "Epoch 01941: LearningRateScheduler reducing learning rate to 0.00027324972398785853.\n",
      "Epoch 1941/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0256\n",
      "Epoch 01941: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0256 - val_loss: 0.0229\n",
      "\n",
      "Epoch 01942: LearningRateScheduler reducing learning rate to 0.00027322295839220327.\n",
      "Epoch 1942/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0142\n",
      "Epoch 01942: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 244ms/step - loss: 0.0142 - val_loss: 0.0234\n",
      "\n",
      "Epoch 01943: LearningRateScheduler reducing learning rate to 0.00027319618073960776.\n",
      "Epoch 1943/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0206\n",
      "Epoch 01943: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0206 - val_loss: 0.0228\n",
      "\n",
      "Epoch 01944: LearningRateScheduler reducing learning rate to 0.00027316939103272435.\n",
      "Epoch 1944/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0187\n",
      "Epoch 01944: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0187 - val_loss: 0.0220\n",
      "\n",
      "Epoch 01945: LearningRateScheduler reducing learning rate to 0.00027314258927420667.\n",
      "Epoch 1945/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0383\n",
      "Epoch 01945: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0383 - val_loss: 0.0229\n",
      "\n",
      "Epoch 01946: LearningRateScheduler reducing learning rate to 0.00027311577546670944.\n",
      "Epoch 1946/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0206\n",
      "Epoch 01946: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0206 - val_loss: 0.0236\n",
      "\n",
      "Epoch 01947: LearningRateScheduler reducing learning rate to 0.0002730889496128886.\n",
      "Epoch 1947/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0335\n",
      "Epoch 01947: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0335 - val_loss: 0.0225\n",
      "\n",
      "Epoch 01948: LearningRateScheduler reducing learning rate to 0.0002730621117154014.\n",
      "Epoch 1948/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0317\n",
      "Epoch 01948: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0317 - val_loss: 0.0212\n",
      "\n",
      "Epoch 01949: LearningRateScheduler reducing learning rate to 0.0002730352617769061.\n",
      "Epoch 1949/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0165\n",
      "Epoch 01949: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0165 - val_loss: 0.0194\n",
      "\n",
      "Epoch 01950: LearningRateScheduler reducing learning rate to 0.0002730083998000624.\n",
      "Epoch 1950/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0213\n",
      "Epoch 01950: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0213 - val_loss: 0.0180\n",
      "\n",
      "Epoch 01951: LearningRateScheduler reducing learning rate to 0.00027298152578753086.\n",
      "Epoch 1951/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0226\n",
      "Epoch 01951: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0226 - val_loss: 0.0171\n",
      "\n",
      "Epoch 01952: LearningRateScheduler reducing learning rate to 0.00027295463974197354.\n",
      "Epoch 1952/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0252\n",
      "Epoch 01952: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 228ms/step - loss: 0.0252 - val_loss: 0.0214\n",
      "\n",
      "Epoch 01953: LearningRateScheduler reducing learning rate to 0.0002729277416660535.\n",
      "Epoch 1953/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0261\n",
      "Epoch 01953: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0261 - val_loss: 0.0386\n",
      "\n",
      "Epoch 01954: LearningRateScheduler reducing learning rate to 0.0002729008315624351.\n",
      "Epoch 1954/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0226\n",
      "Epoch 01954: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0226 - val_loss: 0.0522\n",
      "\n",
      "Epoch 01955: LearningRateScheduler reducing learning rate to 0.00027287390943378384.\n",
      "Epoch 1955/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0369\n",
      "Epoch 01955: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0369 - val_loss: 0.0562\n",
      "\n",
      "Epoch 01956: LearningRateScheduler reducing learning rate to 0.00027284697528276637.\n",
      "Epoch 1956/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0336\n",
      "Epoch 01956: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0336 - val_loss: 0.0510\n",
      "\n",
      "Epoch 01957: LearningRateScheduler reducing learning rate to 0.0002728200291120506.\n",
      "Epoch 1957/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0391\n",
      "Epoch 01957: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0391 - val_loss: 0.0415\n",
      "\n",
      "Epoch 01958: LearningRateScheduler reducing learning rate to 0.00027279307092430567.\n",
      "Epoch 1958/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0334\n",
      "Epoch 01958: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 237ms/step - loss: 0.0334 - val_loss: 0.0362\n",
      "\n",
      "Epoch 01959: LearningRateScheduler reducing learning rate to 0.00027276610072220176.\n",
      "Epoch 1959/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0359\n",
      "Epoch 01959: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0359 - val_loss: 0.0328\n",
      "\n",
      "Epoch 01960: LearningRateScheduler reducing learning rate to 0.0002727391185084104.\n",
      "Epoch 1960/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0251\n",
      "Epoch 01960: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0251 - val_loss: 0.0335\n",
      "\n",
      "Epoch 01961: LearningRateScheduler reducing learning rate to 0.00027271212428560426.\n",
      "Epoch 1961/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0322\n",
      "Epoch 01961: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 242ms/step - loss: 0.0322 - val_loss: 0.0381\n",
      "\n",
      "Epoch 01962: LearningRateScheduler reducing learning rate to 0.00027268511805645713.\n",
      "Epoch 1962/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0251\n",
      "Epoch 01962: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0251 - val_loss: 0.0416\n",
      "\n",
      "Epoch 01963: LearningRateScheduler reducing learning rate to 0.00027265809982364404.\n",
      "Epoch 1963/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0169\n",
      "Epoch 01963: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0169 - val_loss: 0.0432\n",
      "\n",
      "Epoch 01964: LearningRateScheduler reducing learning rate to 0.00027263106958984117.\n",
      "Epoch 1964/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0314\n",
      "Epoch 01964: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0314 - val_loss: 0.0439\n",
      "\n",
      "Epoch 01965: LearningRateScheduler reducing learning rate to 0.000272604027357726.\n",
      "Epoch 1965/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0225\n",
      "Epoch 01965: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0225 - val_loss: 0.0421\n",
      "\n",
      "Epoch 01966: LearningRateScheduler reducing learning rate to 0.0002725769731299771.\n",
      "Epoch 1966/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0240\n",
      "Epoch 01966: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 226ms/step - loss: 0.0240 - val_loss: 0.0381\n",
      "\n",
      "Epoch 01967: LearningRateScheduler reducing learning rate to 0.00027254990690927426.\n",
      "Epoch 1967/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0320\n",
      "Epoch 01967: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0320 - val_loss: 0.0363\n",
      "\n",
      "Epoch 01968: LearningRateScheduler reducing learning rate to 0.00027252282869829844.\n",
      "Epoch 1968/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0374\n",
      "Epoch 01968: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 237ms/step - loss: 0.0374 - val_loss: 0.0372\n",
      "\n",
      "Epoch 01969: LearningRateScheduler reducing learning rate to 0.0002724957384997318.\n",
      "Epoch 1969/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0391\n",
      "Epoch 01969: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0391 - val_loss: 0.0329\n",
      "\n",
      "Epoch 01970: LearningRateScheduler reducing learning rate to 0.0002724686363162577.\n",
      "Epoch 1970/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0406\n",
      "Epoch 01970: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0406 - val_loss: 0.0286\n",
      "\n",
      "Epoch 01971: LearningRateScheduler reducing learning rate to 0.0002724415221505607.\n",
      "Epoch 1971/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0210\n",
      "Epoch 01971: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 240ms/step - loss: 0.0210 - val_loss: 0.0257\n",
      "\n",
      "Epoch 01972: LearningRateScheduler reducing learning rate to 0.0002724143960053264.\n",
      "Epoch 1972/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0229\n",
      "Epoch 01972: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0229 - val_loss: 0.0241\n",
      "\n",
      "Epoch 01973: LearningRateScheduler reducing learning rate to 0.00027238725788324185.\n",
      "Epoch 1973/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0204\n",
      "Epoch 01973: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0204 - val_loss: 0.0244\n",
      "\n",
      "Epoch 01974: LearningRateScheduler reducing learning rate to 0.00027236010778699507.\n",
      "Epoch 1974/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0236\n",
      "Epoch 01974: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 237ms/step - loss: 0.0236 - val_loss: 0.0240\n",
      "\n",
      "Epoch 01975: LearningRateScheduler reducing learning rate to 0.0002723329457192754.\n",
      "Epoch 1975/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0291\n",
      "Epoch 01975: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0291 - val_loss: 0.0215\n",
      "\n",
      "Epoch 01976: LearningRateScheduler reducing learning rate to 0.0002723057716827732.\n",
      "Epoch 1976/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0204\n",
      "Epoch 01976: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0204 - val_loss: 0.0218\n",
      "\n",
      "Epoch 01977: LearningRateScheduler reducing learning rate to 0.0002722785856801802.\n",
      "Epoch 1977/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0266\n",
      "Epoch 01977: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0266 - val_loss: 0.0214\n",
      "\n",
      "Epoch 01978: LearningRateScheduler reducing learning rate to 0.0002722513877141892.\n",
      "Epoch 1978/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0315\n",
      "Epoch 01978: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0315 - val_loss: 0.0218\n",
      "\n",
      "Epoch 01979: LearningRateScheduler reducing learning rate to 0.0002722241777874943.\n",
      "Epoch 1979/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0298\n",
      "Epoch 01979: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0298 - val_loss: 0.0234\n",
      "\n",
      "Epoch 01980: LearningRateScheduler reducing learning rate to 0.0002721969559027906.\n",
      "Epoch 1980/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0265\n",
      "Epoch 01980: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0265 - val_loss: 0.0247\n",
      "\n",
      "Epoch 01981: LearningRateScheduler reducing learning rate to 0.00027216972206277454.\n",
      "Epoch 1981/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0275\n",
      "Epoch 01981: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0275 - val_loss: 0.0253\n",
      "\n",
      "Epoch 01982: LearningRateScheduler reducing learning rate to 0.0002721424762701437.\n",
      "Epoch 1982/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0259\n",
      "Epoch 01982: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0259 - val_loss: 0.0258\n",
      "\n",
      "Epoch 01983: LearningRateScheduler reducing learning rate to 0.0002721152185275968.\n",
      "Epoch 1983/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0345\n",
      "Epoch 01983: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0345 - val_loss: 0.0261\n",
      "\n",
      "Epoch 01984: LearningRateScheduler reducing learning rate to 0.0002720879488378339.\n",
      "Epoch 1984/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0427\n",
      "Epoch 01984: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0427 - val_loss: 0.0257\n",
      "\n",
      "Epoch 01985: LearningRateScheduler reducing learning rate to 0.000272060667203556.\n",
      "Epoch 1985/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0215\n",
      "Epoch 01985: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0215 - val_loss: 0.0248\n",
      "\n",
      "Epoch 01986: LearningRateScheduler reducing learning rate to 0.00027203337362746537.\n",
      "Epoch 1986/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0240\n",
      "Epoch 01986: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0240 - val_loss: 0.0247\n",
      "\n",
      "Epoch 01987: LearningRateScheduler reducing learning rate to 0.00027200606811226565.\n",
      "Epoch 1987/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0227\n",
      "Epoch 01987: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0227 - val_loss: 0.0241\n",
      "\n",
      "Epoch 01988: LearningRateScheduler reducing learning rate to 0.00027197875066066143.\n",
      "Epoch 1988/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0244\n",
      "Epoch 01988: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0244 - val_loss: 0.0236\n",
      "\n",
      "Epoch 01989: LearningRateScheduler reducing learning rate to 0.0002719514212753586.\n",
      "Epoch 1989/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0414\n",
      "Epoch 01989: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 240ms/step - loss: 0.0414 - val_loss: 0.0228\n",
      "\n",
      "Epoch 01990: LearningRateScheduler reducing learning rate to 0.0002719240799590642.\n",
      "Epoch 1990/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0292\n",
      "Epoch 01990: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 238ms/step - loss: 0.0292 - val_loss: 0.0220\n",
      "\n",
      "Epoch 01991: LearningRateScheduler reducing learning rate to 0.00027189672671448637.\n",
      "Epoch 1991/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0200\n",
      "Epoch 01991: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0200 - val_loss: 0.0244\n",
      "\n",
      "Epoch 01992: LearningRateScheduler reducing learning rate to 0.0002718693615443346.\n",
      "Epoch 1992/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0301\n",
      "Epoch 01992: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0301 - val_loss: 0.0389\n",
      "\n",
      "Epoch 01993: LearningRateScheduler reducing learning rate to 0.00027184198445131944.\n",
      "Epoch 1993/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0303\n",
      "Epoch 01993: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 241ms/step - loss: 0.0303 - val_loss: 0.0661\n",
      "\n",
      "Epoch 01994: LearningRateScheduler reducing learning rate to 0.00027181459543815267.\n",
      "Epoch 1994/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0262\n",
      "Epoch 01994: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 226ms/step - loss: 0.0262 - val_loss: 0.0904\n",
      "\n",
      "Epoch 01995: LearningRateScheduler reducing learning rate to 0.0002717871945075472.\n",
      "Epoch 1995/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0213\n",
      "Epoch 01995: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0213 - val_loss: 0.0969\n",
      "\n",
      "Epoch 01996: LearningRateScheduler reducing learning rate to 0.0002717597816622172.\n",
      "Epoch 1996/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0216\n",
      "Epoch 01996: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0216 - val_loss: 0.1195\n",
      "\n",
      "Epoch 01997: LearningRateScheduler reducing learning rate to 0.000271732356904878.\n",
      "Epoch 1997/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0245\n",
      "Epoch 01997: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0245 - val_loss: 0.1286\n",
      "\n",
      "Epoch 01998: LearningRateScheduler reducing learning rate to 0.00027170492023824596.\n",
      "Epoch 1998/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0224\n",
      "Epoch 01998: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0224 - val_loss: 0.1140\n",
      "\n",
      "Epoch 01999: LearningRateScheduler reducing learning rate to 0.00027167747166503885.\n",
      "Epoch 1999/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0295\n",
      "Epoch 01999: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0295 - val_loss: 0.0996\n",
      "\n",
      "Epoch 02000: LearningRateScheduler reducing learning rate to 0.00027165001118797553.\n",
      "Epoch 2000/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0260\n",
      "Epoch 02000: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0260 - val_loss: 0.0915\n",
      "\n",
      "Epoch 02001: LearningRateScheduler reducing learning rate to 0.00027162253880977594.\n",
      "Epoch 2001/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0278\n",
      "Epoch 02001: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0278 - val_loss: 0.0773\n",
      "\n",
      "Epoch 02002: LearningRateScheduler reducing learning rate to 0.00027159505453316137.\n",
      "Epoch 2002/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0379\n",
      "Epoch 02002: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0379 - val_loss: 0.0696\n",
      "\n",
      "Epoch 02003: LearningRateScheduler reducing learning rate to 0.0002715675583608541.\n",
      "Epoch 2003/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0262\n",
      "Epoch 02003: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0262 - val_loss: 0.0680\n",
      "\n",
      "Epoch 02004: LearningRateScheduler reducing learning rate to 0.0002715400502955778.\n",
      "Epoch 2004/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0255\n",
      "Epoch 02004: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0255 - val_loss: 0.0669\n",
      "\n",
      "Epoch 02005: LearningRateScheduler reducing learning rate to 0.00027151253034005715.\n",
      "Epoch 2005/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0249\n",
      "Epoch 02005: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 249ms/step - loss: 0.0249 - val_loss: 0.0675\n",
      "\n",
      "Epoch 02006: LearningRateScheduler reducing learning rate to 0.00027148499849701804.\n",
      "Epoch 2006/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0295\n",
      "Epoch 02006: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0295 - val_loss: 0.0663\n",
      "\n",
      "Epoch 02007: LearningRateScheduler reducing learning rate to 0.00027145745476918765.\n",
      "Epoch 2007/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0239\n",
      "Epoch 02007: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 238ms/step - loss: 0.0239 - val_loss: 0.0665\n",
      "\n",
      "Epoch 02008: LearningRateScheduler reducing learning rate to 0.0002714298991592941.\n",
      "Epoch 2008/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0210\n",
      "Epoch 02008: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0210 - val_loss: 0.0592\n",
      "\n",
      "Epoch 02009: LearningRateScheduler reducing learning rate to 0.000271402331670067.\n",
      "Epoch 2009/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0156\n",
      "Epoch 02009: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0156 - val_loss: 0.0508\n",
      "\n",
      "Epoch 02010: LearningRateScheduler reducing learning rate to 0.0002713747523042369.\n",
      "Epoch 2010/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0229\n",
      "Epoch 02010: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0229 - val_loss: 0.0444\n",
      "\n",
      "Epoch 02011: LearningRateScheduler reducing learning rate to 0.0002713471610645356.\n",
      "Epoch 2011/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0344\n",
      "Epoch 02011: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0344 - val_loss: 0.0499\n",
      "\n",
      "Epoch 02012: LearningRateScheduler reducing learning rate to 0.00027131955795369606.\n",
      "Epoch 2012/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0298\n",
      "Epoch 02012: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0298 - val_loss: 0.0537\n",
      "\n",
      "Epoch 02013: LearningRateScheduler reducing learning rate to 0.0002712919429744525.\n",
      "Epoch 2013/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0289\n",
      "Epoch 02013: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0289 - val_loss: 0.0628\n",
      "\n",
      "Epoch 02014: LearningRateScheduler reducing learning rate to 0.0002712643161295401.\n",
      "Epoch 2014/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0225\n",
      "Epoch 02014: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0225 - val_loss: 0.0789\n",
      "\n",
      "Epoch 02015: LearningRateScheduler reducing learning rate to 0.00027123667742169556.\n",
      "Epoch 2015/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0270\n",
      "Epoch 02015: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 239ms/step - loss: 0.0270 - val_loss: 0.1193\n",
      "\n",
      "Epoch 02016: LearningRateScheduler reducing learning rate to 0.0002712090268536564.\n",
      "Epoch 2016/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0375\n",
      "Epoch 02016: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0375 - val_loss: 0.0950\n",
      "\n",
      "Epoch 02017: LearningRateScheduler reducing learning rate to 0.0002711813644281616.\n",
      "Epoch 2017/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0222\n",
      "Epoch 02017: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 238ms/step - loss: 0.0222 - val_loss: 0.0818\n",
      "\n",
      "Epoch 02018: LearningRateScheduler reducing learning rate to 0.00027115369014795105.\n",
      "Epoch 2018/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0242\n",
      "Epoch 02018: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0242 - val_loss: 0.0764\n",
      "\n",
      "Epoch 02019: LearningRateScheduler reducing learning rate to 0.000271126004015766.\n",
      "Epoch 2019/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0180\n",
      "Epoch 02019: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0180 - val_loss: 0.0757\n",
      "\n",
      "Epoch 02020: LearningRateScheduler reducing learning rate to 0.00027109830603434896.\n",
      "Epoch 2020/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0289\n",
      "Epoch 02020: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0289 - val_loss: 0.0657\n",
      "\n",
      "Epoch 02021: LearningRateScheduler reducing learning rate to 0.0002710705962064433.\n",
      "Epoch 2021/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0234\n",
      "Epoch 02021: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0234 - val_loss: 0.0552\n",
      "\n",
      "Epoch 02022: LearningRateScheduler reducing learning rate to 0.0002710428745347938.\n",
      "Epoch 2022/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0311\n",
      "Epoch 02022: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0311 - val_loss: 0.0467\n",
      "\n",
      "Epoch 02023: LearningRateScheduler reducing learning rate to 0.0002710151410221463.\n",
      "Epoch 2023/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0236\n",
      "Epoch 02023: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0236 - val_loss: 0.0392\n",
      "\n",
      "Epoch 02024: LearningRateScheduler reducing learning rate to 0.00027098739567124803.\n",
      "Epoch 2024/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0228\n",
      "Epoch 02024: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0228 - val_loss: 0.0335\n",
      "\n",
      "Epoch 02025: LearningRateScheduler reducing learning rate to 0.0002709596384848471.\n",
      "Epoch 2025/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0307\n",
      "Epoch 02025: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0307 - val_loss: 0.0323\n",
      "\n",
      "Epoch 02026: LearningRateScheduler reducing learning rate to 0.000270931869465693.\n",
      "Epoch 2026/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0263\n",
      "Epoch 02026: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0263 - val_loss: 0.0291\n",
      "\n",
      "Epoch 02027: LearningRateScheduler reducing learning rate to 0.0002709040886165362.\n",
      "Epoch 2027/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0290\n",
      "Epoch 02027: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0290 - val_loss: 0.0243\n",
      "\n",
      "Epoch 02028: LearningRateScheduler reducing learning rate to 0.0002708762959401286.\n",
      "Epoch 2028/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0250\n",
      "Epoch 02028: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0250 - val_loss: 0.0254\n",
      "\n",
      "Epoch 02029: LearningRateScheduler reducing learning rate to 0.000270848491439223.\n",
      "Epoch 2029/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0268\n",
      "Epoch 02029: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0268 - val_loss: 0.0269\n",
      "\n",
      "Epoch 02030: LearningRateScheduler reducing learning rate to 0.00027082067511657355.\n",
      "Epoch 2030/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0300\n",
      "Epoch 02030: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0300 - val_loss: 0.0366\n",
      "\n",
      "Epoch 02031: LearningRateScheduler reducing learning rate to 0.00027079284697493556.\n",
      "Epoch 2031/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0208\n",
      "Epoch 02031: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0208 - val_loss: 0.0553\n",
      "\n",
      "Epoch 02032: LearningRateScheduler reducing learning rate to 0.00027076500701706544.\n",
      "Epoch 2032/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0152\n",
      "Epoch 02032: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0152 - val_loss: 0.0779\n",
      "\n",
      "Epoch 02033: LearningRateScheduler reducing learning rate to 0.00027073715524572085.\n",
      "Epoch 2033/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0260\n",
      "Epoch 02033: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0260 - val_loss: 0.0858\n",
      "\n",
      "Epoch 02034: LearningRateScheduler reducing learning rate to 0.00027070929166366045.\n",
      "Epoch 2034/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0308\n",
      "Epoch 02034: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0308 - val_loss: 0.0811\n",
      "\n",
      "Epoch 02035: LearningRateScheduler reducing learning rate to 0.0002706814162736443.\n",
      "Epoch 2035/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0247\n",
      "Epoch 02035: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0247 - val_loss: 0.0700\n",
      "\n",
      "Epoch 02036: LearningRateScheduler reducing learning rate to 0.00027065352907843345.\n",
      "Epoch 2036/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0384\n",
      "Epoch 02036: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0384 - val_loss: 0.0593\n",
      "\n",
      "Epoch 02037: LearningRateScheduler reducing learning rate to 0.00027062563008079025.\n",
      "Epoch 2037/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0230\n",
      "Epoch 02037: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0230 - val_loss: 0.0554\n",
      "\n",
      "Epoch 02038: LearningRateScheduler reducing learning rate to 0.0002705977192834782.\n",
      "Epoch 2038/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0306\n",
      "Epoch 02038: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0306 - val_loss: 0.0483\n",
      "\n",
      "Epoch 02039: LearningRateScheduler reducing learning rate to 0.00027056979668926183.\n",
      "Epoch 2039/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0299\n",
      "Epoch 02039: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0299 - val_loss: 0.0417\n",
      "\n",
      "Epoch 02040: LearningRateScheduler reducing learning rate to 0.00027054186230090696.\n",
      "Epoch 2040/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0264\n",
      "Epoch 02040: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 240ms/step - loss: 0.0264 - val_loss: 0.0424\n",
      "\n",
      "Epoch 02041: LearningRateScheduler reducing learning rate to 0.0002705139161211806.\n",
      "Epoch 2041/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0328\n",
      "Epoch 02041: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 244ms/step - loss: 0.0328 - val_loss: 0.0462\n",
      "\n",
      "Epoch 02042: LearningRateScheduler reducing learning rate to 0.0002704859581528508.\n",
      "Epoch 2042/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0346\n",
      "Epoch 02042: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0346 - val_loss: 0.0388\n",
      "\n",
      "Epoch 02043: LearningRateScheduler reducing learning rate to 0.000270457988398687.\n",
      "Epoch 2043/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0237\n",
      "Epoch 02043: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 243ms/step - loss: 0.0237 - val_loss: 0.0309\n",
      "\n",
      "Epoch 02044: LearningRateScheduler reducing learning rate to 0.00027043000686145953.\n",
      "Epoch 2044/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0228\n",
      "Epoch 02044: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0228 - val_loss: 0.0259\n",
      "\n",
      "Epoch 02045: LearningRateScheduler reducing learning rate to 0.00027040201354394005.\n",
      "Epoch 2045/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0317\n",
      "Epoch 02045: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0317 - val_loss: 0.0249\n",
      "\n",
      "Epoch 02046: LearningRateScheduler reducing learning rate to 0.00027037400844890145.\n",
      "Epoch 2046/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0447\n",
      "Epoch 02046: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 228ms/step - loss: 0.0447 - val_loss: 0.0234\n",
      "\n",
      "Epoch 02047: LearningRateScheduler reducing learning rate to 0.00027034599157911766.\n",
      "Epoch 2047/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0172\n",
      "Epoch 02047: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0172 - val_loss: 0.0217\n",
      "\n",
      "Epoch 02048: LearningRateScheduler reducing learning rate to 0.0002703179629373638.\n",
      "Epoch 2048/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0263\n",
      "Epoch 02048: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0263 - val_loss: 0.0198\n",
      "\n",
      "Epoch 02049: LearningRateScheduler reducing learning rate to 0.0002702899225264161.\n",
      "Epoch 2049/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0217\n",
      "Epoch 02049: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 237ms/step - loss: 0.0217 - val_loss: 0.0188\n",
      "\n",
      "Epoch 02050: LearningRateScheduler reducing learning rate to 0.0002702618703490522.\n",
      "Epoch 2050/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0294\n",
      "Epoch 02050: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0294 - val_loss: 0.0192\n",
      "\n",
      "Epoch 02051: LearningRateScheduler reducing learning rate to 0.00027023380640805056.\n",
      "Epoch 2051/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0166\n",
      "Epoch 02051: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0166 - val_loss: 0.0190\n",
      "\n",
      "Epoch 02052: LearningRateScheduler reducing learning rate to 0.00027020573070619106.\n",
      "Epoch 2052/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0235\n",
      "Epoch 02052: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0235 - val_loss: 0.0189\n",
      "\n",
      "Epoch 02053: LearningRateScheduler reducing learning rate to 0.0002701776432462547.\n",
      "Epoch 2053/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0184\n",
      "Epoch 02053: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0184 - val_loss: 0.0190\n",
      "\n",
      "Epoch 02054: LearningRateScheduler reducing learning rate to 0.00027014954403102354.\n",
      "Epoch 2054/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0178\n",
      "Epoch 02054: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0178 - val_loss: 0.0198\n",
      "\n",
      "Epoch 02055: LearningRateScheduler reducing learning rate to 0.00027012143306328083.\n",
      "Epoch 2055/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0299\n",
      "Epoch 02055: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0299 - val_loss: 0.0205\n",
      "\n",
      "Epoch 02056: LearningRateScheduler reducing learning rate to 0.0002700933103458111.\n",
      "Epoch 2056/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0180\n",
      "Epoch 02056: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0180 - val_loss: 0.0217\n",
      "\n",
      "Epoch 02057: LearningRateScheduler reducing learning rate to 0.0002700651758814001.\n",
      "Epoch 2057/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0188\n",
      "Epoch 02057: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0188 - val_loss: 0.0221\n",
      "\n",
      "Epoch 02058: LearningRateScheduler reducing learning rate to 0.00027003702967283433.\n",
      "Epoch 2058/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0258\n",
      "Epoch 02058: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 227ms/step - loss: 0.0258 - val_loss: 0.0223\n",
      "\n",
      "Epoch 02059: LearningRateScheduler reducing learning rate to 0.0002700088717229019.\n",
      "Epoch 2059/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0365\n",
      "Epoch 02059: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 241ms/step - loss: 0.0365 - val_loss: 0.0205\n",
      "\n",
      "Epoch 02060: LearningRateScheduler reducing learning rate to 0.00026998070203439195.\n",
      "Epoch 2060/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0182\n",
      "Epoch 02060: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 239ms/step - loss: 0.0182 - val_loss: 0.0182\n",
      "\n",
      "Epoch 02061: LearningRateScheduler reducing learning rate to 0.0002699525206100947.\n",
      "Epoch 2061/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0208\n",
      "Epoch 02061: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0208 - val_loss: 0.0191\n",
      "\n",
      "Epoch 02062: LearningRateScheduler reducing learning rate to 0.0002699243274528015.\n",
      "Epoch 2062/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0175\n",
      "Epoch 02062: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0175 - val_loss: 0.0194\n",
      "\n",
      "Epoch 02063: LearningRateScheduler reducing learning rate to 0.0002698961225653052.\n",
      "Epoch 2063/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0201\n",
      "Epoch 02063: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0201 - val_loss: 0.0193\n",
      "\n",
      "Epoch 02064: LearningRateScheduler reducing learning rate to 0.0002698679059503993.\n",
      "Epoch 2064/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0253\n",
      "Epoch 02064: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0253 - val_loss: 0.0174\n",
      "\n",
      "Epoch 02065: LearningRateScheduler reducing learning rate to 0.0002698396776108788.\n",
      "Epoch 2065/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0146\n",
      "Epoch 02065: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0146 - val_loss: 0.0156\n",
      "\n",
      "Epoch 02066: LearningRateScheduler reducing learning rate to 0.00026981143754953985.\n",
      "Epoch 2066/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0202\n",
      "Epoch 02066: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 237ms/step - loss: 0.0202 - val_loss: 0.0160\n",
      "\n",
      "Epoch 02067: LearningRateScheduler reducing learning rate to 0.00026978318576917956.\n",
      "Epoch 2067/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0244\n",
      "Epoch 02067: val_loss did not improve from 0.01522\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0244 - val_loss: 0.0159\n",
      "\n",
      "Epoch 02068: LearningRateScheduler reducing learning rate to 0.00026975492227259643.\n",
      "Epoch 2068/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0316\n",
      "Epoch 02068: val_loss improved from 0.01522 to 0.01449, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 10s 3s/step - loss: 0.0316 - val_loss: 0.0145\n",
      "\n",
      "Epoch 02069: LearningRateScheduler reducing learning rate to 0.0002697266470625901.\n",
      "Epoch 2069/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0239\n",
      "Epoch 02069: val_loss did not improve from 0.01449\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0239 - val_loss: 0.0159\n",
      "\n",
      "Epoch 02070: LearningRateScheduler reducing learning rate to 0.00026969836014196114.\n",
      "Epoch 2070/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0209\n",
      "Epoch 02070: val_loss did not improve from 0.01449\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0209 - val_loss: 0.0188\n",
      "\n",
      "Epoch 02071: LearningRateScheduler reducing learning rate to 0.00026967006151351153.\n",
      "Epoch 2071/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0158\n",
      "Epoch 02071: val_loss did not improve from 0.01449\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0158 - val_loss: 0.0211\n",
      "\n",
      "Epoch 02072: LearningRateScheduler reducing learning rate to 0.00026964175118004426.\n",
      "Epoch 2072/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0209\n",
      "Epoch 02072: val_loss did not improve from 0.01449\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0209 - val_loss: 0.0216\n",
      "\n",
      "Epoch 02073: LearningRateScheduler reducing learning rate to 0.0002696134291443636.\n",
      "Epoch 2073/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0280\n",
      "Epoch 02073: val_loss did not improve from 0.01449\n",
      "3/3 [==============================] - 1s 238ms/step - loss: 0.0280 - val_loss: 0.0220\n",
      "\n",
      "Epoch 02074: LearningRateScheduler reducing learning rate to 0.0002695850954092748.\n",
      "Epoch 2074/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0176\n",
      "Epoch 02074: val_loss did not improve from 0.01449\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0176 - val_loss: 0.0241\n",
      "\n",
      "Epoch 02075: LearningRateScheduler reducing learning rate to 0.0002695567499775845.\n",
      "Epoch 2075/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0253\n",
      "Epoch 02075: val_loss did not improve from 0.01449\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0253 - val_loss: 0.0253\n",
      "\n",
      "Epoch 02076: LearningRateScheduler reducing learning rate to 0.00026952839285210035.\n",
      "Epoch 2076/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0198\n",
      "Epoch 02076: val_loss did not improve from 0.01449\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0198 - val_loss: 0.0259\n",
      "\n",
      "Epoch 02077: LearningRateScheduler reducing learning rate to 0.0002695000240356311.\n",
      "Epoch 2077/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0276\n",
      "Epoch 02077: val_loss did not improve from 0.01449\n",
      "3/3 [==============================] - 1s 240ms/step - loss: 0.0276 - val_loss: 0.0253\n",
      "\n",
      "Epoch 02078: LearningRateScheduler reducing learning rate to 0.0002694716435309869.\n",
      "Epoch 2078/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0286\n",
      "Epoch 02078: val_loss did not improve from 0.01449\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0286 - val_loss: 0.0224\n",
      "\n",
      "Epoch 02079: LearningRateScheduler reducing learning rate to 0.0002694432513409788.\n",
      "Epoch 2079/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0180\n",
      "Epoch 02079: val_loss did not improve from 0.01449\n",
      "3/3 [==============================] - 1s 238ms/step - loss: 0.0180 - val_loss: 0.0192\n",
      "\n",
      "Epoch 02080: LearningRateScheduler reducing learning rate to 0.00026941484746841916.\n",
      "Epoch 2080/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0319\n",
      "Epoch 02080: val_loss did not improve from 0.01449\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0319 - val_loss: 0.0179\n",
      "\n",
      "Epoch 02081: LearningRateScheduler reducing learning rate to 0.0002693864319161214.\n",
      "Epoch 2081/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0259\n",
      "Epoch 02081: val_loss did not improve from 0.01449\n",
      "3/3 [==============================] - 1s 238ms/step - loss: 0.0259 - val_loss: 0.0167\n",
      "\n",
      "Epoch 02082: LearningRateScheduler reducing learning rate to 0.00026935800468690015.\n",
      "Epoch 2082/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0187\n",
      "Epoch 02082: val_loss did not improve from 0.01449\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0187 - val_loss: 0.0166\n",
      "\n",
      "Epoch 02083: LearningRateScheduler reducing learning rate to 0.00026932956578357126.\n",
      "Epoch 2083/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0231\n",
      "Epoch 02083: val_loss did not improve from 0.01449\n",
      "3/3 [==============================] - 1s 237ms/step - loss: 0.0231 - val_loss: 0.0163\n",
      "\n",
      "Epoch 02084: LearningRateScheduler reducing learning rate to 0.0002693011152089516.\n",
      "Epoch 2084/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0364\n",
      "Epoch 02084: val_loss did not improve from 0.01449\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0364 - val_loss: 0.0175\n",
      "\n",
      "Epoch 02085: LearningRateScheduler reducing learning rate to 0.00026927265296585926.\n",
      "Epoch 2085/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0299\n",
      "Epoch 02085: val_loss did not improve from 0.01449\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0299 - val_loss: 0.0178\n",
      "\n",
      "Epoch 02086: LearningRateScheduler reducing learning rate to 0.00026924417905711356.\n",
      "Epoch 2086/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0256\n",
      "Epoch 02086: val_loss did not improve from 0.01449\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0256 - val_loss: 0.0193\n",
      "\n",
      "Epoch 02087: LearningRateScheduler reducing learning rate to 0.00026921569348553484.\n",
      "Epoch 2087/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0317\n",
      "Epoch 02087: val_loss did not improve from 0.01449\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0317 - val_loss: 0.0192\n",
      "\n",
      "Epoch 02088: LearningRateScheduler reducing learning rate to 0.00026918719625394473.\n",
      "Epoch 2088/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0340\n",
      "Epoch 02088: val_loss did not improve from 0.01449\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0340 - val_loss: 0.0192\n",
      "\n",
      "Epoch 02089: LearningRateScheduler reducing learning rate to 0.00026915868736516584.\n",
      "Epoch 2089/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0209\n",
      "Epoch 02089: val_loss did not improve from 0.01449\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0209 - val_loss: 0.0185\n",
      "\n",
      "Epoch 02090: LearningRateScheduler reducing learning rate to 0.0002691301668220222.\n",
      "Epoch 2090/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0296\n",
      "Epoch 02090: val_loss did not improve from 0.01449\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0296 - val_loss: 0.0192\n",
      "\n",
      "Epoch 02091: LearningRateScheduler reducing learning rate to 0.0002691016346273387.\n",
      "Epoch 2091/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0224\n",
      "Epoch 02091: val_loss did not improve from 0.01449\n",
      "3/3 [==============================] - 1s 241ms/step - loss: 0.0224 - val_loss: 0.0209\n",
      "\n",
      "Epoch 02092: LearningRateScheduler reducing learning rate to 0.0002690730907839415.\n",
      "Epoch 2092/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0279\n",
      "Epoch 02092: val_loss did not improve from 0.01449\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0279 - val_loss: 0.0226\n",
      "\n",
      "Epoch 02093: LearningRateScheduler reducing learning rate to 0.00026904453529465807.\n",
      "Epoch 2093/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0193\n",
      "Epoch 02093: val_loss did not improve from 0.01449\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0193 - val_loss: 0.0229\n",
      "\n",
      "Epoch 02094: LearningRateScheduler reducing learning rate to 0.00026901596816231675.\n",
      "Epoch 2094/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0247\n",
      "Epoch 02094: val_loss did not improve from 0.01449\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0247 - val_loss: 0.0222\n",
      "\n",
      "Epoch 02095: LearningRateScheduler reducing learning rate to 0.0002689873893897473.\n",
      "Epoch 2095/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0226\n",
      "Epoch 02095: val_loss did not improve from 0.01449\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0226 - val_loss: 0.0201\n",
      "\n",
      "Epoch 02096: LearningRateScheduler reducing learning rate to 0.0002689587989797805.\n",
      "Epoch 2096/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0171\n",
      "Epoch 02096: val_loss did not improve from 0.01449\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0171 - val_loss: 0.0181\n",
      "\n",
      "Epoch 02097: LearningRateScheduler reducing learning rate to 0.0002689301969352482.\n",
      "Epoch 2097/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0222\n",
      "Epoch 02097: val_loss did not improve from 0.01449\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0222 - val_loss: 0.0168\n",
      "\n",
      "Epoch 02098: LearningRateScheduler reducing learning rate to 0.0002689015832589836.\n",
      "Epoch 2098/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0339\n",
      "Epoch 02098: val_loss did not improve from 0.01449\n",
      "3/3 [==============================] - 1s 237ms/step - loss: 0.0339 - val_loss: 0.0167\n",
      "\n",
      "Epoch 02099: LearningRateScheduler reducing learning rate to 0.0002688729579538209.\n",
      "Epoch 2099/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0284\n",
      "Epoch 02099: val_loss did not improve from 0.01449\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0284 - val_loss: 0.0160\n",
      "\n",
      "Epoch 02100: LearningRateScheduler reducing learning rate to 0.0002688443210225955.\n",
      "Epoch 2100/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0225\n",
      "Epoch 02100: val_loss did not improve from 0.01449\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0225 - val_loss: 0.0157\n",
      "\n",
      "Epoch 02101: LearningRateScheduler reducing learning rate to 0.000268815672468144.\n",
      "Epoch 2101/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0269\n",
      "Epoch 02101: val_loss did not improve from 0.01449\n",
      "3/3 [==============================] - 1s 239ms/step - loss: 0.0269 - val_loss: 0.0163\n",
      "\n",
      "Epoch 02102: LearningRateScheduler reducing learning rate to 0.0002687870122933041.\n",
      "Epoch 2102/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0230\n",
      "Epoch 02102: val_loss did not improve from 0.01449\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0230 - val_loss: 0.0164\n",
      "\n",
      "Epoch 02103: LearningRateScheduler reducing learning rate to 0.0002687583405009146.\n",
      "Epoch 2103/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0197\n",
      "Epoch 02103: val_loss did not improve from 0.01449\n",
      "3/3 [==============================] - 1s 227ms/step - loss: 0.0197 - val_loss: 0.0157\n",
      "\n",
      "Epoch 02104: LearningRateScheduler reducing learning rate to 0.0002687296570938155.\n",
      "Epoch 2104/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0247\n",
      "Epoch 02104: val_loss did not improve from 0.01449\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0247 - val_loss: 0.0154\n",
      "\n",
      "Epoch 02105: LearningRateScheduler reducing learning rate to 0.00026870096207484804.\n",
      "Epoch 2105/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0318\n",
      "Epoch 02105: val_loss did not improve from 0.01449\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0318 - val_loss: 0.0160\n",
      "\n",
      "Epoch 02106: LearningRateScheduler reducing learning rate to 0.00026867225544685444.\n",
      "Epoch 2106/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0190\n",
      "Epoch 02106: val_loss did not improve from 0.01449\n",
      "3/3 [==============================] - 1s 246ms/step - loss: 0.0190 - val_loss: 0.0158\n",
      "\n",
      "Epoch 02107: LearningRateScheduler reducing learning rate to 0.0002686435372126782.\n",
      "Epoch 2107/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0232\n",
      "Epoch 02107: val_loss did not improve from 0.01449\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0232 - val_loss: 0.0170\n",
      "\n",
      "Epoch 02108: LearningRateScheduler reducing learning rate to 0.000268614807375164.\n",
      "Epoch 2108/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0229\n",
      "Epoch 02108: val_loss did not improve from 0.01449\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0229 - val_loss: 0.0183\n",
      "\n",
      "Epoch 02109: LearningRateScheduler reducing learning rate to 0.00026858606593715745.\n",
      "Epoch 2109/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0144\n",
      "Epoch 02109: val_loss did not improve from 0.01449\n",
      "3/3 [==============================] - 1s 240ms/step - loss: 0.0144 - val_loss: 0.0171\n",
      "\n",
      "Epoch 02110: LearningRateScheduler reducing learning rate to 0.0002685573129015056.\n",
      "Epoch 2110/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0242\n",
      "Epoch 02110: val_loss did not improve from 0.01449\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0242 - val_loss: 0.0163\n",
      "\n",
      "Epoch 02111: LearningRateScheduler reducing learning rate to 0.0002685285482710564.\n",
      "Epoch 2111/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0205\n",
      "Epoch 02111: val_loss did not improve from 0.01449\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0205 - val_loss: 0.0175\n",
      "\n",
      "Epoch 02112: LearningRateScheduler reducing learning rate to 0.00026849977204865906.\n",
      "Epoch 2112/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0264\n",
      "Epoch 02112: val_loss did not improve from 0.01449\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0264 - val_loss: 0.0172\n",
      "\n",
      "Epoch 02113: LearningRateScheduler reducing learning rate to 0.00026847098423716403.\n",
      "Epoch 2113/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0291\n",
      "Epoch 02113: val_loss did not improve from 0.01449\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0291 - val_loss: 0.0176\n",
      "\n",
      "Epoch 02114: LearningRateScheduler reducing learning rate to 0.0002684421848394227.\n",
      "Epoch 2114/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0268\n",
      "Epoch 02114: val_loss did not improve from 0.01449\n",
      "3/3 [==============================] - 1s 227ms/step - loss: 0.0268 - val_loss: 0.0187\n",
      "\n",
      "Epoch 02115: LearningRateScheduler reducing learning rate to 0.0002684133738582877.\n",
      "Epoch 2115/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0274\n",
      "Epoch 02115: val_loss did not improve from 0.01449\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0274 - val_loss: 0.0217\n",
      "\n",
      "Epoch 02116: LearningRateScheduler reducing learning rate to 0.000268384551296613.\n",
      "Epoch 2116/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0243\n",
      "Epoch 02116: val_loss did not improve from 0.01449\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0243 - val_loss: 0.0232\n",
      "\n",
      "Epoch 02117: LearningRateScheduler reducing learning rate to 0.00026835571715725336.\n",
      "Epoch 2117/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0292\n",
      "Epoch 02117: val_loss did not improve from 0.01449\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0292 - val_loss: 0.0229\n",
      "\n",
      "Epoch 02118: LearningRateScheduler reducing learning rate to 0.0002683268714430649.\n",
      "Epoch 2118/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0182\n",
      "Epoch 02118: val_loss did not improve from 0.01449\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0182 - val_loss: 0.0219\n",
      "\n",
      "Epoch 02119: LearningRateScheduler reducing learning rate to 0.0002682980141569049.\n",
      "Epoch 2119/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0229\n",
      "Epoch 02119: val_loss did not improve from 0.01449\n",
      "3/3 [==============================] - 1s 228ms/step - loss: 0.0229 - val_loss: 0.0212\n",
      "\n",
      "Epoch 02120: LearningRateScheduler reducing learning rate to 0.0002682691453016318.\n",
      "Epoch 2120/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0196\n",
      "Epoch 02120: val_loss did not improve from 0.01449\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0196 - val_loss: 0.0210\n",
      "\n",
      "Epoch 02121: LearningRateScheduler reducing learning rate to 0.00026824026488010493.\n",
      "Epoch 2121/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0206\n",
      "Epoch 02121: val_loss did not improve from 0.01449\n",
      "3/3 [==============================] - 1s 237ms/step - loss: 0.0206 - val_loss: 0.0227\n",
      "\n",
      "Epoch 02122: LearningRateScheduler reducing learning rate to 0.0002682113728951852.\n",
      "Epoch 2122/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0209\n",
      "Epoch 02122: val_loss did not improve from 0.01449\n",
      "3/3 [==============================] - 1s 227ms/step - loss: 0.0209 - val_loss: 0.0248\n",
      "\n",
      "Epoch 02123: LearningRateScheduler reducing learning rate to 0.0002681824693497342.\n",
      "Epoch 2123/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0196\n",
      "Epoch 02123: val_loss did not improve from 0.01449\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0196 - val_loss: 0.0262\n",
      "\n",
      "Epoch 02124: LearningRateScheduler reducing learning rate to 0.0002681535542466151.\n",
      "Epoch 2124/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0299\n",
      "Epoch 02124: val_loss did not improve from 0.01449\n",
      "3/3 [==============================] - 1s 237ms/step - loss: 0.0299 - val_loss: 0.0280\n",
      "\n",
      "Epoch 02125: LearningRateScheduler reducing learning rate to 0.0002681246275886919.\n",
      "Epoch 2125/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0288\n",
      "Epoch 02125: val_loss did not improve from 0.01449\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0288 - val_loss: 0.0283\n",
      "\n",
      "Epoch 02126: LearningRateScheduler reducing learning rate to 0.00026809568937882983.\n",
      "Epoch 2126/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0264\n",
      "Epoch 02126: val_loss did not improve from 0.01449\n",
      "3/3 [==============================] - 1s 228ms/step - loss: 0.0264 - val_loss: 0.0260\n",
      "\n",
      "Epoch 02127: LearningRateScheduler reducing learning rate to 0.00026806673961989535.\n",
      "Epoch 2127/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0223\n",
      "Epoch 02127: val_loss did not improve from 0.01449\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0223 - val_loss: 0.0225\n",
      "\n",
      "Epoch 02128: LearningRateScheduler reducing learning rate to 0.000268037778314756.\n",
      "Epoch 2128/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0180\n",
      "Epoch 02128: val_loss did not improve from 0.01449\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0180 - val_loss: 0.0206\n",
      "\n",
      "Epoch 02129: LearningRateScheduler reducing learning rate to 0.0002680088054662804.\n",
      "Epoch 2129/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0199\n",
      "Epoch 02129: val_loss did not improve from 0.01449\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0199 - val_loss: 0.0190\n",
      "\n",
      "Epoch 02130: LearningRateScheduler reducing learning rate to 0.0002679798210773384.\n",
      "Epoch 2130/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0177\n",
      "Epoch 02130: val_loss did not improve from 0.01449\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0177 - val_loss: 0.0198\n",
      "\n",
      "Epoch 02131: LearningRateScheduler reducing learning rate to 0.000267950825150801.\n",
      "Epoch 2131/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0316\n",
      "Epoch 02131: val_loss did not improve from 0.01449\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0316 - val_loss: 0.0214\n",
      "\n",
      "Epoch 02132: LearningRateScheduler reducing learning rate to 0.00026792181768954033.\n",
      "Epoch 2132/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0256\n",
      "Epoch 02132: val_loss did not improve from 0.01449\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0256 - val_loss: 0.0208\n",
      "\n",
      "Epoch 02133: LearningRateScheduler reducing learning rate to 0.00026789279869642957.\n",
      "Epoch 2133/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0251\n",
      "Epoch 02133: val_loss did not improve from 0.01449\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0251 - val_loss: 0.0195\n",
      "\n",
      "Epoch 02134: LearningRateScheduler reducing learning rate to 0.0002678637681743432.\n",
      "Epoch 2134/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0191\n",
      "Epoch 02134: val_loss did not improve from 0.01449\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0191 - val_loss: 0.0198\n",
      "\n",
      "Epoch 02135: LearningRateScheduler reducing learning rate to 0.0002678347261261567.\n",
      "Epoch 2135/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0173\n",
      "Epoch 02135: val_loss did not improve from 0.01449\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0173 - val_loss: 0.0214\n",
      "\n",
      "Epoch 02136: LearningRateScheduler reducing learning rate to 0.0002678056725547468.\n",
      "Epoch 2136/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0235\n",
      "Epoch 02136: val_loss did not improve from 0.01449\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0235 - val_loss: 0.0230\n",
      "\n",
      "Epoch 02137: LearningRateScheduler reducing learning rate to 0.00026777660746299126.\n",
      "Epoch 2137/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0182\n",
      "Epoch 02137: val_loss did not improve from 0.01449\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0182 - val_loss: 0.0268\n",
      "\n",
      "Epoch 02138: LearningRateScheduler reducing learning rate to 0.0002677475308537691.\n",
      "Epoch 2138/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0279\n",
      "Epoch 02138: val_loss did not improve from 0.01449\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0279 - val_loss: 0.0272\n",
      "\n",
      "Epoch 02139: LearningRateScheduler reducing learning rate to 0.0002677184427299604.\n",
      "Epoch 2139/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0290\n",
      "Epoch 02139: val_loss did not improve from 0.01449\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0290 - val_loss: 0.0250\n",
      "\n",
      "Epoch 02140: LearningRateScheduler reducing learning rate to 0.00026768934309444645.\n",
      "Epoch 2140/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0281\n",
      "Epoch 02140: val_loss did not improve from 0.01449\n",
      "3/3 [==============================] - 1s 238ms/step - loss: 0.0281 - val_loss: 0.0233\n",
      "\n",
      "Epoch 02141: LearningRateScheduler reducing learning rate to 0.0002676602319501096.\n",
      "Epoch 2141/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0375\n",
      "Epoch 02141: val_loss did not improve from 0.01449\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0375 - val_loss: 0.0221\n",
      "\n",
      "Epoch 02142: LearningRateScheduler reducing learning rate to 0.0002676311092998334.\n",
      "Epoch 2142/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0185\n",
      "Epoch 02142: val_loss did not improve from 0.01449\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0185 - val_loss: 0.0226\n",
      "\n",
      "Epoch 02143: LearningRateScheduler reducing learning rate to 0.0002676019751465024.\n",
      "Epoch 2143/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0222\n",
      "Epoch 02143: val_loss did not improve from 0.01449\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0222 - val_loss: 0.0238\n",
      "\n",
      "Epoch 02144: LearningRateScheduler reducing learning rate to 0.00026757282949300254.\n",
      "Epoch 2144/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0208\n",
      "Epoch 02144: val_loss did not improve from 0.01449\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0208 - val_loss: 0.0250\n",
      "\n",
      "Epoch 02145: LearningRateScheduler reducing learning rate to 0.00026754367234222077.\n",
      "Epoch 2145/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0180\n",
      "Epoch 02145: val_loss did not improve from 0.01449\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0180 - val_loss: 0.0250\n",
      "\n",
      "Epoch 02146: LearningRateScheduler reducing learning rate to 0.00026751450369704507.\n",
      "Epoch 2146/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0248\n",
      "Epoch 02146: val_loss did not improve from 0.01449\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0248 - val_loss: 0.0242\n",
      "\n",
      "Epoch 02147: LearningRateScheduler reducing learning rate to 0.00026748532356036473.\n",
      "Epoch 2147/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0204\n",
      "Epoch 02147: val_loss did not improve from 0.01449\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0204 - val_loss: 0.0227\n",
      "\n",
      "Epoch 02148: LearningRateScheduler reducing learning rate to 0.00026745613193507014.\n",
      "Epoch 2148/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0222\n",
      "Epoch 02148: val_loss did not improve from 0.01449\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0222 - val_loss: 0.0215\n",
      "\n",
      "Epoch 02149: LearningRateScheduler reducing learning rate to 0.0002674269288240527.\n",
      "Epoch 2149/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0129\n",
      "Epoch 02149: val_loss did not improve from 0.01449\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0129 - val_loss: 0.0198\n",
      "\n",
      "Epoch 02150: LearningRateScheduler reducing learning rate to 0.00026739771423020515.\n",
      "Epoch 2150/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0180\n",
      "Epoch 02150: val_loss did not improve from 0.01449\n",
      "3/3 [==============================] - 1s 242ms/step - loss: 0.0180 - val_loss: 0.0188\n",
      "\n",
      "Epoch 02151: LearningRateScheduler reducing learning rate to 0.00026736848815642113.\n",
      "Epoch 2151/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0183\n",
      "Epoch 02151: val_loss did not improve from 0.01449\n",
      "3/3 [==============================] - 1s 237ms/step - loss: 0.0183 - val_loss: 0.0187\n",
      "\n",
      "Epoch 02152: LearningRateScheduler reducing learning rate to 0.0002673392506055957.\n",
      "Epoch 2152/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0164\n",
      "Epoch 02152: val_loss did not improve from 0.01449\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0164 - val_loss: 0.0183\n",
      "\n",
      "Epoch 02153: LearningRateScheduler reducing learning rate to 0.00026731000158062483.\n",
      "Epoch 2153/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0233\n",
      "Epoch 02153: val_loss did not improve from 0.01449\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0233 - val_loss: 0.0195\n",
      "\n",
      "Epoch 02154: LearningRateScheduler reducing learning rate to 0.0002672807410844057.\n",
      "Epoch 2154/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0176\n",
      "Epoch 02154: val_loss did not improve from 0.01449\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0176 - val_loss: 0.0205\n",
      "\n",
      "Epoch 02155: LearningRateScheduler reducing learning rate to 0.00026725146911983665.\n",
      "Epoch 2155/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0206\n",
      "Epoch 02155: val_loss did not improve from 0.01449\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0206 - val_loss: 0.0218\n",
      "\n",
      "Epoch 02156: LearningRateScheduler reducing learning rate to 0.0002672221856898172.\n",
      "Epoch 2156/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0296\n",
      "Epoch 02156: val_loss did not improve from 0.01449\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0296 - val_loss: 0.0236\n",
      "\n",
      "Epoch 02157: LearningRateScheduler reducing learning rate to 0.00026719289079724773.\n",
      "Epoch 2157/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0251\n",
      "Epoch 02157: val_loss did not improve from 0.01449\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0251 - val_loss: 0.0237\n",
      "\n",
      "Epoch 02158: LearningRateScheduler reducing learning rate to 0.00026716358444503024.\n",
      "Epoch 2158/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0250\n",
      "Epoch 02158: val_loss did not improve from 0.01449\n",
      "3/3 [==============================] - 1s 242ms/step - loss: 0.0250 - val_loss: 0.0242\n",
      "\n",
      "Epoch 02159: LearningRateScheduler reducing learning rate to 0.0002671342666360674.\n",
      "Epoch 2159/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0218\n",
      "Epoch 02159: val_loss did not improve from 0.01449\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0218 - val_loss: 0.0247\n",
      "\n",
      "Epoch 02160: LearningRateScheduler reducing learning rate to 0.00026710493737326326.\n",
      "Epoch 2160/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0143\n",
      "Epoch 02160: val_loss did not improve from 0.01449\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0143 - val_loss: 0.0246\n",
      "\n",
      "Epoch 02161: LearningRateScheduler reducing learning rate to 0.000267075596659523.\n",
      "Epoch 2161/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0387\n",
      "Epoch 02161: val_loss did not improve from 0.01449\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0387 - val_loss: 0.0241\n",
      "\n",
      "Epoch 02162: LearningRateScheduler reducing learning rate to 0.00026704624449775283.\n",
      "Epoch 2162/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0160\n",
      "Epoch 02162: val_loss did not improve from 0.01449\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0160 - val_loss: 0.0240\n",
      "\n",
      "Epoch 02163: LearningRateScheduler reducing learning rate to 0.0002670168808908601.\n",
      "Epoch 2163/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0203\n",
      "Epoch 02163: val_loss did not improve from 0.01449\n",
      "3/3 [==============================] - 1s 238ms/step - loss: 0.0203 - val_loss: 0.0234\n",
      "\n",
      "Epoch 02164: LearningRateScheduler reducing learning rate to 0.0002669875058417535.\n",
      "Epoch 2164/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0213\n",
      "Epoch 02164: val_loss did not improve from 0.01449\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0213 - val_loss: 0.0222\n",
      "\n",
      "Epoch 02165: LearningRateScheduler reducing learning rate to 0.0002669581193533425.\n",
      "Epoch 2165/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0221\n",
      "Epoch 02165: val_loss did not improve from 0.01449\n",
      "3/3 [==============================] - 1s 250ms/step - loss: 0.0221 - val_loss: 0.0212\n",
      "\n",
      "Epoch 02166: LearningRateScheduler reducing learning rate to 0.0002669287214285381.\n",
      "Epoch 2166/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0334\n",
      "Epoch 02166: val_loss did not improve from 0.01449\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0334 - val_loss: 0.0195\n",
      "\n",
      "Epoch 02167: LearningRateScheduler reducing learning rate to 0.00026689931207025215.\n",
      "Epoch 2167/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0191\n",
      "Epoch 02167: val_loss did not improve from 0.01449\n",
      "3/3 [==============================] - 1s 228ms/step - loss: 0.0191 - val_loss: 0.0157\n",
      "\n",
      "Epoch 02168: LearningRateScheduler reducing learning rate to 0.00026686989128139757.\n",
      "Epoch 2168/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0232\n",
      "Epoch 02168: val_loss did not improve from 0.01449\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0232 - val_loss: 0.0145\n",
      "\n",
      "Epoch 02169: LearningRateScheduler reducing learning rate to 0.0002668404590648888.\n",
      "Epoch 2169/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0269\n",
      "Epoch 02169: val_loss improved from 0.01449 to 0.01360, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 10s 3s/step - loss: 0.0269 - val_loss: 0.0136\n",
      "\n",
      "Epoch 02170: LearningRateScheduler reducing learning rate to 0.00026681101542364103.\n",
      "Epoch 2170/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0229\n",
      "Epoch 02170: val_loss improved from 0.01360 to 0.01270, saving model to logs\\best_epoch_weights.h5\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.0229 - val_loss: 0.0127\n",
      "\n",
      "Epoch 02171: LearningRateScheduler reducing learning rate to 0.0002667815603605707.\n",
      "Epoch 2171/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0290\n",
      "Epoch 02171: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0290 - val_loss: 0.0136\n",
      "\n",
      "Epoch 02172: LearningRateScheduler reducing learning rate to 0.0002667520938785955.\n",
      "Epoch 2172/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0186\n",
      "Epoch 02172: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0186 - val_loss: 0.0147\n",
      "\n",
      "Epoch 02173: LearningRateScheduler reducing learning rate to 0.00026672261598063407.\n",
      "Epoch 2173/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0252\n",
      "Epoch 02173: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0252 - val_loss: 0.0152\n",
      "\n",
      "Epoch 02174: LearningRateScheduler reducing learning rate to 0.0002666931266696063.\n",
      "Epoch 2174/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0250\n",
      "Epoch 02174: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0250 - val_loss: 0.0155\n",
      "\n",
      "Epoch 02175: LearningRateScheduler reducing learning rate to 0.0002666636259484332.\n",
      "Epoch 2175/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0300\n",
      "Epoch 02175: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0300 - val_loss: 0.0173\n",
      "\n",
      "Epoch 02176: LearningRateScheduler reducing learning rate to 0.0002666341138200368.\n",
      "Epoch 2176/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0327\n",
      "Epoch 02176: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0327 - val_loss: 0.0180\n",
      "\n",
      "Epoch 02177: LearningRateScheduler reducing learning rate to 0.0002666045902873405.\n",
      "Epoch 2177/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0181\n",
      "Epoch 02177: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0181 - val_loss: 0.0184\n",
      "\n",
      "Epoch 02178: LearningRateScheduler reducing learning rate to 0.0002665750553532685.\n",
      "Epoch 2178/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0206\n",
      "Epoch 02178: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0206 - val_loss: 0.0276\n",
      "\n",
      "Epoch 02179: LearningRateScheduler reducing learning rate to 0.0002665455090207464.\n",
      "Epoch 2179/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0223\n",
      "Epoch 02179: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0223 - val_loss: 0.0339\n",
      "\n",
      "Epoch 02180: LearningRateScheduler reducing learning rate to 0.00026651595129270075.\n",
      "Epoch 2180/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0169\n",
      "Epoch 02180: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0169 - val_loss: 0.0340\n",
      "\n",
      "Epoch 02181: LearningRateScheduler reducing learning rate to 0.00026648638217205943.\n",
      "Epoch 2181/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0172\n",
      "Epoch 02181: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0172 - val_loss: 0.0299\n",
      "\n",
      "Epoch 02182: LearningRateScheduler reducing learning rate to 0.00026645680166175124.\n",
      "Epoch 2182/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0211\n",
      "Epoch 02182: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0211 - val_loss: 0.0279\n",
      "\n",
      "Epoch 02183: LearningRateScheduler reducing learning rate to 0.0002664272097647062.\n",
      "Epoch 2183/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0334\n",
      "Epoch 02183: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0334 - val_loss: 0.0299\n",
      "\n",
      "Epoch 02184: LearningRateScheduler reducing learning rate to 0.00026639760648385557.\n",
      "Epoch 2184/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0198\n",
      "Epoch 02184: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0198 - val_loss: 0.0352\n",
      "\n",
      "Epoch 02185: LearningRateScheduler reducing learning rate to 0.0002663679918221315.\n",
      "Epoch 2185/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0265\n",
      "Epoch 02185: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0265 - val_loss: 0.0402\n",
      "\n",
      "Epoch 02186: LearningRateScheduler reducing learning rate to 0.0002663383657824674.\n",
      "Epoch 2186/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0170\n",
      "Epoch 02186: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0170 - val_loss: 0.0420\n",
      "\n",
      "Epoch 02187: LearningRateScheduler reducing learning rate to 0.00026630872836779786.\n",
      "Epoch 2187/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0306\n",
      "Epoch 02187: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0306 - val_loss: 0.0413\n",
      "\n",
      "Epoch 02188: LearningRateScheduler reducing learning rate to 0.0002662790795810585.\n",
      "Epoch 2188/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0264\n",
      "Epoch 02188: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0264 - val_loss: 0.0374\n",
      "\n",
      "Epoch 02189: LearningRateScheduler reducing learning rate to 0.0002662494194251861.\n",
      "Epoch 2189/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0172\n",
      "Epoch 02189: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0172 - val_loss: 0.0364\n",
      "\n",
      "Epoch 02190: LearningRateScheduler reducing learning rate to 0.0002662197479031185.\n",
      "Epoch 2190/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0370\n",
      "Epoch 02190: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0370 - val_loss: 0.0347\n",
      "\n",
      "Epoch 02191: LearningRateScheduler reducing learning rate to 0.00026619006501779483.\n",
      "Epoch 2191/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0275\n",
      "Epoch 02191: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0275 - val_loss: 0.0317\n",
      "\n",
      "Epoch 02192: LearningRateScheduler reducing learning rate to 0.0002661603707721553.\n",
      "Epoch 2192/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0257\n",
      "Epoch 02192: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0257 - val_loss: 0.0262\n",
      "\n",
      "Epoch 02193: LearningRateScheduler reducing learning rate to 0.000266130665169141.\n",
      "Epoch 2193/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0207\n",
      "Epoch 02193: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 237ms/step - loss: 0.0207 - val_loss: 0.0238\n",
      "\n",
      "Epoch 02194: LearningRateScheduler reducing learning rate to 0.0002661009482116945.\n",
      "Epoch 2194/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0178\n",
      "Epoch 02194: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0178 - val_loss: 0.0224\n",
      "\n",
      "Epoch 02195: LearningRateScheduler reducing learning rate to 0.0002660712199027593.\n",
      "Epoch 2195/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0227\n",
      "Epoch 02195: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0227 - val_loss: 0.0215\n",
      "\n",
      "Epoch 02196: LearningRateScheduler reducing learning rate to 0.00026604148024528.\n",
      "Epoch 2196/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0303\n",
      "Epoch 02196: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 228ms/step - loss: 0.0303 - val_loss: 0.0207\n",
      "\n",
      "Epoch 02197: LearningRateScheduler reducing learning rate to 0.0002660117292422025.\n",
      "Epoch 2197/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0320\n",
      "Epoch 02197: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0320 - val_loss: 0.0244\n",
      "\n",
      "Epoch 02198: LearningRateScheduler reducing learning rate to 0.0002659819668964736.\n",
      "Epoch 2198/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0371\n",
      "Epoch 02198: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0371 - val_loss: 0.0284\n",
      "\n",
      "Epoch 02199: LearningRateScheduler reducing learning rate to 0.0002659521932110413.\n",
      "Epoch 2199/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0220\n",
      "Epoch 02199: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 242ms/step - loss: 0.0220 - val_loss: 0.0282\n",
      "\n",
      "Epoch 02200: LearningRateScheduler reducing learning rate to 0.00026592240818885493.\n",
      "Epoch 2200/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0186\n",
      "Epoch 02200: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0186 - val_loss: 0.0262\n",
      "\n",
      "Epoch 02201: LearningRateScheduler reducing learning rate to 0.00026589261183286453.\n",
      "Epoch 2201/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0212\n",
      "Epoch 02201: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 237ms/step - loss: 0.0212 - val_loss: 0.0243\n",
      "\n",
      "Epoch 02202: LearningRateScheduler reducing learning rate to 0.00026586280414602165.\n",
      "Epoch 2202/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0246\n",
      "Epoch 02202: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0246 - val_loss: 0.0247\n",
      "\n",
      "Epoch 02203: LearningRateScheduler reducing learning rate to 0.00026583298513127877.\n",
      "Epoch 2203/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0267\n",
      "Epoch 02203: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0267 - val_loss: 0.0264\n",
      "\n",
      "Epoch 02204: LearningRateScheduler reducing learning rate to 0.0002658031547915896.\n",
      "Epoch 2204/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0211\n",
      "Epoch 02204: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0211 - val_loss: 0.0276\n",
      "\n",
      "Epoch 02205: LearningRateScheduler reducing learning rate to 0.00026577331312990874.\n",
      "Epoch 2205/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0156\n",
      "Epoch 02205: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0156 - val_loss: 0.0288\n",
      "\n",
      "Epoch 02206: LearningRateScheduler reducing learning rate to 0.0002657434601491922.\n",
      "Epoch 2206/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0250\n",
      "Epoch 02206: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 241ms/step - loss: 0.0250 - val_loss: 0.0307\n",
      "\n",
      "Epoch 02207: LearningRateScheduler reducing learning rate to 0.00026571359585239704.\n",
      "Epoch 2207/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0167\n",
      "Epoch 02207: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0167 - val_loss: 0.0304\n",
      "\n",
      "Epoch 02208: LearningRateScheduler reducing learning rate to 0.0002656837202424813.\n",
      "Epoch 2208/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0128\n",
      "Epoch 02208: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 239ms/step - loss: 0.0128 - val_loss: 0.0277\n",
      "\n",
      "Epoch 02209: LearningRateScheduler reducing learning rate to 0.00026565383332240416.\n",
      "Epoch 2209/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0185\n",
      "Epoch 02209: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0185 - val_loss: 0.0238\n",
      "\n",
      "Epoch 02210: LearningRateScheduler reducing learning rate to 0.00026562393509512605.\n",
      "Epoch 2210/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0343\n",
      "Epoch 02210: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0343 - val_loss: 0.0225\n",
      "\n",
      "Epoch 02211: LearningRateScheduler reducing learning rate to 0.0002655940255636086.\n",
      "Epoch 2211/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0212\n",
      "Epoch 02211: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 241ms/step - loss: 0.0212 - val_loss: 0.0240\n",
      "\n",
      "Epoch 02212: LearningRateScheduler reducing learning rate to 0.0002655641047308142.\n",
      "Epoch 2212/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0205\n",
      "Epoch 02212: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 239ms/step - loss: 0.0205 - val_loss: 0.0250\n",
      "\n",
      "Epoch 02213: LearningRateScheduler reducing learning rate to 0.0002655341725997067.\n",
      "Epoch 2213/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0202\n",
      "Epoch 02213: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0202 - val_loss: 0.0271\n",
      "\n",
      "Epoch 02214: LearningRateScheduler reducing learning rate to 0.0002655042291732509.\n",
      "Epoch 2214/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0204\n",
      "Epoch 02214: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0204 - val_loss: 0.0271\n",
      "\n",
      "Epoch 02215: LearningRateScheduler reducing learning rate to 0.0002654742744544128.\n",
      "Epoch 2215/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0175\n",
      "Epoch 02215: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 237ms/step - loss: 0.0175 - val_loss: 0.0249\n",
      "\n",
      "Epoch 02216: LearningRateScheduler reducing learning rate to 0.0002654443084461595.\n",
      "Epoch 2216/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0225\n",
      "Epoch 02216: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0225 - val_loss: 0.0237\n",
      "\n",
      "Epoch 02217: LearningRateScheduler reducing learning rate to 0.0002654143311514591.\n",
      "Epoch 2217/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0217\n",
      "Epoch 02217: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0217 - val_loss: 0.0233\n",
      "\n",
      "Epoch 02218: LearningRateScheduler reducing learning rate to 0.00026538434257328107.\n",
      "Epoch 2218/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0300\n",
      "Epoch 02218: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0300 - val_loss: 0.0229\n",
      "\n",
      "Epoch 02219: LearningRateScheduler reducing learning rate to 0.00026535434271459567.\n",
      "Epoch 2219/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0191\n",
      "Epoch 02219: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0191 - val_loss: 0.0222\n",
      "\n",
      "Epoch 02220: LearningRateScheduler reducing learning rate to 0.0002653243315783746.\n",
      "Epoch 2220/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0248\n",
      "Epoch 02220: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0248 - val_loss: 0.0211\n",
      "\n",
      "Epoch 02221: LearningRateScheduler reducing learning rate to 0.0002652943091675905.\n",
      "Epoch 2221/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0295\n",
      "Epoch 02221: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 240ms/step - loss: 0.0295 - val_loss: 0.0194\n",
      "\n",
      "Epoch 02222: LearningRateScheduler reducing learning rate to 0.00026526427548521713.\n",
      "Epoch 2222/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0270\n",
      "Epoch 02222: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0270 - val_loss: 0.0190\n",
      "\n",
      "Epoch 02223: LearningRateScheduler reducing learning rate to 0.00026523423053422944.\n",
      "Epoch 2223/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0162\n",
      "Epoch 02223: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0162 - val_loss: 0.0193\n",
      "\n",
      "Epoch 02224: LearningRateScheduler reducing learning rate to 0.00026520417431760343.\n",
      "Epoch 2224/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0201\n",
      "Epoch 02224: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0201 - val_loss: 0.0195\n",
      "\n",
      "Epoch 02225: LearningRateScheduler reducing learning rate to 0.00026517410683831615.\n",
      "Epoch 2225/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0213\n",
      "Epoch 02225: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0213 - val_loss: 0.0211\n",
      "\n",
      "Epoch 02226: LearningRateScheduler reducing learning rate to 0.000265144028099346.\n",
      "Epoch 2226/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0206\n",
      "Epoch 02226: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0206 - val_loss: 0.0228\n",
      "\n",
      "Epoch 02227: LearningRateScheduler reducing learning rate to 0.0002651139381036723.\n",
      "Epoch 2227/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0192\n",
      "Epoch 02227: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 227ms/step - loss: 0.0192 - val_loss: 0.0220\n",
      "\n",
      "Epoch 02228: LearningRateScheduler reducing learning rate to 0.0002650838368542755.\n",
      "Epoch 2228/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0202\n",
      "Epoch 02228: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 240ms/step - loss: 0.0202 - val_loss: 0.0206\n",
      "\n",
      "Epoch 02229: LearningRateScheduler reducing learning rate to 0.0002650537243541372.\n",
      "Epoch 2229/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0175\n",
      "Epoch 02229: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0175 - val_loss: 0.0207\n",
      "\n",
      "Epoch 02230: LearningRateScheduler reducing learning rate to 0.0002650236006062401.\n",
      "Epoch 2230/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0214\n",
      "Epoch 02230: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0214 - val_loss: 0.0208\n",
      "\n",
      "Epoch 02231: LearningRateScheduler reducing learning rate to 0.0002649934656135681.\n",
      "Epoch 2231/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0238\n",
      "Epoch 02231: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0238 - val_loss: 0.0217\n",
      "\n",
      "Epoch 02232: LearningRateScheduler reducing learning rate to 0.0002649633193791061.\n",
      "Epoch 2232/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0209\n",
      "Epoch 02232: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0209 - val_loss: 0.0228\n",
      "\n",
      "Epoch 02233: LearningRateScheduler reducing learning rate to 0.0002649331619058401.\n",
      "Epoch 2233/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0226\n",
      "Epoch 02233: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 240ms/step - loss: 0.0226 - val_loss: 0.0260\n",
      "\n",
      "Epoch 02234: LearningRateScheduler reducing learning rate to 0.00026490299319675733.\n",
      "Epoch 2234/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0226\n",
      "Epoch 02234: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 237ms/step - loss: 0.0226 - val_loss: 0.0284\n",
      "\n",
      "Epoch 02235: LearningRateScheduler reducing learning rate to 0.0002648728132548461.\n",
      "Epoch 2235/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0213\n",
      "Epoch 02235: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 245ms/step - loss: 0.0213 - val_loss: 0.0291\n",
      "\n",
      "Epoch 02236: LearningRateScheduler reducing learning rate to 0.00026484262208309575.\n",
      "Epoch 2236/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0164\n",
      "Epoch 02236: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0164 - val_loss: 0.0281\n",
      "\n",
      "Epoch 02237: LearningRateScheduler reducing learning rate to 0.0002648124196844968.\n",
      "Epoch 2237/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0190\n",
      "Epoch 02237: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0190 - val_loss: 0.0256\n",
      "\n",
      "Epoch 02238: LearningRateScheduler reducing learning rate to 0.00026478220606204083.\n",
      "Epoch 2238/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0271\n",
      "Epoch 02238: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 228ms/step - loss: 0.0271 - val_loss: 0.0244\n",
      "\n",
      "Epoch 02239: LearningRateScheduler reducing learning rate to 0.00026475198121872056.\n",
      "Epoch 2239/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0255\n",
      "Epoch 02239: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 240ms/step - loss: 0.0255 - val_loss: 0.0242\n",
      "\n",
      "Epoch 02240: LearningRateScheduler reducing learning rate to 0.00026472174515752996.\n",
      "Epoch 2240/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0237\n",
      "Epoch 02240: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0237 - val_loss: 0.0272\n",
      "\n",
      "Epoch 02241: LearningRateScheduler reducing learning rate to 0.0002646914978814638.\n",
      "Epoch 2241/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0166\n",
      "Epoch 02241: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0166 - val_loss: 0.0305\n",
      "\n",
      "Epoch 02242: LearningRateScheduler reducing learning rate to 0.0002646612393935183.\n",
      "Epoch 2242/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0183\n",
      "Epoch 02242: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0183 - val_loss: 0.0321\n",
      "\n",
      "Epoch 02243: LearningRateScheduler reducing learning rate to 0.0002646309696966906.\n",
      "Epoch 2243/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0330\n",
      "Epoch 02243: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0330 - val_loss: 0.0318\n",
      "\n",
      "Epoch 02244: LearningRateScheduler reducing learning rate to 0.0002646006887939789.\n",
      "Epoch 2244/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0273\n",
      "Epoch 02244: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 237ms/step - loss: 0.0273 - val_loss: 0.0303\n",
      "\n",
      "Epoch 02245: LearningRateScheduler reducing learning rate to 0.00026457039668838266.\n",
      "Epoch 2245/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0143\n",
      "Epoch 02245: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 240ms/step - loss: 0.0143 - val_loss: 0.0278\n",
      "\n",
      "Epoch 02246: LearningRateScheduler reducing learning rate to 0.00026454009338290233.\n",
      "Epoch 2246/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0287\n",
      "Epoch 02246: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0287 - val_loss: 0.0269\n",
      "\n",
      "Epoch 02247: LearningRateScheduler reducing learning rate to 0.0002645097788805396.\n",
      "Epoch 2247/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0245\n",
      "Epoch 02247: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0245 - val_loss: 0.0286\n",
      "\n",
      "Epoch 02248: LearningRateScheduler reducing learning rate to 0.0002644794531842972.\n",
      "Epoch 2248/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0323\n",
      "Epoch 02248: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 228ms/step - loss: 0.0323 - val_loss: 0.0278\n",
      "\n",
      "Epoch 02249: LearningRateScheduler reducing learning rate to 0.00026444911629717885.\n",
      "Epoch 2249/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0264\n",
      "Epoch 02249: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0264 - val_loss: 0.0261\n",
      "\n",
      "Epoch 02250: LearningRateScheduler reducing learning rate to 0.00026441876822218957.\n",
      "Epoch 2250/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0163\n",
      "Epoch 02250: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0163 - val_loss: 0.0259\n",
      "\n",
      "Epoch 02251: LearningRateScheduler reducing learning rate to 0.0002643884089623354.\n",
      "Epoch 2251/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0264\n",
      "Epoch 02251: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0264 - val_loss: 0.0276\n",
      "\n",
      "Epoch 02252: LearningRateScheduler reducing learning rate to 0.00026435803852062345.\n",
      "Epoch 2252/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0205\n",
      "Epoch 02252: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 227ms/step - loss: 0.0205 - val_loss: 0.0286\n",
      "\n",
      "Epoch 02253: LearningRateScheduler reducing learning rate to 0.00026432765690006203.\n",
      "Epoch 2253/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0226\n",
      "Epoch 02253: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0226 - val_loss: 0.0271\n",
      "\n",
      "Epoch 02254: LearningRateScheduler reducing learning rate to 0.0002642972641036606.\n",
      "Epoch 2254/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0151\n",
      "Epoch 02254: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0151 - val_loss: 0.0253\n",
      "\n",
      "Epoch 02255: LearningRateScheduler reducing learning rate to 0.0002642668601344294.\n",
      "Epoch 2255/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0200\n",
      "Epoch 02255: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0200 - val_loss: 0.0234\n",
      "\n",
      "Epoch 02256: LearningRateScheduler reducing learning rate to 0.00026423644499538017.\n",
      "Epoch 2256/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0224\n",
      "Epoch 02256: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0224 - val_loss: 0.0335\n",
      "\n",
      "Epoch 02257: LearningRateScheduler reducing learning rate to 0.0002642060186895256.\n",
      "Epoch 2257/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0241\n",
      "Epoch 02257: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0241 - val_loss: 0.0315\n",
      "\n",
      "Epoch 02258: LearningRateScheduler reducing learning rate to 0.0002641755812198794.\n",
      "Epoch 2258/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0187\n",
      "Epoch 02258: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0187 - val_loss: 0.0285\n",
      "\n",
      "Epoch 02259: LearningRateScheduler reducing learning rate to 0.00026414513258945664.\n",
      "Epoch 2259/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0250\n",
      "Epoch 02259: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0250 - val_loss: 0.0267\n",
      "\n",
      "Epoch 02260: LearningRateScheduler reducing learning rate to 0.00026411467280127316.\n",
      "Epoch 2260/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0216\n",
      "Epoch 02260: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0216 - val_loss: 0.0241\n",
      "\n",
      "Epoch 02261: LearningRateScheduler reducing learning rate to 0.0002640842018583461.\n",
      "Epoch 2261/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0223\n",
      "Epoch 02261: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0223 - val_loss: 0.0246\n",
      "\n",
      "Epoch 02262: LearningRateScheduler reducing learning rate to 0.0002640537197636938.\n",
      "Epoch 2262/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0225\n",
      "Epoch 02262: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0225 - val_loss: 0.0276\n",
      "\n",
      "Epoch 02263: LearningRateScheduler reducing learning rate to 0.0002640232265203354.\n",
      "Epoch 2263/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0367\n",
      "Epoch 02263: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0367 - val_loss: 0.0287\n",
      "\n",
      "Epoch 02264: LearningRateScheduler reducing learning rate to 0.0002639927221312915.\n",
      "Epoch 2264/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0337\n",
      "Epoch 02264: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 227ms/step - loss: 0.0337 - val_loss: 0.0282\n",
      "\n",
      "Epoch 02265: LearningRateScheduler reducing learning rate to 0.0002639622065995835.\n",
      "Epoch 2265/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0373\n",
      "Epoch 02265: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0373 - val_loss: 0.0266\n",
      "\n",
      "Epoch 02266: LearningRateScheduler reducing learning rate to 0.00026393167992823407.\n",
      "Epoch 2266/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0331\n",
      "Epoch 02266: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0331 - val_loss: 0.0256\n",
      "\n",
      "Epoch 02267: LearningRateScheduler reducing learning rate to 0.000263901142120267.\n",
      "Epoch 2267/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0154\n",
      "Epoch 02267: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0154 - val_loss: 0.0272\n",
      "\n",
      "Epoch 02268: LearningRateScheduler reducing learning rate to 0.0002638705931787071.\n",
      "Epoch 2268/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0199\n",
      "Epoch 02268: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0199 - val_loss: 0.0249\n",
      "\n",
      "Epoch 02269: LearningRateScheduler reducing learning rate to 0.00026384003310658024.\n",
      "Epoch 2269/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0359\n",
      "Epoch 02269: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0359 - val_loss: 0.0235\n",
      "\n",
      "Epoch 02270: LearningRateScheduler reducing learning rate to 0.00026380946190691354.\n",
      "Epoch 2270/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0226\n",
      "Epoch 02270: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 237ms/step - loss: 0.0226 - val_loss: 0.0228\n",
      "\n",
      "Epoch 02271: LearningRateScheduler reducing learning rate to 0.00026377887958273525.\n",
      "Epoch 2271/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0318\n",
      "Epoch 02271: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0318 - val_loss: 0.0230\n",
      "\n",
      "Epoch 02272: LearningRateScheduler reducing learning rate to 0.0002637482861370744.\n",
      "Epoch 2272/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0208\n",
      "Epoch 02272: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 237ms/step - loss: 0.0208 - val_loss: 0.0234\n",
      "\n",
      "Epoch 02273: LearningRateScheduler reducing learning rate to 0.00026371768157296144.\n",
      "Epoch 2273/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0312\n",
      "Epoch 02273: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0312 - val_loss: 0.0249\n",
      "\n",
      "Epoch 02274: LearningRateScheduler reducing learning rate to 0.0002636870658934279.\n",
      "Epoch 2274/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0340\n",
      "Epoch 02274: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0340 - val_loss: 0.0271\n",
      "\n",
      "Epoch 02275: LearningRateScheduler reducing learning rate to 0.0002636564391015062.\n",
      "Epoch 2275/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0310\n",
      "Epoch 02275: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0310 - val_loss: 0.0313\n",
      "\n",
      "Epoch 02276: LearningRateScheduler reducing learning rate to 0.0002636258012002301.\n",
      "Epoch 2276/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0248\n",
      "Epoch 02276: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0248 - val_loss: 0.0375\n",
      "\n",
      "Epoch 02277: LearningRateScheduler reducing learning rate to 0.0002635951521926343.\n",
      "Epoch 2277/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0320\n",
      "Epoch 02277: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0320 - val_loss: 0.0316\n",
      "\n",
      "Epoch 02278: LearningRateScheduler reducing learning rate to 0.00026356449208175467.\n",
      "Epoch 2278/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0292\n",
      "Epoch 02278: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 237ms/step - loss: 0.0292 - val_loss: 0.0271\n",
      "\n",
      "Epoch 02279: LearningRateScheduler reducing learning rate to 0.0002635338208706282.\n",
      "Epoch 2279/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0275\n",
      "Epoch 02279: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0275 - val_loss: 0.0261\n",
      "\n",
      "Epoch 02280: LearningRateScheduler reducing learning rate to 0.0002635031385622929.\n",
      "Epoch 2280/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0249\n",
      "Epoch 02280: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0249 - val_loss: 0.0311\n",
      "\n",
      "Epoch 02281: LearningRateScheduler reducing learning rate to 0.00026347244515978783.\n",
      "Epoch 2281/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0345\n",
      "Epoch 02281: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0345 - val_loss: 0.0333\n",
      "\n",
      "Epoch 02282: LearningRateScheduler reducing learning rate to 0.00026344174066615346.\n",
      "Epoch 2282/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0287\n",
      "Epoch 02282: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0287 - val_loss: 0.0301\n",
      "\n",
      "Epoch 02283: LearningRateScheduler reducing learning rate to 0.000263411025084431.\n",
      "Epoch 2283/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0214\n",
      "Epoch 02283: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 237ms/step - loss: 0.0214 - val_loss: 0.0263\n",
      "\n",
      "Epoch 02284: LearningRateScheduler reducing learning rate to 0.0002633802984176629.\n",
      "Epoch 2284/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0178\n",
      "Epoch 02284: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0178 - val_loss: 0.0249\n",
      "\n",
      "Epoch 02285: LearningRateScheduler reducing learning rate to 0.00026334956066889273.\n",
      "Epoch 2285/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0188\n",
      "Epoch 02285: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0188 - val_loss: 0.0235\n",
      "\n",
      "Epoch 02286: LearningRateScheduler reducing learning rate to 0.0002633188118411652.\n",
      "Epoch 2286/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0222\n",
      "Epoch 02286: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0222 - val_loss: 0.0232\n",
      "\n",
      "Epoch 02287: LearningRateScheduler reducing learning rate to 0.0002632880519375259.\n",
      "Epoch 2287/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0199\n",
      "Epoch 02287: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0199 - val_loss: 0.0241\n",
      "\n",
      "Epoch 02288: LearningRateScheduler reducing learning rate to 0.00026325728096102184.\n",
      "Epoch 2288/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0242\n",
      "Epoch 02288: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0242 - val_loss: 0.0228\n",
      "\n",
      "Epoch 02289: LearningRateScheduler reducing learning rate to 0.00026322649891470087.\n",
      "Epoch 2289/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0223\n",
      "Epoch 02289: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0223 - val_loss: 0.0219\n",
      "\n",
      "Epoch 02290: LearningRateScheduler reducing learning rate to 0.00026319570580161196.\n",
      "Epoch 2290/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0266\n",
      "Epoch 02290: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0266 - val_loss: 0.0225\n",
      "\n",
      "Epoch 02291: LearningRateScheduler reducing learning rate to 0.0002631649016248054.\n",
      "Epoch 2291/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0264\n",
      "Epoch 02291: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0264 - val_loss: 0.0247\n",
      "\n",
      "Epoch 02292: LearningRateScheduler reducing learning rate to 0.00026313408638733223.\n",
      "Epoch 2292/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0227\n",
      "Epoch 02292: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 238ms/step - loss: 0.0227 - val_loss: 0.0261\n",
      "\n",
      "Epoch 02293: LearningRateScheduler reducing learning rate to 0.00026310326009224493.\n",
      "Epoch 2293/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0183\n",
      "Epoch 02293: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0183 - val_loss: 0.0251\n",
      "\n",
      "Epoch 02294: LearningRateScheduler reducing learning rate to 0.00026307242274259683.\n",
      "Epoch 2294/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0270\n",
      "Epoch 02294: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0270 - val_loss: 0.0236\n",
      "\n",
      "Epoch 02295: LearningRateScheduler reducing learning rate to 0.00026304157434144245.\n",
      "Epoch 2295/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0215\n",
      "Epoch 02295: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0215 - val_loss: 0.0237\n",
      "\n",
      "Epoch 02296: LearningRateScheduler reducing learning rate to 0.0002630107148918374.\n",
      "Epoch 2296/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0299\n",
      "Epoch 02296: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0299 - val_loss: 0.0238\n",
      "\n",
      "Epoch 02297: LearningRateScheduler reducing learning rate to 0.0002629798443968384.\n",
      "Epoch 2297/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0274\n",
      "Epoch 02297: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 238ms/step - loss: 0.0274 - val_loss: 0.0241\n",
      "\n",
      "Epoch 02298: LearningRateScheduler reducing learning rate to 0.0002629489628595032.\n",
      "Epoch 2298/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0178\n",
      "Epoch 02298: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0178 - val_loss: 0.0241\n",
      "\n",
      "Epoch 02299: LearningRateScheduler reducing learning rate to 0.0002629180702828908.\n",
      "Epoch 2299/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0301\n",
      "Epoch 02299: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0301 - val_loss: 0.0239\n",
      "\n",
      "Epoch 02300: LearningRateScheduler reducing learning rate to 0.000262887166670061.\n",
      "Epoch 2300/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0210\n",
      "Epoch 02300: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0210 - val_loss: 0.0244\n",
      "\n",
      "Epoch 02301: LearningRateScheduler reducing learning rate to 0.00026285625202407506.\n",
      "Epoch 2301/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0252\n",
      "Epoch 02301: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0252 - val_loss: 0.0241\n",
      "\n",
      "Epoch 02302: LearningRateScheduler reducing learning rate to 0.00026282532634799497.\n",
      "Epoch 2302/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0211\n",
      "Epoch 02302: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0211 - val_loss: 0.0249\n",
      "\n",
      "Epoch 02303: LearningRateScheduler reducing learning rate to 0.0002627943896448841.\n",
      "Epoch 2303/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0206\n",
      "Epoch 02303: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0206 - val_loss: 0.0257\n",
      "\n",
      "Epoch 02304: LearningRateScheduler reducing learning rate to 0.0002627634419178068.\n",
      "Epoch 2304/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0172\n",
      "Epoch 02304: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0172 - val_loss: 0.0267\n",
      "\n",
      "Epoch 02305: LearningRateScheduler reducing learning rate to 0.00026273248316982853.\n",
      "Epoch 2305/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0288\n",
      "Epoch 02305: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0288 - val_loss: 0.0270\n",
      "\n",
      "Epoch 02306: LearningRateScheduler reducing learning rate to 0.00026270151340401576.\n",
      "Epoch 2306/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0207\n",
      "Epoch 02306: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0207 - val_loss: 0.0273\n",
      "\n",
      "Epoch 02307: LearningRateScheduler reducing learning rate to 0.00026267053262343614.\n",
      "Epoch 2307/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0190\n",
      "Epoch 02307: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0190 - val_loss: 0.0262\n",
      "\n",
      "Epoch 02308: LearningRateScheduler reducing learning rate to 0.0002626395408311584.\n",
      "Epoch 2308/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0186\n",
      "Epoch 02308: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 238ms/step - loss: 0.0186 - val_loss: 0.0249\n",
      "\n",
      "Epoch 02309: LearningRateScheduler reducing learning rate to 0.0002626085380302523.\n",
      "Epoch 2309/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0220\n",
      "Epoch 02309: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 238ms/step - loss: 0.0220 - val_loss: 0.0244\n",
      "\n",
      "Epoch 02310: LearningRateScheduler reducing learning rate to 0.00026257752422378884.\n",
      "Epoch 2310/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0191\n",
      "Epoch 02310: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0191 - val_loss: 0.0245\n",
      "\n",
      "Epoch 02311: LearningRateScheduler reducing learning rate to 0.00026254649941483994.\n",
      "Epoch 2311/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0241\n",
      "Epoch 02311: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0241 - val_loss: 0.0249\n",
      "\n",
      "Epoch 02312: LearningRateScheduler reducing learning rate to 0.0002625154636064787.\n",
      "Epoch 2312/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0214\n",
      "Epoch 02312: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0214 - val_loss: 0.0252\n",
      "\n",
      "Epoch 02313: LearningRateScheduler reducing learning rate to 0.00026248441680177924.\n",
      "Epoch 2313/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0180\n",
      "Epoch 02313: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0180 - val_loss: 0.0261\n",
      "\n",
      "Epoch 02314: LearningRateScheduler reducing learning rate to 0.0002624533590038169.\n",
      "Epoch 2314/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0212\n",
      "Epoch 02314: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0212 - val_loss: 0.0267\n",
      "\n",
      "Epoch 02315: LearningRateScheduler reducing learning rate to 0.00026242229021566796.\n",
      "Epoch 2315/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0180\n",
      "Epoch 02315: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0180 - val_loss: 0.0269\n",
      "\n",
      "Epoch 02316: LearningRateScheduler reducing learning rate to 0.0002623912104404099.\n",
      "Epoch 2316/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0180\n",
      "Epoch 02316: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0180 - val_loss: 0.0271\n",
      "\n",
      "Epoch 02317: LearningRateScheduler reducing learning rate to 0.0002623601196811212.\n",
      "Epoch 2317/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0253\n",
      "Epoch 02317: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0253 - val_loss: 0.0278\n",
      "\n",
      "Epoch 02318: LearningRateScheduler reducing learning rate to 0.0002623290179408815.\n",
      "Epoch 2318/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0170\n",
      "Epoch 02318: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0170 - val_loss: 0.0286\n",
      "\n",
      "Epoch 02319: LearningRateScheduler reducing learning rate to 0.00026229790522277146.\n",
      "Epoch 2319/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0154\n",
      "Epoch 02319: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0154 - val_loss: 0.0308\n",
      "\n",
      "Epoch 02320: LearningRateScheduler reducing learning rate to 0.0002622667815298729.\n",
      "Epoch 2320/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0191\n",
      "Epoch 02320: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0191 - val_loss: 0.0332\n",
      "\n",
      "Epoch 02321: LearningRateScheduler reducing learning rate to 0.00026223564686526875.\n",
      "Epoch 2321/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0126\n",
      "Epoch 02321: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 227ms/step - loss: 0.0126 - val_loss: 0.0312\n",
      "\n",
      "Epoch 02322: LearningRateScheduler reducing learning rate to 0.0002622045012320429.\n",
      "Epoch 2322/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0194\n",
      "Epoch 02322: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0194 - val_loss: 0.0282\n",
      "\n",
      "Epoch 02323: LearningRateScheduler reducing learning rate to 0.0002621733446332804.\n",
      "Epoch 2323/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0148\n",
      "Epoch 02323: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 239ms/step - loss: 0.0148 - val_loss: 0.0263\n",
      "\n",
      "Epoch 02324: LearningRateScheduler reducing learning rate to 0.0002621421770720674.\n",
      "Epoch 2324/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0257\n",
      "Epoch 02324: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0257 - val_loss: 0.0247\n",
      "\n",
      "Epoch 02325: LearningRateScheduler reducing learning rate to 0.0002621109985514911.\n",
      "Epoch 2325/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0204\n",
      "Epoch 02325: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0204 - val_loss: 0.0238\n",
      "\n",
      "Epoch 02326: LearningRateScheduler reducing learning rate to 0.00026207980907463983.\n",
      "Epoch 2326/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0277\n",
      "Epoch 02326: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0277 - val_loss: 0.0223\n",
      "\n",
      "Epoch 02327: LearningRateScheduler reducing learning rate to 0.0002620486086446029.\n",
      "Epoch 2327/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0222\n",
      "Epoch 02327: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0222 - val_loss: 0.0216\n",
      "\n",
      "Epoch 02328: LearningRateScheduler reducing learning rate to 0.00026201739726447096.\n",
      "Epoch 2328/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0222\n",
      "Epoch 02328: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0222 - val_loss: 0.0231\n",
      "\n",
      "Epoch 02329: LearningRateScheduler reducing learning rate to 0.0002619861749373354.\n",
      "Epoch 2329/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0173\n",
      "Epoch 02329: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0173 - val_loss: 0.0239\n",
      "\n",
      "Epoch 02330: LearningRateScheduler reducing learning rate to 0.00026195494166628895.\n",
      "Epoch 2330/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0200\n",
      "Epoch 02330: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0200 - val_loss: 0.0237\n",
      "\n",
      "Epoch 02331: LearningRateScheduler reducing learning rate to 0.0002619236974544253.\n",
      "Epoch 2331/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0149\n",
      "Epoch 02331: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 238ms/step - loss: 0.0149 - val_loss: 0.0233\n",
      "\n",
      "Epoch 02332: LearningRateScheduler reducing learning rate to 0.00026189244230483924.\n",
      "Epoch 2332/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0254\n",
      "Epoch 02332: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0254 - val_loss: 0.0236\n",
      "\n",
      "Epoch 02333: LearningRateScheduler reducing learning rate to 0.00026186117622062675.\n",
      "Epoch 2333/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0197\n",
      "Epoch 02333: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0197 - val_loss: 0.0235\n",
      "\n",
      "Epoch 02334: LearningRateScheduler reducing learning rate to 0.00026182989920488475.\n",
      "Epoch 2334/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0210\n",
      "Epoch 02334: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0210 - val_loss: 0.0217\n",
      "\n",
      "Epoch 02335: LearningRateScheduler reducing learning rate to 0.0002617986112607113.\n",
      "Epoch 2335/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0215\n",
      "Epoch 02335: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0215 - val_loss: 0.0215\n",
      "\n",
      "Epoch 02336: LearningRateScheduler reducing learning rate to 0.00026176731239120556.\n",
      "Epoch 2336/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0176\n",
      "Epoch 02336: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 241ms/step - loss: 0.0176 - val_loss: 0.0217\n",
      "\n",
      "Epoch 02337: LearningRateScheduler reducing learning rate to 0.0002617360025994677.\n",
      "Epoch 2337/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0161\n",
      "Epoch 02337: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 228ms/step - loss: 0.0161 - val_loss: 0.0217\n",
      "\n",
      "Epoch 02338: LearningRateScheduler reducing learning rate to 0.0002617046818885991.\n",
      "Epoch 2338/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0175\n",
      "Epoch 02338: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 241ms/step - loss: 0.0175 - val_loss: 0.0228\n",
      "\n",
      "Epoch 02339: LearningRateScheduler reducing learning rate to 0.0002616733502617021.\n",
      "Epoch 2339/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0231\n",
      "Epoch 02339: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0231 - val_loss: 0.0236\n",
      "\n",
      "Epoch 02340: LearningRateScheduler reducing learning rate to 0.0002616420077218803.\n",
      "Epoch 2340/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0219\n",
      "Epoch 02340: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 239ms/step - loss: 0.0219 - val_loss: 0.0225\n",
      "\n",
      "Epoch 02341: LearningRateScheduler reducing learning rate to 0.000261610654272238.\n",
      "Epoch 2341/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0244\n",
      "Epoch 02341: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0244 - val_loss: 0.0212\n",
      "\n",
      "Epoch 02342: LearningRateScheduler reducing learning rate to 0.0002615792899158811.\n",
      "Epoch 2342/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0233\n",
      "Epoch 02342: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 238ms/step - loss: 0.0233 - val_loss: 0.0207\n",
      "\n",
      "Epoch 02343: LearningRateScheduler reducing learning rate to 0.0002615479146559161.\n",
      "Epoch 2343/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0226\n",
      "Epoch 02343: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0226 - val_loss: 0.0198\n",
      "\n",
      "Epoch 02344: LearningRateScheduler reducing learning rate to 0.0002615165284954509.\n",
      "Epoch 2344/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0228\n",
      "Epoch 02344: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0228 - val_loss: 0.0256\n",
      "\n",
      "Epoch 02345: LearningRateScheduler reducing learning rate to 0.0002614851314375943.\n",
      "Epoch 2345/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0172\n",
      "Epoch 02345: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0172 - val_loss: 0.0636\n",
      "\n",
      "Epoch 02346: LearningRateScheduler reducing learning rate to 0.00026145372348545636.\n",
      "Epoch 2346/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0251\n",
      "Epoch 02346: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0251 - val_loss: 0.1162\n",
      "\n",
      "Epoch 02347: LearningRateScheduler reducing learning rate to 0.0002614223046421481.\n",
      "Epoch 2347/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0238\n",
      "Epoch 02347: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0238 - val_loss: 0.1718\n",
      "\n",
      "Epoch 02348: LearningRateScheduler reducing learning rate to 0.0002613908749107815.\n",
      "Epoch 2348/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0189\n",
      "Epoch 02348: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0189 - val_loss: 0.2853\n",
      "\n",
      "Epoch 02349: LearningRateScheduler reducing learning rate to 0.0002613594342944698.\n",
      "Epoch 2349/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0295\n",
      "Epoch 02349: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0295 - val_loss: 0.3492\n",
      "\n",
      "Epoch 02350: LearningRateScheduler reducing learning rate to 0.0002613279827963274.\n",
      "Epoch 2350/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0232\n",
      "Epoch 02350: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0232 - val_loss: 0.3518\n",
      "\n",
      "Epoch 02351: LearningRateScheduler reducing learning rate to 0.00026129652041946945.\n",
      "Epoch 2351/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0196\n",
      "Epoch 02351: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0196 - val_loss: 0.3071\n",
      "\n",
      "Epoch 02352: LearningRateScheduler reducing learning rate to 0.00026126504716701254.\n",
      "Epoch 2352/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0231\n",
      "Epoch 02352: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0231 - val_loss: 0.2623\n",
      "\n",
      "Epoch 02353: LearningRateScheduler reducing learning rate to 0.000261233563042074.\n",
      "Epoch 2353/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0220\n",
      "Epoch 02353: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0220 - val_loss: 0.2095\n",
      "\n",
      "Epoch 02354: LearningRateScheduler reducing learning rate to 0.00026120206804777253.\n",
      "Epoch 2354/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0316\n",
      "Epoch 02354: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0316 - val_loss: 0.1886\n",
      "\n",
      "Epoch 02355: LearningRateScheduler reducing learning rate to 0.00026117056218722773.\n",
      "Epoch 2355/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0204\n",
      "Epoch 02355: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0204 - val_loss: 0.1554\n",
      "\n",
      "Epoch 02356: LearningRateScheduler reducing learning rate to 0.00026113904546356035.\n",
      "Epoch 2356/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0244\n",
      "Epoch 02356: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0244 - val_loss: 0.0908\n",
      "\n",
      "Epoch 02357: LearningRateScheduler reducing learning rate to 0.0002611075178798922.\n",
      "Epoch 2357/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0277\n",
      "Epoch 02357: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 243ms/step - loss: 0.0277 - val_loss: 0.0580\n",
      "\n",
      "Epoch 02358: LearningRateScheduler reducing learning rate to 0.00026107597943934613.\n",
      "Epoch 2358/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0343\n",
      "Epoch 02358: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0343 - val_loss: 0.0410\n",
      "\n",
      "Epoch 02359: LearningRateScheduler reducing learning rate to 0.0002610444301450461.\n",
      "Epoch 2359/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0146\n",
      "Epoch 02359: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 240ms/step - loss: 0.0146 - val_loss: 0.0393\n",
      "\n",
      "Epoch 02360: LearningRateScheduler reducing learning rate to 0.0002610128700001172.\n",
      "Epoch 2360/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0180\n",
      "Epoch 02360: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0180 - val_loss: 0.0337\n",
      "\n",
      "Epoch 02361: LearningRateScheduler reducing learning rate to 0.00026098129900768545.\n",
      "Epoch 2361/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0228\n",
      "Epoch 02361: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 238ms/step - loss: 0.0228 - val_loss: 0.0324\n",
      "\n",
      "Epoch 02362: LearningRateScheduler reducing learning rate to 0.0002609497171708781.\n",
      "Epoch 2362/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0217\n",
      "Epoch 02362: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 239ms/step - loss: 0.0217 - val_loss: 0.0301\n",
      "\n",
      "Epoch 02363: LearningRateScheduler reducing learning rate to 0.00026091812449282334.\n",
      "Epoch 2363/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0191\n",
      "Epoch 02363: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 240ms/step - loss: 0.0191 - val_loss: 0.0293\n",
      "\n",
      "Epoch 02364: LearningRateScheduler reducing learning rate to 0.00026088652097665054.\n",
      "Epoch 2364/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0173\n",
      "Epoch 02364: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0173 - val_loss: 0.0304\n",
      "\n",
      "Epoch 02365: LearningRateScheduler reducing learning rate to 0.0002608549066254901.\n",
      "Epoch 2365/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0239\n",
      "Epoch 02365: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 237ms/step - loss: 0.0239 - val_loss: 0.0302\n",
      "\n",
      "Epoch 02366: LearningRateScheduler reducing learning rate to 0.00026082328144247347.\n",
      "Epoch 2366/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0281\n",
      "Epoch 02366: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0281 - val_loss: 0.0302\n",
      "\n",
      "Epoch 02367: LearningRateScheduler reducing learning rate to 0.0002607916454307332.\n",
      "Epoch 2367/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0192\n",
      "Epoch 02367: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 248ms/step - loss: 0.0192 - val_loss: 0.0301\n",
      "\n",
      "Epoch 02368: LearningRateScheduler reducing learning rate to 0.00026075999859340296.\n",
      "Epoch 2368/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0216\n",
      "Epoch 02368: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 238ms/step - loss: 0.0216 - val_loss: 0.0292\n",
      "\n",
      "Epoch 02369: LearningRateScheduler reducing learning rate to 0.0002607283409336174.\n",
      "Epoch 2369/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0173\n",
      "Epoch 02369: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0173 - val_loss: 0.0278\n",
      "\n",
      "Epoch 02370: LearningRateScheduler reducing learning rate to 0.00026069667245451225.\n",
      "Epoch 2370/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0245\n",
      "Epoch 02370: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 239ms/step - loss: 0.0245 - val_loss: 0.0269\n",
      "\n",
      "Epoch 02371: LearningRateScheduler reducing learning rate to 0.0002606649931592244.\n",
      "Epoch 2371/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0186\n",
      "Epoch 02371: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0186 - val_loss: 0.0272\n",
      "\n",
      "Epoch 02372: LearningRateScheduler reducing learning rate to 0.00026063330305089175.\n",
      "Epoch 2372/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0171\n",
      "Epoch 02372: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 238ms/step - loss: 0.0171 - val_loss: 0.0286\n",
      "\n",
      "Epoch 02373: LearningRateScheduler reducing learning rate to 0.00026060160213265326.\n",
      "Epoch 2373/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0240\n",
      "Epoch 02373: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0240 - val_loss: 0.0314\n",
      "\n",
      "Epoch 02374: LearningRateScheduler reducing learning rate to 0.000260569890407649.\n",
      "Epoch 2374/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0205\n",
      "Epoch 02374: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0205 - val_loss: 0.0342\n",
      "\n",
      "Epoch 02375: LearningRateScheduler reducing learning rate to 0.0002605381678790201.\n",
      "Epoch 2375/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0168\n",
      "Epoch 02375: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0168 - val_loss: 0.0344\n",
      "\n",
      "Epoch 02376: LearningRateScheduler reducing learning rate to 0.00026050643454990873.\n",
      "Epoch 2376/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0195\n",
      "Epoch 02376: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0195 - val_loss: 0.0329\n",
      "\n",
      "Epoch 02377: LearningRateScheduler reducing learning rate to 0.0002604746904234581.\n",
      "Epoch 2377/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0283\n",
      "Epoch 02377: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0283 - val_loss: 0.0299\n",
      "\n",
      "Epoch 02378: LearningRateScheduler reducing learning rate to 0.00026044293550281264.\n",
      "Epoch 2378/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0224\n",
      "Epoch 02378: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 238ms/step - loss: 0.0224 - val_loss: 0.0273\n",
      "\n",
      "Epoch 02379: LearningRateScheduler reducing learning rate to 0.00026041116979111763.\n",
      "Epoch 2379/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0216\n",
      "Epoch 02379: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0216 - val_loss: 0.0256\n",
      "\n",
      "Epoch 02380: LearningRateScheduler reducing learning rate to 0.00026037939329151967.\n",
      "Epoch 2380/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0225\n",
      "Epoch 02380: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 228ms/step - loss: 0.0225 - val_loss: 0.0227\n",
      "\n",
      "Epoch 02381: LearningRateScheduler reducing learning rate to 0.00026034760600716623.\n",
      "Epoch 2381/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0190\n",
      "Epoch 02381: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0190 - val_loss: 0.0198\n",
      "\n",
      "Epoch 02382: LearningRateScheduler reducing learning rate to 0.00026031580794120586.\n",
      "Epoch 2382/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0313\n",
      "Epoch 02382: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0313 - val_loss: 0.0193\n",
      "\n",
      "Epoch 02383: LearningRateScheduler reducing learning rate to 0.0002602839990967883.\n",
      "Epoch 2383/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0128\n",
      "Epoch 02383: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 227ms/step - loss: 0.0128 - val_loss: 0.0214\n",
      "\n",
      "Epoch 02384: LearningRateScheduler reducing learning rate to 0.00026025217947706433.\n",
      "Epoch 2384/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0177\n",
      "Epoch 02384: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0177 - val_loss: 0.0258\n",
      "\n",
      "Epoch 02385: LearningRateScheduler reducing learning rate to 0.00026022034908518564.\n",
      "Epoch 2385/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0199\n",
      "Epoch 02385: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0199 - val_loss: 0.0289\n",
      "\n",
      "Epoch 02386: LearningRateScheduler reducing learning rate to 0.0002601885079243052.\n",
      "Epoch 2386/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0249\n",
      "Epoch 02386: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0249 - val_loss: 0.0280\n",
      "\n",
      "Epoch 02387: LearningRateScheduler reducing learning rate to 0.0002601566559975769.\n",
      "Epoch 2387/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0217\n",
      "Epoch 02387: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0217 - val_loss: 0.0255\n",
      "\n",
      "Epoch 02388: LearningRateScheduler reducing learning rate to 0.0002601247933081558.\n",
      "Epoch 2388/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0262\n",
      "Epoch 02388: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0262 - val_loss: 0.0225\n",
      "\n",
      "Epoch 02389: LearningRateScheduler reducing learning rate to 0.0002600929198591979.\n",
      "Epoch 2389/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0230\n",
      "Epoch 02389: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 238ms/step - loss: 0.0230 - val_loss: 0.0205\n",
      "\n",
      "Epoch 02390: LearningRateScheduler reducing learning rate to 0.00026006103565386036.\n",
      "Epoch 2390/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0260\n",
      "Epoch 02390: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 227ms/step - loss: 0.0260 - val_loss: 0.0209\n",
      "\n",
      "Epoch 02391: LearningRateScheduler reducing learning rate to 0.00026002914069530144.\n",
      "Epoch 2391/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0228\n",
      "Epoch 02391: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0228 - val_loss: 0.0215\n",
      "\n",
      "Epoch 02392: LearningRateScheduler reducing learning rate to 0.0002599972349866803.\n",
      "Epoch 2392/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0223\n",
      "Epoch 02392: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0223 - val_loss: 0.0232\n",
      "\n",
      "Epoch 02393: LearningRateScheduler reducing learning rate to 0.0002599653185311574.\n",
      "Epoch 2393/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0245\n",
      "Epoch 02393: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0245 - val_loss: 0.0236\n",
      "\n",
      "Epoch 02394: LearningRateScheduler reducing learning rate to 0.00025993339133189403.\n",
      "Epoch 2394/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0200\n",
      "Epoch 02394: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 228ms/step - loss: 0.0200 - val_loss: 0.0221\n",
      "\n",
      "Epoch 02395: LearningRateScheduler reducing learning rate to 0.0002599014533920528.\n",
      "Epoch 2395/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0171\n",
      "Epoch 02395: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 238ms/step - loss: 0.0171 - val_loss: 0.0214\n",
      "\n",
      "Epoch 02396: LearningRateScheduler reducing learning rate to 0.0002598695047147971.\n",
      "Epoch 2396/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0168\n",
      "Epoch 02396: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0168 - val_loss: 0.0209\n",
      "\n",
      "Epoch 02397: LearningRateScheduler reducing learning rate to 0.0002598375453032915.\n",
      "Epoch 2397/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0206\n",
      "Epoch 02397: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0206 - val_loss: 0.0193\n",
      "\n",
      "Epoch 02398: LearningRateScheduler reducing learning rate to 0.0002598055751607018.\n",
      "Epoch 2398/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0243\n",
      "Epoch 02398: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0243 - val_loss: 0.0176\n",
      "\n",
      "Epoch 02399: LearningRateScheduler reducing learning rate to 0.00025977359429019456.\n",
      "Epoch 2399/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0263\n",
      "Epoch 02399: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0263 - val_loss: 0.0175\n",
      "\n",
      "Epoch 02400: LearningRateScheduler reducing learning rate to 0.00025974160269493765.\n",
      "Epoch 2400/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0310\n",
      "Epoch 02400: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 239ms/step - loss: 0.0310 - val_loss: 0.0177\n",
      "\n",
      "Epoch 02401: LearningRateScheduler reducing learning rate to 0.00025970960037809994.\n",
      "Epoch 2401/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0170\n",
      "Epoch 02401: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0170 - val_loss: 0.0191\n",
      "\n",
      "Epoch 02402: LearningRateScheduler reducing learning rate to 0.0002596775873428512.\n",
      "Epoch 2402/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0302\n",
      "Epoch 02402: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0302 - val_loss: 0.0197\n",
      "\n",
      "Epoch 02403: LearningRateScheduler reducing learning rate to 0.00025964556359236253.\n",
      "Epoch 2403/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0160\n",
      "Epoch 02403: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0160 - val_loss: 0.0211\n",
      "\n",
      "Epoch 02404: LearningRateScheduler reducing learning rate to 0.00025961352912980595.\n",
      "Epoch 2404/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0263\n",
      "Epoch 02404: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0263 - val_loss: 0.0230\n",
      "\n",
      "Epoch 02405: LearningRateScheduler reducing learning rate to 0.00025958148395835445.\n",
      "Epoch 2405/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0252\n",
      "Epoch 02405: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0252 - val_loss: 0.0242\n",
      "\n",
      "Epoch 02406: LearningRateScheduler reducing learning rate to 0.00025954942808118224.\n",
      "Epoch 2406/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0244\n",
      "Epoch 02406: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0244 - val_loss: 0.0254\n",
      "\n",
      "Epoch 02407: LearningRateScheduler reducing learning rate to 0.0002595173615014645.\n",
      "Epoch 2407/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0205\n",
      "Epoch 02407: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0205 - val_loss: 0.0272\n",
      "\n",
      "Epoch 02408: LearningRateScheduler reducing learning rate to 0.00025948528422237757.\n",
      "Epoch 2408/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0193\n",
      "Epoch 02408: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0193 - val_loss: 0.0312\n",
      "\n",
      "Epoch 02409: LearningRateScheduler reducing learning rate to 0.0002594531962470987.\n",
      "Epoch 2409/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0230\n",
      "Epoch 02409: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0230 - val_loss: 0.0323\n",
      "\n",
      "Epoch 02410: LearningRateScheduler reducing learning rate to 0.0002594210975788063.\n",
      "Epoch 2410/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0217\n",
      "Epoch 02410: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 242ms/step - loss: 0.0217 - val_loss: 0.0276\n",
      "\n",
      "Epoch 02411: LearningRateScheduler reducing learning rate to 0.00025938898822067987.\n",
      "Epoch 2411/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0294\n",
      "Epoch 02411: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0294 - val_loss: 0.0241\n",
      "\n",
      "Epoch 02412: LearningRateScheduler reducing learning rate to 0.0002593568681758998.\n",
      "Epoch 2412/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0158\n",
      "Epoch 02412: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0158 - val_loss: 0.0236\n",
      "\n",
      "Epoch 02413: LearningRateScheduler reducing learning rate to 0.0002593247374476478.\n",
      "Epoch 2413/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0121\n",
      "Epoch 02413: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0121 - val_loss: 0.0242\n",
      "\n",
      "Epoch 02414: LearningRateScheduler reducing learning rate to 0.0002592925960391063.\n",
      "Epoch 2414/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0151\n",
      "Epoch 02414: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0151 - val_loss: 0.0241\n",
      "\n",
      "Epoch 02415: LearningRateScheduler reducing learning rate to 0.0002592604439534592.\n",
      "Epoch 2415/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0158\n",
      "Epoch 02415: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0158 - val_loss: 0.0210\n",
      "\n",
      "Epoch 02416: LearningRateScheduler reducing learning rate to 0.0002592282811938911.\n",
      "Epoch 2416/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0209\n",
      "Epoch 02416: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0209 - val_loss: 0.0197\n",
      "\n",
      "Epoch 02417: LearningRateScheduler reducing learning rate to 0.0002591961077635878.\n",
      "Epoch 2417/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0265\n",
      "Epoch 02417: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0265 - val_loss: 0.0193\n",
      "\n",
      "Epoch 02418: LearningRateScheduler reducing learning rate to 0.00025916392366573626.\n",
      "Epoch 2418/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0356\n",
      "Epoch 02418: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0356 - val_loss: 0.0203\n",
      "\n",
      "Epoch 02419: LearningRateScheduler reducing learning rate to 0.0002591317289035243.\n",
      "Epoch 2419/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0157\n",
      "Epoch 02419: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0157 - val_loss: 0.0212\n",
      "\n",
      "Epoch 02420: LearningRateScheduler reducing learning rate to 0.00025909952348014085.\n",
      "Epoch 2420/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0190\n",
      "Epoch 02420: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0190 - val_loss: 0.0192\n",
      "\n",
      "Epoch 02421: LearningRateScheduler reducing learning rate to 0.000259067307398776.\n",
      "Epoch 2421/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0162\n",
      "Epoch 02421: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 237ms/step - loss: 0.0162 - val_loss: 0.0177\n",
      "\n",
      "Epoch 02422: LearningRateScheduler reducing learning rate to 0.00025903508066262086.\n",
      "Epoch 2422/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0223\n",
      "Epoch 02422: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 239ms/step - loss: 0.0223 - val_loss: 0.0174\n",
      "\n",
      "Epoch 02423: LearningRateScheduler reducing learning rate to 0.00025900284327486745.\n",
      "Epoch 2423/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0148\n",
      "Epoch 02423: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0148 - val_loss: 0.0178\n",
      "\n",
      "Epoch 02424: LearningRateScheduler reducing learning rate to 0.00025897059523870904.\n",
      "Epoch 2424/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0206\n",
      "Epoch 02424: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 238ms/step - loss: 0.0206 - val_loss: 0.0205\n",
      "\n",
      "Epoch 02425: LearningRateScheduler reducing learning rate to 0.0002589383365573399.\n",
      "Epoch 2425/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0243\n",
      "Epoch 02425: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0243 - val_loss: 0.0219\n",
      "\n",
      "Epoch 02426: LearningRateScheduler reducing learning rate to 0.00025890606723395526.\n",
      "Epoch 2426/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0199\n",
      "Epoch 02426: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0199 - val_loss: 0.0216\n",
      "\n",
      "Epoch 02427: LearningRateScheduler reducing learning rate to 0.0002588737872717514.\n",
      "Epoch 2427/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0265\n",
      "Epoch 02427: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0265 - val_loss: 0.0202\n",
      "\n",
      "Epoch 02428: LearningRateScheduler reducing learning rate to 0.0002588414966739259.\n",
      "Epoch 2428/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0161\n",
      "Epoch 02428: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 228ms/step - loss: 0.0161 - val_loss: 0.0201\n",
      "\n",
      "Epoch 02429: LearningRateScheduler reducing learning rate to 0.0002588091954436771.\n",
      "Epoch 2429/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0186\n",
      "Epoch 02429: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0186 - val_loss: 0.0202\n",
      "\n",
      "Epoch 02430: LearningRateScheduler reducing learning rate to 0.00025877688358420456.\n",
      "Epoch 2430/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0153\n",
      "Epoch 02430: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0153 - val_loss: 0.0184\n",
      "\n",
      "Epoch 02431: LearningRateScheduler reducing learning rate to 0.0002587445610987088.\n",
      "Epoch 2431/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0250\n",
      "Epoch 02431: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 228ms/step - loss: 0.0250 - val_loss: 0.0168\n",
      "\n",
      "Epoch 02432: LearningRateScheduler reducing learning rate to 0.0002587122279903915.\n",
      "Epoch 2432/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0173\n",
      "Epoch 02432: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0173 - val_loss: 0.0168\n",
      "\n",
      "Epoch 02433: LearningRateScheduler reducing learning rate to 0.00025867988426245525.\n",
      "Epoch 2433/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0168\n",
      "Epoch 02433: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0168 - val_loss: 0.0175\n",
      "\n",
      "Epoch 02434: LearningRateScheduler reducing learning rate to 0.0002586475299181039.\n",
      "Epoch 2434/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0185\n",
      "Epoch 02434: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0185 - val_loss: 0.0172\n",
      "\n",
      "Epoch 02435: LearningRateScheduler reducing learning rate to 0.00025861516496054204.\n",
      "Epoch 2435/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0138\n",
      "Epoch 02435: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0138 - val_loss: 0.0178\n",
      "\n",
      "Epoch 02436: LearningRateScheduler reducing learning rate to 0.00025858278939297567.\n",
      "Epoch 2436/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0256\n",
      "Epoch 02436: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0256 - val_loss: 0.0201\n",
      "\n",
      "Epoch 02437: LearningRateScheduler reducing learning rate to 0.00025855040321861157.\n",
      "Epoch 2437/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0257\n",
      "Epoch 02437: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0257 - val_loss: 0.0189\n",
      "\n",
      "Epoch 02438: LearningRateScheduler reducing learning rate to 0.00025851800644065764.\n",
      "Epoch 2438/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0272\n",
      "Epoch 02438: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0272 - val_loss: 0.0178\n",
      "\n",
      "Epoch 02439: LearningRateScheduler reducing learning rate to 0.00025848559906232294.\n",
      "Epoch 2439/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0145\n",
      "Epoch 02439: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 237ms/step - loss: 0.0145 - val_loss: 0.0199\n",
      "\n",
      "Epoch 02440: LearningRateScheduler reducing learning rate to 0.00025845318108681743.\n",
      "Epoch 2440/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0212\n",
      "Epoch 02440: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 238ms/step - loss: 0.0212 - val_loss: 0.0223\n",
      "\n",
      "Epoch 02441: LearningRateScheduler reducing learning rate to 0.00025842075251735223.\n",
      "Epoch 2441/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0182\n",
      "Epoch 02441: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0182 - val_loss: 0.0242\n",
      "\n",
      "Epoch 02442: LearningRateScheduler reducing learning rate to 0.0002583883133571394.\n",
      "Epoch 2442/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0166\n",
      "Epoch 02442: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0166 - val_loss: 0.0257\n",
      "\n",
      "Epoch 02443: LearningRateScheduler reducing learning rate to 0.0002583558636093922.\n",
      "Epoch 2443/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0227\n",
      "Epoch 02443: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0227 - val_loss: 0.0248\n",
      "\n",
      "Epoch 02444: LearningRateScheduler reducing learning rate to 0.0002583234032773248.\n",
      "Epoch 2444/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0276\n",
      "Epoch 02444: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0276 - val_loss: 0.0237\n",
      "\n",
      "Epoch 02445: LearningRateScheduler reducing learning rate to 0.0002582909323641525.\n",
      "Epoch 2445/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0257\n",
      "Epoch 02445: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 237ms/step - loss: 0.0257 - val_loss: 0.0230\n",
      "\n",
      "Epoch 02446: LearningRateScheduler reducing learning rate to 0.00025825845087309155.\n",
      "Epoch 2446/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0199\n",
      "Epoch 02446: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0199 - val_loss: 0.0222\n",
      "\n",
      "Epoch 02447: LearningRateScheduler reducing learning rate to 0.0002582259588073594.\n",
      "Epoch 2447/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0309\n",
      "Epoch 02447: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0309 - val_loss: 0.0211\n",
      "\n",
      "Epoch 02448: LearningRateScheduler reducing learning rate to 0.0002581934561701745.\n",
      "Epoch 2448/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0196\n",
      "Epoch 02448: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0196 - val_loss: 0.0207\n",
      "\n",
      "Epoch 02449: LearningRateScheduler reducing learning rate to 0.0002581609429647561.\n",
      "Epoch 2449/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0204\n",
      "Epoch 02449: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0204 - val_loss: 0.0201\n",
      "\n",
      "Epoch 02450: LearningRateScheduler reducing learning rate to 0.0002581284191943249.\n",
      "Epoch 2450/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0182\n",
      "Epoch 02450: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0182 - val_loss: 0.0209\n",
      "\n",
      "Epoch 02451: LearningRateScheduler reducing learning rate to 0.00025809588486210244.\n",
      "Epoch 2451/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0277\n",
      "Epoch 02451: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0277 - val_loss: 0.0225\n",
      "\n",
      "Epoch 02452: LearningRateScheduler reducing learning rate to 0.00025806333997131125.\n",
      "Epoch 2452/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0201\n",
      "Epoch 02452: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0201 - val_loss: 0.0234\n",
      "\n",
      "Epoch 02453: LearningRateScheduler reducing learning rate to 0.00025803078452517504.\n",
      "Epoch 2453/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0218\n",
      "Epoch 02453: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0218 - val_loss: 0.0234\n",
      "\n",
      "Epoch 02454: LearningRateScheduler reducing learning rate to 0.0002579982185269185.\n",
      "Epoch 2454/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0236\n",
      "Epoch 02454: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 237ms/step - loss: 0.0236 - val_loss: 0.0228\n",
      "\n",
      "Epoch 02455: LearningRateScheduler reducing learning rate to 0.0002579656419797673.\n",
      "Epoch 2455/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0268\n",
      "Epoch 02455: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0268 - val_loss: 0.0229\n",
      "\n",
      "Epoch 02456: LearningRateScheduler reducing learning rate to 0.0002579330548869483.\n",
      "Epoch 2456/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0156\n",
      "Epoch 02456: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0156 - val_loss: 0.0246\n",
      "\n",
      "Epoch 02457: LearningRateScheduler reducing learning rate to 0.00025790045725168924.\n",
      "Epoch 2457/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0213\n",
      "Epoch 02457: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0213 - val_loss: 0.0256\n",
      "\n",
      "Epoch 02458: LearningRateScheduler reducing learning rate to 0.00025786784907721907.\n",
      "Epoch 2458/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0129\n",
      "Epoch 02458: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 238ms/step - loss: 0.0129 - val_loss: 0.0252\n",
      "\n",
      "Epoch 02459: LearningRateScheduler reducing learning rate to 0.0002578352303667677.\n",
      "Epoch 2459/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0171\n",
      "Epoch 02459: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0171 - val_loss: 0.0233\n",
      "\n",
      "Epoch 02460: LearningRateScheduler reducing learning rate to 0.00025780260112356597.\n",
      "Epoch 2460/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0299\n",
      "Epoch 02460: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0299 - val_loss: 0.0256\n",
      "\n",
      "Epoch 02461: LearningRateScheduler reducing learning rate to 0.0002577699613508461.\n",
      "Epoch 2461/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0288\n",
      "Epoch 02461: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0288 - val_loss: 0.0274\n",
      "\n",
      "Epoch 02462: LearningRateScheduler reducing learning rate to 0.0002577373110518409.\n",
      "Epoch 2462/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0208\n",
      "Epoch 02462: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 241ms/step - loss: 0.0208 - val_loss: 0.0275\n",
      "\n",
      "Epoch 02463: LearningRateScheduler reducing learning rate to 0.0002577046502297846.\n",
      "Epoch 2463/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0151\n",
      "Epoch 02463: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0151 - val_loss: 0.0260\n",
      "\n",
      "Epoch 02464: LearningRateScheduler reducing learning rate to 0.00025767197888791236.\n",
      "Epoch 2464/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0258\n",
      "Epoch 02464: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0258 - val_loss: 0.0236\n",
      "\n",
      "Epoch 02465: LearningRateScheduler reducing learning rate to 0.0002576392970294603.\n",
      "Epoch 2465/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0251\n",
      "Epoch 02465: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0251 - val_loss: 0.0228\n",
      "\n",
      "Epoch 02466: LearningRateScheduler reducing learning rate to 0.00025760660465766553.\n",
      "Epoch 2466/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0201\n",
      "Epoch 02466: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0201 - val_loss: 0.0229\n",
      "\n",
      "Epoch 02467: LearningRateScheduler reducing learning rate to 0.00025757390177576645.\n",
      "Epoch 2467/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0242\n",
      "Epoch 02467: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0242 - val_loss: 0.0212\n",
      "\n",
      "Epoch 02468: LearningRateScheduler reducing learning rate to 0.00025754118838700236.\n",
      "Epoch 2468/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0201\n",
      "Epoch 02468: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 239ms/step - loss: 0.0201 - val_loss: 0.0207\n",
      "\n",
      "Epoch 02469: LearningRateScheduler reducing learning rate to 0.00025750846449461344.\n",
      "Epoch 2469/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0173\n",
      "Epoch 02469: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 237ms/step - loss: 0.0173 - val_loss: 0.0219\n",
      "\n",
      "Epoch 02470: LearningRateScheduler reducing learning rate to 0.00025747573010184125.\n",
      "Epoch 2470/10000\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0265\n",
      "Epoch 02470: val_loss did not improve from 0.01270\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.0265 - val_loss: 0.0230\n",
      "Epoch 02470: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2a5ea45aa58>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_dataloader, epochs=Max_epoch, initial_epoch=stop_epoch, validation_data=val_dataloader, callbacks=callbacks)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3114c5a3de934491c2bf9db70eab0f2401d1d0e147bdce7e226ba3ca05564493"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 ('ops_keras')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
